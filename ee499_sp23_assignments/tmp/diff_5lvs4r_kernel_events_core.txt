1d0
< // SPDX-License-Identifier: GPL-2.0
8a8,9
>  *
>  * For licensing details see kernel-base/COPYING
31d31
< #include <linux/hugetlb.h>
53,56d52
< #include <linux/min_heap.h>
< #include <linux/highmem.h>
< #include <linux/pgtable.h>
< #include <linux/buildid.h>
101,103c97
<  * be on the current CPU, which just calls the function directly.  This will
<  * retry due to any failures in smp_call_function_single(), such as if the
<  * task_cpu() goes offline concurrently.
---
>  * be on the current CPU, which just calls the function directly
105c99,101
<  * returns @func return value or -ESRCH or -ENXIO when the process isn't running
---
>  * returns: @func return value, or
>  *	    -ESRCH  - when the process isn't running
>  *	    -EAGAIN - when the process moved away
118,120c114,115
< 	for (;;) {
< 		ret = smp_call_function_single(task_cpu(p), remote_function,
< 					       &data, 1);
---
> 	do {
> 		ret = smp_call_function_single(task_cpu(p), remote_function, &data, 1);
123,128c118
< 
< 		if (ret != -EAGAIN)
< 			break;
< 
< 		cond_resched();
< 	}
---
> 	} while (ret == -EAGAIN);
135d124
<  * @cpu:	target cpu to queue this function
273c262
< 		 * stabilize the event->ctx relation. See
---
> 		 * stabilize the the event->ctx relation. See
399,403d387
< static atomic_t nr_ksymbol_events __read_mostly;
< static atomic_t nr_bpf_events __read_mostly;
< static atomic_t nr_cgroup_events __read_mostly;
< static atomic_t nr_text_poke_events __read_mostly;
< static atomic_t nr_build_id_events __read_mostly;
409d392
< static struct kmem_cache *perf_event_cache;
450c433
< static bool perf_rotate_context(struct perf_cpu_context *cpuctx);
---
> static int perf_rotate_context(struct perf_cpu_context *cpuctx);
453c436,437
< 		void *buffer, size_t *lenp, loff_t *ppos)
---
> 		void __user *buffer, size_t *lenp,
> 		loff_t *ppos)
455,456c439,443
< 	int ret;
< 	int perf_cpu = sysctl_perf_cpu_time_max_percent;
---
> 	int ret = proc_dointvec_minmax(table, write, buffer, lenp, ppos);
> 
> 	if (ret || !write)
> 		return ret;
> 
460c447,448
< 	if (write && (perf_cpu == 100 || perf_cpu == 0))
---
> 	if (sysctl_perf_cpu_time_max_percent == 100 ||
> 	    sysctl_perf_cpu_time_max_percent == 0)
463,466d450
< 	ret = proc_dointvec_minmax(table, write, buffer, lenp, ppos);
< 	if (ret || !write)
< 		return ret;
< 
477c461,462
< 		void *buffer, size_t *lenp, loff_t *ppos)
---
> 				void __user *buffer, size_t *lenp,
> 				loff_t *ppos)
584a570,574
> extern __weak const char *perf_pmu_name(void)
> {
> 	return "pmu";
> }
> 
656c646
< 	for_each_sibling_event(sibling, leader)
---
> 	list_for_each_entry(sibling, &leader->sibling_list, group_entry)
737,745c727,729
< 	struct perf_cgroup *cgrp = cpuctx->cgrp;
< 	struct cgroup_subsys_state *css;
< 
< 	if (cgrp) {
< 		for (css = &cgrp->css; css; css = css->parent) {
< 			cgrp = container_of(css, struct perf_cgroup, css);
< 			__update_cgrp_time(cgrp);
< 		}
< 	}
---
> 	struct perf_cgroup *cgrp_out = cpuctx->cgrp;
> 	if (cgrp_out)
> 		__update_cgrp_time(cgrp_out);
763c747
< 	if (cgroup_is_descendant(cgrp->css.cgroup, event->cgrp->css.cgroup))
---
>        if (cgroup_is_descendant(cgrp->css.cgroup, event->cgrp->css.cgroup))
773d756
< 	struct cgroup_subsys_state *css;
784,789c767,768
< 
< 	for (css = &cgrp->css; css; css = css->parent) {
< 		cgrp = container_of(css, struct perf_cgroup, css);
< 		info = this_cpu_ptr(cgrp->info);
< 		info->timestamp = ctx->timestamp;
< 	}
---
> 	info = this_cpu_ptr(cgrp->info);
> 	info->timestamp = ctx->timestamp;
903,943d881
< static int perf_cgroup_ensure_storage(struct perf_event *event,
< 				struct cgroup_subsys_state *css)
< {
< 	struct perf_cpu_context *cpuctx;
< 	struct perf_event **storage;
< 	int cpu, heap_size, ret = 0;
< 
< 	/*
< 	 * Allow storage to have sufficent space for an iterator for each
< 	 * possibly nested cgroup plus an iterator for events with no cgroup.
< 	 */
< 	for (heap_size = 1; css; css = css->parent)
< 		heap_size++;
< 
< 	for_each_possible_cpu(cpu) {
< 		cpuctx = per_cpu_ptr(event->pmu->pmu_cpu_context, cpu);
< 		if (heap_size <= cpuctx->heap_size)
< 			continue;
< 
< 		storage = kmalloc_node(heap_size * sizeof(struct perf_event *),
< 				       GFP_KERNEL, cpu_to_node(cpu));
< 		if (!storage) {
< 			ret = -ENOMEM;
< 			break;
< 		}
< 
< 		raw_spin_lock_irq(&cpuctx->ctx.lock);
< 		if (cpuctx->heap_size < heap_size) {
< 			swap(cpuctx->heap, storage);
< 			if (storage == cpuctx->heap_default)
< 				storage = NULL;
< 			cpuctx->heap_size = heap_size;
< 		}
< 		raw_spin_unlock_irq(&cpuctx->ctx.lock);
< 
< 		kfree(storage);
< 	}
< 
< 	return ret;
< }
< 
963,966d900
< 	ret = perf_cgroup_ensure_storage(event, css);
< 	if (ret)
< 		goto out;
< 
991a926,929
> /*
>  * Update cpuctx->cgrp so that it is set when first cgroup event is added and
>  * cleared when last cgroup event is removed.
>  */
993c931,932
< perf_cgroup_event_enable(struct perf_event *event, struct perf_event_context *ctx)
---
> list_update_cgroup_event(struct perf_event *event,
> 			 struct perf_event_context *ctx, bool add)
995a935
> 	struct list_head *cpuctx_entry;
999a940,943
> 	if (add && ctx->nr_cgroups++)
> 		return;
> 	else if (!add && --ctx->nr_cgroups)
> 		return;
1002c946
< 	 * @ctx == &cpuctx->ctx.
---
> 	 * this will always be called from the right CPU.
1004,1012c948,951
< 	cpuctx = container_of(ctx, struct perf_cpu_context, ctx);
< 
< 	/*
< 	 * Since setting cpuctx->cgrp is conditional on the current @cgrp
< 	 * matching the event's cgroup, we must do this for every new event,
< 	 * because if the first would mismatch, the second would not try again
< 	 * and we would leave cpuctx->cgrp unset.
< 	 */
< 	if (ctx->is_active && !cpuctx->cgrp) {
---
> 	cpuctx = __get_cpu_context(ctx);
> 	cpuctx_entry = &cpuctx->cgrp_cpuctx_entry;
> 	/* cpuctx->cgrp is NULL unless a cgroup event is active in this CPU .*/
> 	if (add) {
1014a954
> 		list_add(cpuctx_entry, this_cpu_ptr(&cgrp_cpuctx_list));
1017,1043c957,958
< 	}
< 
< 	if (ctx->nr_cgroups++)
< 		return;
< 
< 	list_add(&cpuctx->cgrp_cpuctx_entry,
< 			per_cpu_ptr(&cgrp_cpuctx_list, event->cpu));
< }
< 
< static inline void
< perf_cgroup_event_disable(struct perf_event *event, struct perf_event_context *ctx)
< {
< 	struct perf_cpu_context *cpuctx;
< 
< 	if (!is_cgroup_event(event))
< 		return;
< 
< 	/*
< 	 * Because cgroup events are always per-cpu events,
< 	 * @ctx == &cpuctx->ctx.
< 	 */
< 	cpuctx = container_of(ctx, struct perf_cpu_context, ctx);
< 
< 	if (--ctx->nr_cgroups)
< 		return;
< 
< 	if (ctx->is_active && cpuctx->cgrp)
---
> 	} else {
> 		list_del(cpuctx_entry);
1045,1046c960
< 
< 	list_del(&cpuctx->cgrp_cpuctx_entry);
---
> 	}
1096c1010
< static inline void
---
> void
1112c1026,1027
< perf_cgroup_event_enable(struct perf_event *event, struct perf_event_context *ctx)
---
> list_update_cgroup_event(struct perf_event *event,
> 			 struct perf_event_context *ctx, bool add)
1116,1119d1030
< static inline void
< perf_cgroup_event_disable(struct perf_event *event, struct perf_event_context *ctx)
< {
< }
1133c1044
< 	bool rotations;
---
> 	int rotations = 0;
1171c1082
< 	hrtimer_init(timer, CLOCK_MONOTONIC, HRTIMER_MODE_ABS_PINNED_HARD);
---
> 	hrtimer_init(timer, CLOCK_MONOTONIC, HRTIMER_MODE_ABS_PINNED);
1189c1100
< 		hrtimer_start_expires(timer, HRTIMER_MODE_ABS_PINNED_HARD);
---
> 		hrtimer_start_expires(timer, HRTIMER_MODE_ABS_PINNED);
1240,1254c1151
< 	refcount_inc(&ctx->refcount);
< }
< 
< static void *alloc_task_ctx_data(struct pmu *pmu)
< {
< 	if (pmu->task_ctx_cache)
< 		return kmem_cache_zalloc(pmu->task_ctx_cache, GFP_KERNEL);
< 
< 	return NULL;
< }
< 
< static void free_task_ctx_data(struct pmu *pmu, void *task_ctx_data)
< {
< 	if (pmu->task_ctx_cache && task_ctx_data)
< 		kmem_cache_free(pmu->task_ctx_cache, task_ctx_data);
---
> 	WARN_ON(!atomic_inc_not_zero(&ctx->refcount));
1262c1159
< 	free_task_ctx_data(ctx->pmu, ctx->task_ctx_data);
---
> 	kfree(ctx->task_ctx_data);
1268c1165
< 	if (refcount_dec_and_test(&ctx->refcount)) {
---
> 	if (atomic_dec_and_test(&ctx->refcount)) {
1304c1201
<  * But remember that these are parent<->child context relations, and
---
>  * But remember that that these are parent<->child context relations, and
1330c1227
<  *    exec_update_lock
---
>  *    cred_guard_mutex
1336,1337c1233
<  *	    mmap_lock
<  *	      perf_addr_filters_head::lock
---
>  *	    mmap_sem
1351c1247
< 	if (!refcount_inc_not_zero(&ctx->refcount)) {
---
> 	if (!atomic_inc_not_zero(&ctx->refcount)) {
1418c1314
< 	return perf_event_pid_type(event, p, PIDTYPE_TGID);
---
> 	return perf_event_pid_type(event, p, __PIDTYPE_TGID);
1443c1339
<  * This has to cope with the fact that until it is locked,
---
>  * This has to cope with with the fact that until it is locked,
1484c1380
< 		    !refcount_inc_not_zero(&ctx->refcount)) {
---
> 		    !atomic_inc_not_zero(&ctx->refcount)) {
1567,1581c1463,1464
< /*
<  * Helper function to initialize event group nodes.
<  */
< static void init_event_group(struct perf_event *event)
< {
< 	RB_CLEAR_NODE(&event->group_node);
< 	event->group_index = 0;
< }
< 
< /*
<  * Extract pinned or flexible groups from the context
<  * based on event attrs bits.
<  */
< static struct perf_event_groups *
< get_event_groups(struct perf_event *event, struct perf_event_context *ctx)
---
> static struct list_head *
> ctx_group_list(struct perf_event *event, struct perf_event_context *ctx)
1590,1786c1473
<  * Helper function to initializes perf_event_group trees.
<  */
< static void perf_event_groups_init(struct perf_event_groups *groups)
< {
< 	groups->tree = RB_ROOT;
< 	groups->index = 0;
< }
< 
< static inline struct cgroup *event_cgroup(const struct perf_event *event)
< {
< 	struct cgroup *cgroup = NULL;
< 
< #ifdef CONFIG_CGROUP_PERF
< 	if (event->cgrp)
< 		cgroup = event->cgrp->css.cgroup;
< #endif
< 
< 	return cgroup;
< }
< 
< /*
<  * Compare function for event groups;
<  *
<  * Implements complex key that first sorts by CPU and then by virtual index
<  * which provides ordering when rotating groups for the same CPU.
<  */
< static __always_inline int
< perf_event_groups_cmp(const int left_cpu, const struct cgroup *left_cgroup,
< 		      const u64 left_group_index, const struct perf_event *right)
< {
< 	if (left_cpu < right->cpu)
< 		return -1;
< 	if (left_cpu > right->cpu)
< 		return 1;
< 
< #ifdef CONFIG_CGROUP_PERF
< 	{
< 		const struct cgroup *right_cgroup = event_cgroup(right);
< 
< 		if (left_cgroup != right_cgroup) {
< 			if (!left_cgroup) {
< 				/*
< 				 * Left has no cgroup but right does, no
< 				 * cgroups come first.
< 				 */
< 				return -1;
< 			}
< 			if (!right_cgroup) {
< 				/*
< 				 * Right has no cgroup but left does, no
< 				 * cgroups come first.
< 				 */
< 				return 1;
< 			}
< 			/* Two dissimilar cgroups, order by id. */
< 			if (cgroup_id(left_cgroup) < cgroup_id(right_cgroup))
< 				return -1;
< 
< 			return 1;
< 		}
< 	}
< #endif
< 
< 	if (left_group_index < right->group_index)
< 		return -1;
< 	if (left_group_index > right->group_index)
< 		return 1;
< 
< 	return 0;
< }
< 
< #define __node_2_pe(node) \
< 	rb_entry((node), struct perf_event, group_node)
< 
< static inline bool __group_less(struct rb_node *a, const struct rb_node *b)
< {
< 	struct perf_event *e = __node_2_pe(a);
< 	return perf_event_groups_cmp(e->cpu, event_cgroup(e), e->group_index,
< 				     __node_2_pe(b)) < 0;
< }
< 
< struct __group_key {
< 	int cpu;
< 	struct cgroup *cgroup;
< };
< 
< static inline int __group_cmp(const void *key, const struct rb_node *node)
< {
< 	const struct __group_key *a = key;
< 	const struct perf_event *b = __node_2_pe(node);
< 
< 	/* partial/subtree match: @cpu, @cgroup; ignore: @group_index */
< 	return perf_event_groups_cmp(a->cpu, a->cgroup, b->group_index, b);
< }
< 
< /*
<  * Insert @event into @groups' tree; using {@event->cpu, ++@groups->index} for
<  * key (see perf_event_groups_less). This places it last inside the CPU
<  * subtree.
<  */
< static void
< perf_event_groups_insert(struct perf_event_groups *groups,
< 			 struct perf_event *event)
< {
< 	event->group_index = ++groups->index;
< 
< 	rb_add(&event->group_node, &groups->tree, __group_less);
< }
< 
< /*
<  * Helper function to insert event into the pinned or flexible groups.
<  */
< static void
< add_event_to_groups(struct perf_event *event, struct perf_event_context *ctx)
< {
< 	struct perf_event_groups *groups;
< 
< 	groups = get_event_groups(event, ctx);
< 	perf_event_groups_insert(groups, event);
< }
< 
< /*
<  * Delete a group from a tree.
<  */
< static void
< perf_event_groups_delete(struct perf_event_groups *groups,
< 			 struct perf_event *event)
< {
< 	WARN_ON_ONCE(RB_EMPTY_NODE(&event->group_node) ||
< 		     RB_EMPTY_ROOT(&groups->tree));
< 
< 	rb_erase(&event->group_node, &groups->tree);
< 	init_event_group(event);
< }
< 
< /*
<  * Helper function to delete event from its groups.
<  */
< static void
< del_event_from_groups(struct perf_event *event, struct perf_event_context *ctx)
< {
< 	struct perf_event_groups *groups;
< 
< 	groups = get_event_groups(event, ctx);
< 	perf_event_groups_delete(groups, event);
< }
< 
< /*
<  * Get the leftmost event in the cpu/cgroup subtree.
<  */
< static struct perf_event *
< perf_event_groups_first(struct perf_event_groups *groups, int cpu,
< 			struct cgroup *cgrp)
< {
< 	struct __group_key key = {
< 		.cpu = cpu,
< 		.cgroup = cgrp,
< 	};
< 	struct rb_node *node;
< 
< 	node = rb_find_first(&key, &groups->tree, __group_cmp);
< 	if (node)
< 		return __node_2_pe(node);
< 
< 	return NULL;
< }
< 
< /*
<  * Like rb_entry_next_safe() for the @cpu subtree.
<  */
< static struct perf_event *
< perf_event_groups_next(struct perf_event *event)
< {
< 	struct __group_key key = {
< 		.cpu = event->cpu,
< 		.cgroup = event_cgroup(event),
< 	};
< 	struct rb_node *next;
< 
< 	next = rb_next_match(&key, &event->group_node, __group_cmp);
< 	if (next)
< 		return __node_2_pe(next);
< 
< 	return NULL;
< }
< 
< /*
<  * Iterate through the whole groups tree.
<  */
< #define perf_event_groups_for_each(event, groups)			\
< 	for (event = rb_entry_safe(rb_first(&((groups)->tree)),		\
< 				typeof(*event), group_node); event;	\
< 		event = rb_entry_safe(rb_next(&event->group_node),	\
< 				typeof(*event), group_node))
< 
< /*
<  * Add an event from the lists for its context.
---
>  * Add a event from the lists for its context.
1804a1492,1493
> 		struct list_head *list;
> 
1806c1495,1497
< 		add_event_to_groups(event, ctx);
---
> 
> 		list = ctx_group_list(event, ctx);
> 		list_add_tail(&event->group_entry, list);
1808a1500,1501
> 	list_update_cgroup_event(event, ctx, true);
> 
1814,1816d1506
< 	if (event->state > PERF_EVENT_STATE_OFF)
< 		perf_cgroup_event_enable(event, ctx);
< 
1867,1868c1557,1558
< 	if (sample_type & PERF_SAMPLE_WEIGHT_TYPE)
< 		size += sizeof(data->weight.full);
---
> 	if (sample_type & PERF_SAMPLE_WEIGHT)
> 		size += sizeof(data->weight);
1882,1890d1571
< 	if (sample_type & PERF_SAMPLE_CGROUP)
< 		size += sizeof(data->cgroup);
< 
< 	if (sample_type & PERF_SAMPLE_DATA_PAGE_SIZE)
< 		size += sizeof(data->data_page_size);
< 
< 	if (sample_type & PERF_SAMPLE_CODE_PAGE_SIZE)
< 		size += sizeof(data->code_page_size);
< 
1974c1655
< 	list_add_tail(&event->sibling_list, &group_leader->sibling_list);
---
> 	list_add_tail(&event->group_entry, &group_leader->sibling_list);
1979c1660
< 	for_each_sibling_event(pos, group_leader)
---
> 	list_for_each_entry(pos, &group_leader->sibling_list, group_entry)
1984c1665
<  * Remove an event from the lists for its context.
---
>  * Remove a event from the lists for its context.
2000a1682,1683
> 	list_update_cgroup_event(event, ctx, false);
> 
2008c1691
< 		del_event_from_groups(event, ctx);
---
> 		list_del_init(&event->group_entry);
2017,2018c1700
< 	if (event->state > PERF_EVENT_STATE_OFF) {
< 		perf_cgroup_event_disable(event, ctx);
---
> 	if (event->state > PERF_EVENT_STATE_OFF)
2020d1701
< 	}
2025,2143d1705
< static int
< perf_aux_output_match(struct perf_event *event, struct perf_event *aux_event)
< {
< 	if (!has_aux(aux_event))
< 		return 0;
< 
< 	if (!event->pmu->aux_output_match)
< 		return 0;
< 
< 	return event->pmu->aux_output_match(aux_event);
< }
< 
< static void put_event(struct perf_event *event);
< static void event_sched_out(struct perf_event *event,
< 			    struct perf_cpu_context *cpuctx,
< 			    struct perf_event_context *ctx);
< 
< static void perf_put_aux_event(struct perf_event *event)
< {
< 	struct perf_event_context *ctx = event->ctx;
< 	struct perf_cpu_context *cpuctx = __get_cpu_context(ctx);
< 	struct perf_event *iter;
< 
< 	/*
< 	 * If event uses aux_event tear down the link
< 	 */
< 	if (event->aux_event) {
< 		iter = event->aux_event;
< 		event->aux_event = NULL;
< 		put_event(iter);
< 		return;
< 	}
< 
< 	/*
< 	 * If the event is an aux_event, tear down all links to
< 	 * it from other events.
< 	 */
< 	for_each_sibling_event(iter, event->group_leader) {
< 		if (iter->aux_event != event)
< 			continue;
< 
< 		iter->aux_event = NULL;
< 		put_event(event);
< 
< 		/*
< 		 * If it's ACTIVE, schedule it out and put it into ERROR
< 		 * state so that we don't try to schedule it again. Note
< 		 * that perf_event_enable() will clear the ERROR status.
< 		 */
< 		event_sched_out(iter, cpuctx, ctx);
< 		perf_event_set_state(event, PERF_EVENT_STATE_ERROR);
< 	}
< }
< 
< static bool perf_need_aux_event(struct perf_event *event)
< {
< 	return !!event->attr.aux_output || !!event->attr.aux_sample_size;
< }
< 
< static int perf_get_aux_event(struct perf_event *event,
< 			      struct perf_event *group_leader)
< {
< 	/*
< 	 * Our group leader must be an aux event if we want to be
< 	 * an aux_output. This way, the aux event will precede its
< 	 * aux_output events in the group, and therefore will always
< 	 * schedule first.
< 	 */
< 	if (!group_leader)
< 		return 0;
< 
< 	/*
< 	 * aux_output and aux_sample_size are mutually exclusive.
< 	 */
< 	if (event->attr.aux_output && event->attr.aux_sample_size)
< 		return 0;
< 
< 	if (event->attr.aux_output &&
< 	    !perf_aux_output_match(event, group_leader))
< 		return 0;
< 
< 	if (event->attr.aux_sample_size && !group_leader->pmu->snapshot_aux)
< 		return 0;
< 
< 	if (!atomic_long_inc_not_zero(&group_leader->refcount))
< 		return 0;
< 
< 	/*
< 	 * Link aux_outputs to their aux event; this is undone in
< 	 * perf_group_detach() by perf_put_aux_event(). When the
< 	 * group in torn down, the aux_output events loose their
< 	 * link to the aux_event and can't schedule any more.
< 	 */
< 	event->aux_event = group_leader;
< 
< 	return 1;
< }
< 
< static inline struct list_head *get_event_list(struct perf_event *event)
< {
< 	struct perf_event_context *ctx = event->ctx;
< 	return event->attr.pinned ? &ctx->pinned_active : &ctx->flexible_active;
< }
< 
< /*
<  * Events that have PERF_EV_CAP_SIBLING require being part of a group and
<  * cannot exist on their own, schedule them out and move them into the ERROR
<  * state. Also see _perf_event_enable(), it will not be able to recover
<  * this ERROR state.
<  */
< static inline void perf_remove_sibling_event(struct perf_event *event)
< {
< 	struct perf_event_context *ctx = event->ctx;
< 	struct perf_cpu_context *cpuctx = __get_cpu_context(ctx);
< 
< 	event_sched_out(event, cpuctx, ctx);
< 	perf_event_set_state(event, PERF_EVENT_STATE_ERROR);
< }
< 
2146d1707
< 	struct perf_event *leader = event->group_leader;
2148c1709
< 	struct perf_event_context *ctx = event->ctx;
---
> 	struct list_head *list = NULL;
2150c1711
< 	lockdep_assert_held(&ctx->lock);
---
> 	lockdep_assert_held(&event->ctx->lock);
2160,2161d1720
< 	perf_put_aux_event(event);
< 
2165,2166c1724,1725
< 	if (leader != event) {
< 		list_del_init(&event->sibling_list);
---
> 	if (event->group_leader != event) {
> 		list_del_init(&event->group_entry);
2170a1730,1732
> 	if (!list_empty(&event->group_entry))
> 		list = &event->group_entry;
> 
2176,2180c1738,1740
< 	list_for_each_entry_safe(sibling, tmp, &event->sibling_list, sibling_list) {
< 
< 		if (sibling->event_caps & PERF_EV_CAP_SIBLING)
< 			perf_remove_sibling_event(sibling);
< 
---
> 	list_for_each_entry_safe(sibling, tmp, &event->sibling_list, group_entry) {
> 		if (list)
> 			list_move_tail(&sibling->group_entry, list);
2182d1741
< 		list_del_init(&sibling->sibling_list);
2187,2193d1745
< 		if (!RB_EMPTY_NODE(&event->group_node)) {
< 			add_event_to_groups(sibling, event->ctx);
< 
< 			if (sibling->state == PERF_EVENT_STATE_ACTIVE)
< 				list_add_tail(&sibling->active_list, get_event_list(sibling));
< 		}
< 
2198,2199c1750
< 	for_each_sibling_event(tmp, leader)
< 		perf_event__header_size(tmp);
---
> 	perf_event__header_size(event->group_leader);
2201,2221c1752,1753
< 	perf_event__header_size(leader);
< }
< 
< static void sync_child_event(struct perf_event *child_event);
< 
< static void perf_child_detach(struct perf_event *event)
< {
< 	struct perf_event *parent_event = event->parent;
< 
< 	if (!(event->attach_state & PERF_ATTACH_CHILD))
< 		return;
< 
< 	event->attach_state &= ~PERF_ATTACH_CHILD;
< 
< 	if (WARN_ON_ONCE(!parent_event))
< 		return;
< 
< 	lockdep_assert_held(&parent_event->child_mutex);
< 
< 	sync_child_event(event);
< 	list_del_init(&event->child_list);
---
> 	list_for_each_entry(tmp, &event->group_leader->sibling_list, group_entry)
> 		perf_event__header_size(tmp);
2243c1775
< 	struct perf_event *sibling;
---
> 	struct perf_event *child;
2248,2249c1780,1781
< 	for_each_sibling_event(sibling, event) {
< 		if (!__pmu_filter_match(sibling))
---
> 	list_for_each_entry(child, &event->sibling_list, group_entry) {
> 		if (!__pmu_filter_match(child))
2276,2282d1807
< 	/*
< 	 * Asymmetry; we only schedule events _IN_ through ctx_sched_in(), but
< 	 * we can schedule events _OUT_ individually through things like
< 	 * __perf_remove_from_context().
< 	 */
< 	list_del_init(&event->active_list);
< 
2288,2290c1813,1814
< 	if (READ_ONCE(event->pending_disable) >= 0) {
< 		WRITE_ONCE(event->pending_disable, -1);
< 		perf_cgroup_event_disable(event, ctx);
---
> 	if (event->pending_disable) {
> 		event->pending_disable = 0;
2324c1848
< 	for_each_sibling_event(event, group_event)
---
> 	list_for_each_entry(event, &group_event->sibling_list, group_entry)
2327a1852,1854
> 
> 	if (group_event->attr.exclusive)
> 		cpuctx->exclusive = 0;
2331d1857
< #define DETACH_CHILD	0x02UL
2355,2356d1880
< 	if (flags & DETACH_CHILD)
< 		perf_child_detach(event);
2361d1884
< 		ctx->rotate_necessary = 0;
2384a1908,1909
> 	event_function_call(event, __perf_remove_from_context, (void *)flags);
> 
2386,2388c1911,1914
< 	 * Because of perf_event_exit_task(), perf_remove_from_context() ought
< 	 * to work in the face of TASK_TOMBSTONE, unlike every other
< 	 * event_function_call() user.
---
> 	 * The above event_function_call() can NO-OP when it hits
> 	 * TASK_TOMBSTONE. In that case we must already have been detached
> 	 * from the context (by perf_event_exit_event()) but the grouping
> 	 * might still be in-tact.
2390,2393c1916,1924
< 	raw_spin_lock_irq(&ctx->lock);
< 	if (!ctx->is_active) {
< 		__perf_remove_from_context(event, __get_cpu_context(ctx),
< 					   ctx, (void *)flags);
---
> 	WARN_ON_ONCE(event->attach_state & PERF_ATTACH_CONTEXT);
> 	if ((flags & DETACH_GROUP) &&
> 	    (event->attach_state & PERF_ATTACH_GROUP)) {
> 		/*
> 		 * Since in that case we cannot possibly be scheduled, simply
> 		 * detach now.
> 		 */
> 		raw_spin_lock_irq(&ctx->lock);
> 		perf_group_detach(event);
2395d1925
< 		return;
2397,2399d1926
< 	raw_spin_unlock_irq(&ctx->lock);
< 
< 	event_function_call(event, __perf_remove_from_context, (void *)flags);
2424d1950
< 	perf_cgroup_event_disable(event, ctx);
2428c1954
<  * Disable an event.
---
>  * Disable a event.
2432c1958
<  * remains valid.  This condition is satisfied when called through
---
>  * remains valid.  This condition is satisifed when called through
2476,2477c2002
< 	WRITE_ONCE(event->pending_disable, smp_processor_id());
< 	/* can fail, see perf_pending_event_disable() */
---
> 	event->pending_disable = 1;
2506c2031
< 	 * within the time source all along. We believe it
---
> 	 * within the time time source all along. We believe it
2527,2528d2051
< 	WARN_ON_ONCE(event->ctx != ctx);
< 
2595,2596c2118,2122
< 	if (event_sched_in(group_event, cpuctx, ctx))
< 		goto error;
---
> 	if (event_sched_in(group_event, cpuctx, ctx)) {
> 		pmu->cancel_txn(pmu);
> 		perf_mux_hrtimer_restart(cpuctx);
> 		return -EAGAIN;
> 	}
2601c2127
< 	for_each_sibling_event(event, group_event) {
---
> 	list_for_each_entry(event, &group_event->sibling_list, group_entry) {
2617c2143
< 	for_each_sibling_event(event, group_event) {
---
> 	list_for_each_entry(event, &group_event->sibling_list, group_entry) {
2625d2150
< error:
2626a2152,2154
> 
> 	perf_mux_hrtimer_restart(cpuctx);
> 
2652c2180
< 	if (event->attr.exclusive && !list_empty(get_event_list(event)))
---
> 	if (event->attr.exclusive && cpuctx->active_oncpu)
2721c2249
< 	enum event_type_t ctx_event_type;
---
> 	enum event_type_t ctx_event_type = event_type & EVENT_ALL;
2731,2732d2258
< 	ctx_event_type = event_type & EVENT_ALL;
< 
2753,2762d2278
< void perf_pmu_resched(struct pmu *pmu)
< {
< 	struct perf_cpu_context *cpuctx = this_cpu_ptr(pmu->pmu_cpu_context);
< 	struct perf_event_context *task_ctx = cpuctx->task_ctx;
< 
< 	perf_ctx_lock(cpuctx, task_ctx);
< 	ctx_resched(cpuctx, task_ctx, EVENT_ALL|EVENT_CPU);
< 	perf_ctx_unlock(cpuctx, task_ctx);
< }
< 
2802,2813d2317
< #ifdef CONFIG_CGROUP_PERF
< 	if (event->state > PERF_EVENT_STATE_OFF && is_cgroup_event(event)) {
< 		/*
< 		 * If the current cgroup doesn't match the event's
< 		 * cgroup, we should not try to schedule it.
< 		 */
< 		struct perf_cgroup *cgrp = perf_cgroup_from_task(current, ctx);
< 		reprogram = cgroup_is_descendant(cgrp->css.cgroup,
< 					event->cgrp->css.cgroup);
< 	}
< #endif
< 
2828,2830d2331
< static bool exclusive_event_installable(struct perf_event *event,
< 					struct perf_event_context *ctx);
< 
2845,2846d2345
< 	WARN_ON_ONCE(!exclusive_event_installable(event, ctx));
< 
2856,2874d2354
< 	/*
< 	 * perf_event_attr::disabled events will not run and can be initialized
< 	 * without IPI. Except when this is the first event for the context, in
< 	 * that case we need the magic of the IPI to set ctx->is_active.
< 	 *
< 	 * The IOC_ENABLE that is sure to follow the creation of a disabled
< 	 * event will issue the IPI and reprogram the hardware.
< 	 */
< 	if (__perf_effective_state(event) == PERF_EVENT_STATE_OFF && ctx->nr_events) {
< 		raw_spin_lock_irq(&ctx->lock);
< 		if (ctx->task == TASK_TOMBSTONE) {
< 			raw_spin_unlock_irq(&ctx->lock);
< 			return;
< 		}
< 		add_event_to_ctx(event, ctx);
< 		raw_spin_unlock_irq(&ctx->lock);
< 		return;
< 	}
< 
2963d2442
< 	perf_cgroup_event_enable(event, ctx);
2990c2469
<  * Enable an event.
---
>  * Enable a event.
3005d2483
< out:
3017,3024c2495
< 	if (event->state == PERF_EVENT_STATE_ERROR) {
< 		/*
< 		 * Detached SIBLING events cannot leave ERROR state.
< 		 */
< 		if (event->event_caps & PERF_EV_CAP_SIBLING &&
< 		    event->group_leader == event)
< 			goto out;
< 
---
> 	if (event->state == PERF_EVENT_STATE_ERROR)
3026d2496
< 	}
3077c2547
< 	 * Since this is happening on an event-local CPU, no trace is lost
---
> 	 * Since this is happening on a event-local CPU, no trace is lost
3119c2589
<  *	event::addr_filter_ranges array and bump the event::addr_filters_gen;
---
>  *	event::addr_filters_offs array and bump the event::addr_filters_gen;
3130c2600
<  *     registered mapping, called for every new mmap(), with mm::mmap_lock down
---
>  *     registered mapping, called for every new mmap(), with mm::mmap_sem down
3181,3230d2650
< static int perf_event_modify_breakpoint(struct perf_event *bp,
< 					 struct perf_event_attr *attr)
< {
< 	int err;
< 
< 	_perf_event_disable(bp);
< 
< 	err = modify_user_hw_breakpoint_check(bp, attr, true);
< 
< 	if (!bp->attr.disabled)
< 		_perf_event_enable(bp);
< 
< 	return err;
< }
< 
< static int perf_event_modify_attr(struct perf_event *event,
< 				  struct perf_event_attr *attr)
< {
< 	int (*func)(struct perf_event *, struct perf_event_attr *);
< 	struct perf_event *child;
< 	int err;
< 
< 	if (event->attr.type != attr->type)
< 		return -EINVAL;
< 
< 	switch (event->attr.type) {
< 	case PERF_TYPE_BREAKPOINT:
< 		func = perf_event_modify_breakpoint;
< 		break;
< 	default:
< 		/* Place holder for future additions. */
< 		return -EOPNOTSUPP;
< 	}
< 
< 	WARN_ON_ONCE(event->ctx->parent_ctx);
< 
< 	mutex_lock(&event->child_mutex);
< 	err = func(event, attr);
< 	if (err)
< 		goto out;
< 	list_for_each_entry(child, &event->child_list, child_list) {
< 		err = func(child, attr);
< 		if (err)
< 			goto out;
< 	}
< out:
< 	mutex_unlock(&event->child_mutex);
< 	return err;
< }
< 
3235d2654
< 	struct perf_event *event, *tmp;
3236a2656
> 	struct perf_event *event;
3283c2703
< 		list_for_each_entry_safe(event, tmp, &ctx->pinned_active, active_list)
---
> 		list_for_each_entry(event, &ctx->pinned_groups, group_entry)
3288c2708
< 		list_for_each_entry_safe(event, tmp, &ctx->flexible_active, active_list)
---
> 		list_for_each_entry(event, &ctx->flexible_groups, group_entry)
3290,3296d2709
< 
< 		/*
< 		 * Since we cleared EVENT_FLEXIBLE, also clear
< 		 * rotate_necessary, is will be reset by
< 		 * ctx_flexible_sched_in() when needed.
< 		 */
< 		ctx->rotate_necessary = 0;
3411d2823
< 	struct pmu *pmu;
3416d2827
< 	pmu = ctx->pmu;
3446d2856
< 
3450,3466c2860
< 			perf_pmu_disable(pmu);
< 
< 			if (cpuctx->sched_cb_usage && pmu->sched_task)
< 				pmu->sched_task(ctx, false);
< 
< 			/*
< 			 * PMU specific parts of task perf context can require
< 			 * additional synchronization. As an example of such
< 			 * synchronization see implementation details of Intel
< 			 * LBR call stack data profiling;
< 			 */
< 			if (pmu->swap_task_ctx)
< 				pmu->swap_task_ctx(ctx, next_ctx);
< 			else
< 				swap(ctx->task_ctx_data, next_ctx->task_ctx_data);
< 
< 			perf_pmu_enable(pmu);
---
> 			swap(ctx->task_ctx_data, next_ctx->task_ctx_data);
3490,3493d2883
< 		perf_pmu_disable(pmu);
< 
< 		if (cpuctx->sched_cb_usage && pmu->sched_task)
< 			pmu->sched_task(ctx, false);
3495,3496d2884
< 
< 		perf_pmu_enable(pmu);
3532,3549d2919
< static void __perf_pmu_sched_task(struct perf_cpu_context *cpuctx, bool sched_in)
< {
< 	struct pmu *pmu;
< 
< 	pmu = cpuctx->ctx.pmu; /* software PMUs will not have sched_task */
< 
< 	if (WARN_ON_ONCE(!pmu->sched_task))
< 		return;
< 
< 	perf_ctx_lock(cpuctx, cpuctx->task_ctx);
< 	perf_pmu_disable(pmu);
< 
< 	pmu->sched_task(cpuctx->task_ctx, sched_in);
< 
< 	perf_pmu_enable(pmu);
< 	perf_ctx_unlock(cpuctx, cpuctx->task_ctx);
< }
< 
3554a2925
> 	struct pmu *pmu;
3560,3561c2931,2933
< 		/* will be handled in perf_event_context_sched_in/out */
< 		if (cpuctx->task_ctx)
---
> 		pmu = cpuctx->ctx.pmu; /* software PMUs will not have sched_task */
> 
> 		if (WARN_ON_ONCE(!pmu->sched_task))
3564c2936,2942
< 		__perf_pmu_sched_task(cpuctx, sched_in);
---
> 		perf_ctx_lock(cpuctx, cpuctx->task_ctx);
> 		perf_pmu_disable(pmu);
> 
> 		pmu->sched_task(cpuctx->task_ctx, sched_in);
> 
> 		perf_pmu_enable(pmu);
> 		perf_ctx_unlock(cpuctx, cpuctx->task_ctx);
3617,3722c2995,2997
< static bool perf_less_group_idx(const void *l, const void *r)
< {
< 	const struct perf_event *le = *(const struct perf_event **)l;
< 	const struct perf_event *re = *(const struct perf_event **)r;
< 
< 	return le->group_index < re->group_index;
< }
< 
< static void swap_ptr(void *l, void *r)
< {
< 	void **lp = l, **rp = r;
< 
< 	swap(*lp, *rp);
< }
< 
< static const struct min_heap_callbacks perf_min_heap = {
< 	.elem_size = sizeof(struct perf_event *),
< 	.less = perf_less_group_idx,
< 	.swp = swap_ptr,
< };
< 
< static void __heap_add(struct min_heap *heap, struct perf_event *event)
< {
< 	struct perf_event **itrs = heap->data;
< 
< 	if (event) {
< 		itrs[heap->nr] = event;
< 		heap->nr++;
< 	}
< }
< 
< static noinline int visit_groups_merge(struct perf_cpu_context *cpuctx,
< 				struct perf_event_groups *groups, int cpu,
< 				int (*func)(struct perf_event *, void *),
< 				void *data)
< {
< #ifdef CONFIG_CGROUP_PERF
< 	struct cgroup_subsys_state *css = NULL;
< #endif
< 	/* Space for per CPU and/or any CPU event iterators. */
< 	struct perf_event *itrs[2];
< 	struct min_heap event_heap;
< 	struct perf_event **evt;
< 	int ret;
< 
< 	if (cpuctx) {
< 		event_heap = (struct min_heap){
< 			.data = cpuctx->heap,
< 			.nr = 0,
< 			.size = cpuctx->heap_size,
< 		};
< 
< 		lockdep_assert_held(&cpuctx->ctx.lock);
< 
< #ifdef CONFIG_CGROUP_PERF
< 		if (cpuctx->cgrp)
< 			css = &cpuctx->cgrp->css;
< #endif
< 	} else {
< 		event_heap = (struct min_heap){
< 			.data = itrs,
< 			.nr = 0,
< 			.size = ARRAY_SIZE(itrs),
< 		};
< 		/* Events not within a CPU context may be on any CPU. */
< 		__heap_add(&event_heap, perf_event_groups_first(groups, -1, NULL));
< 	}
< 	evt = event_heap.data;
< 
< 	__heap_add(&event_heap, perf_event_groups_first(groups, cpu, NULL));
< 
< #ifdef CONFIG_CGROUP_PERF
< 	for (; css; css = css->parent)
< 		__heap_add(&event_heap, perf_event_groups_first(groups, cpu, css->cgroup));
< #endif
< 
< 	min_heapify_all(&event_heap, &perf_min_heap);
< 
< 	while (event_heap.nr) {
< 		ret = func(*evt, data);
< 		if (ret)
< 			return ret;
< 
< 		*evt = perf_event_groups_next(*evt);
< 		if (*evt)
< 			min_heapify(&event_heap, 0, &perf_min_heap);
< 		else
< 			min_heap_pop(&event_heap, &perf_min_heap);
< 	}
< 
< 	return 0;
< }
< 
< static inline bool event_update_userpage(struct perf_event *event)
< {
< 	if (likely(!atomic_read(&event->mmap_count)))
< 		return false;
< 
< 	perf_event_update_time(event);
< 	perf_set_shadow_time(event, event->ctx);
< 	perf_event_update_userpage(event);
< 
< 	return true;
< }
< 
< static inline void group_update_userpage(struct perf_event *group_event)
---
> static void
> ctx_pinned_sched_in(struct perf_event_context *ctx,
> 		    struct perf_cpu_context *cpuctx)
3726,3743c3001,3005
< 	if (!event_update_userpage(group_event))
< 		return;
< 
< 	for_each_sibling_event(event, group_event)
< 		event_update_userpage(event);
< }
< 
< static int merge_sched_in(struct perf_event *event, void *data)
< {
< 	struct perf_event_context *ctx = event->ctx;
< 	struct perf_cpu_context *cpuctx = __get_cpu_context(ctx);
< 	int *can_add_hw = data;
< 
< 	if (event->state <= PERF_EVENT_STATE_OFF)
< 		return 0;
< 
< 	if (!event_filter_match(event))
< 		return 0;
---
> 	list_for_each_entry(event, &ctx->pinned_groups, group_entry) {
> 		if (event->state <= PERF_EVENT_STATE_OFF)
> 			continue;
> 		if (!event_filter_match(event))
> 			continue;
3745,3748c3007,3008
< 	if (group_can_go_on(event, cpuctx, *can_add_hw)) {
< 		if (!group_sched_in(event, cpuctx, ctx))
< 			list_add_tail(&event->active_list, get_event_list(event));
< 	}
---
> 		if (group_can_go_on(event, cpuctx, 1))
> 			group_sched_in(event, cpuctx, ctx);
3750,3753c3010,3014
< 	if (event->state == PERF_EVENT_STATE_INACTIVE) {
< 		*can_add_hw = 0;
< 		if (event->attr.pinned) {
< 			perf_cgroup_event_disable(event, ctx);
---
> 		/*
> 		 * If this pinned group hasn't been scheduled,
> 		 * put it in error state.
> 		 */
> 		if (event->state == PERF_EVENT_STATE_INACTIVE)
3755,3759d3015
< 		} else {
< 			ctx->rotate_necessary = 1;
< 			perf_mux_hrtimer_restart(cpuctx);
< 			group_update_userpage(event);
< 		}
3761,3776d3016
< 
< 	return 0;
< }
< 
< static void
< ctx_pinned_sched_in(struct perf_event_context *ctx,
< 		    struct perf_cpu_context *cpuctx)
< {
< 	int can_add_hw = 1;
< 
< 	if (ctx != &cpuctx->ctx)
< 		cpuctx = NULL;
< 
< 	visit_groups_merge(cpuctx, &ctx->pinned_groups,
< 			   smp_processor_id(),
< 			   merge_sched_in, &can_add_hw);
3782a3023
> 	struct perf_event *event;
3785,3786c3026,3035
< 	if (ctx != &cpuctx->ctx)
< 		cpuctx = NULL;
---
> 	list_for_each_entry(event, &ctx->flexible_groups, group_entry) {
> 		/* Ignore events in OFF or ERROR state */
> 		if (event->state <= PERF_EVENT_STATE_OFF)
> 			continue;
> 		/*
> 		 * Listen to the 'cpu' scheduling filter constraint
> 		 * of events:
> 		 */
> 		if (!event_filter_match(event))
> 			continue;
3788,3790c3037,3041
< 	visit_groups_merge(cpuctx, &ctx->flexible_groups,
< 			   smp_processor_id(),
< 			   merge_sched_in, &can_add_hw);
---
> 		if (group_can_go_on(event, cpuctx, can_add_hw)) {
> 			if (group_sched_in(event, cpuctx, ctx))
> 				can_add_hw = 0;
> 		}
> 	}
3849d3099
< 	struct pmu *pmu;
3852,3861c3102
< 
< 	/*
< 	 * HACK: for HETEROGENEOUS the task context might have switched to a
< 	 * different PMU, force (re)set the context,
< 	 */
< 	pmu = ctx->pmu = cpuctx->ctx.pmu;
< 
< 	if (cpuctx->task_ctx == ctx) {
< 		if (cpuctx->sched_cb_usage)
< 			__perf_pmu_sched_task(cpuctx, true);
---
> 	if (cpuctx->task_ctx == ctx)
3863d3103
< 	}
3873c3113
< 	perf_pmu_disable(pmu);
---
> 	perf_pmu_disable(ctx->pmu);
3882c3122
< 	if (!RB_EMPTY_ROOT(&ctx->pinned_groups.tree))
---
> 	if (!list_empty(&ctx->pinned_groups))
3885,3889c3125
< 
< 	if (cpuctx->sched_cb_usage && pmu->sched_task)
< 		pmu->sched_task(cpuctx->task_ctx, true);
< 
< 	perf_pmu_enable(pmu);
---
> 	perf_pmu_enable(ctx->pmu);
4115c3351
<  * Move @event to the tail of the @ctx's elegible events.
---
>  * Round-robin a context's events:
4117c3353
< static void rotate_ctx(struct perf_event_context *ctx, struct perf_event *event)
---
> static void rotate_ctx(struct perf_event_context *ctx)
4123,4127c3359,3360
< 	if (ctx->rotate_disable)
< 		return;
< 
< 	perf_event_groups_delete(&ctx->flexible_groups, event);
< 	perf_event_groups_insert(&ctx->flexible_groups, event);
---
> 	if (!ctx->rotate_disable)
> 		list_rotate_left(&ctx->flexible_groups);
4130,4132c3363
< /* pick an event from the flexible_groups to rotate */
< static inline struct perf_event *
< ctx_event_to_rotate(struct perf_event_context *ctx)
---
> static int perf_rotate_context(struct perf_cpu_context *cpuctx)
4134,4138c3365,3366
< 	struct perf_event *event;
< 
< 	/* pick the first active flexible event */
< 	event = list_first_entry_or_null(&ctx->flexible_active,
< 					 struct perf_event, active_list);
---
> 	struct perf_event_context *ctx = NULL;
> 	int rotate = 0;
4140,4143c3368,3370
< 	/* if no active flexible event, pick the first event */
< 	if (!event) {
< 		event = rb_entry_safe(rb_first(&ctx->flexible_groups.tree),
< 				      typeof(*event), group_node);
---
> 	if (cpuctx->ctx.nr_events) {
> 		if (cpuctx->ctx.nr_events != cpuctx->ctx.nr_active)
> 			rotate = 1;
4146,4168c3373,3377
< 	/*
< 	 * Unconditionally clear rotate_necessary; if ctx_flexible_sched_in()
< 	 * finds there are unschedulable events, it will set it again.
< 	 */
< 	ctx->rotate_necessary = 0;
< 
< 	return event;
< }
< 
< static bool perf_rotate_context(struct perf_cpu_context *cpuctx)
< {
< 	struct perf_event *cpu_event = NULL, *task_event = NULL;
< 	struct perf_event_context *task_ctx = NULL;
< 	int cpu_rotate, task_rotate;
< 
< 	/*
< 	 * Since we run this from IRQ context, nobody can install new
< 	 * events, thus the event count values are stable.
< 	 */
< 
< 	cpu_rotate = cpuctx->ctx.rotate_necessary;
< 	task_ctx = cpuctx->task_ctx;
< 	task_rotate = task_ctx ? task_ctx->rotate_necessary : 0;
---
> 	ctx = cpuctx->task_ctx;
> 	if (ctx && ctx->nr_events) {
> 		if (ctx->nr_events != ctx->nr_active)
> 			rotate = 1;
> 	}
4170,4171c3379,3380
< 	if (!(cpu_rotate || task_rotate))
< 		return false;
---
> 	if (!rotate)
> 		goto done;
4176,4188c3385,3387
< 	if (task_rotate)
< 		task_event = ctx_event_to_rotate(task_ctx);
< 	if (cpu_rotate)
< 		cpu_event = ctx_event_to_rotate(&cpuctx->ctx);
< 
< 	/*
< 	 * As per the order given at ctx_resched() first 'pop' task flexible
< 	 * and then, if needed CPU flexible.
< 	 */
< 	if (task_event || (task_ctx && cpu_event))
< 		ctx_sched_out(task_ctx, cpuctx, EVENT_FLEXIBLE);
< 	if (cpu_event)
< 		cpu_ctx_sched_out(cpuctx, EVENT_FLEXIBLE);
---
> 	cpu_ctx_sched_out(cpuctx, EVENT_FLEXIBLE);
> 	if (ctx)
> 		ctx_sched_out(ctx, cpuctx, EVENT_FLEXIBLE);
4190,4193c3389,3391
< 	if (task_event)
< 		rotate_ctx(task_ctx, task_event);
< 	if (cpu_event)
< 		rotate_ctx(&cpuctx->ctx, cpu_event);
---
> 	rotate_ctx(&cpuctx->ctx);
> 	if (ctx)
> 		rotate_ctx(ctx);
4195c3393
< 	perf_event_sched_in(cpuctx, task_ctx, current);
---
> 	perf_event_sched_in(cpuctx, ctx, current);
4198a3397
> done:
4200c3399
< 	return true;
---
> 	return rotate;
4278,4328d3476
< static void perf_remove_from_owner(struct perf_event *event);
< static void perf_event_exit_event(struct perf_event *event,
< 				  struct perf_event_context *ctx);
< 
< /*
<  * Removes all events from the current task that have been marked
<  * remove-on-exec, and feeds their values back to parent events.
<  */
< static void perf_event_remove_on_exec(int ctxn)
< {
< 	struct perf_event_context *ctx, *clone_ctx = NULL;
< 	struct perf_event *event, *next;
< 	LIST_HEAD(free_list);
< 	unsigned long flags;
< 	bool modified = false;
< 
< 	ctx = perf_pin_task_context(current, ctxn);
< 	if (!ctx)
< 		return;
< 
< 	mutex_lock(&ctx->mutex);
< 
< 	if (WARN_ON_ONCE(ctx->task != current))
< 		goto unlock;
< 
< 	list_for_each_entry_safe(event, next, &ctx->event_list, event_entry) {
< 		if (!event->attr.remove_on_exec)
< 			continue;
< 
< 		if (!is_kernel_event(event))
< 			perf_remove_from_owner(event);
< 
< 		modified = true;
< 
< 		perf_event_exit_event(event, ctx);
< 	}
< 
< 	raw_spin_lock_irqsave(&ctx->lock, flags);
< 	if (modified)
< 		clone_ctx = unclone_ctx(ctx);
< 	--ctx->pin_count;
< 	raw_spin_unlock_irqrestore(&ctx->lock, flags);
< 
< unlock:
< 	mutex_unlock(&ctx->mutex);
< 
< 	put_ctx(ctx);
< 	if (clone_ctx)
< 		put_ctx(clone_ctx);
< }
< 
4396c3544
< 	for_each_sibling_event(sub, event) {
---
> 	list_for_each_entry(sub, &event->sibling_list, group_entry) {
4460,4465d3607
< 	/* If this is a pinned event it must be running on this CPU */
< 	if (event->attr.pinned && event->oncpu != smp_processor_id()) {
< 		ret = -EBUSY;
< 		goto out;
< 	}
< 
4576,4577c3718,3719
< 	perf_event_groups_init(&ctx->pinned_groups);
< 	perf_event_groups_init(&ctx->flexible_groups);
---
> 	INIT_LIST_HEAD(&ctx->pinned_groups);
> 	INIT_LIST_HEAD(&ctx->flexible_groups);
4579,4581c3721
< 	INIT_LIST_HEAD(&ctx->pinned_active);
< 	INIT_LIST_HEAD(&ctx->flexible_active);
< 	refcount_set(&ctx->refcount, 1);
---
> 	atomic_set(&ctx->refcount, 1);
4594,4595c3734,3737
< 	if (task)
< 		ctx->task = get_task_struct(task);
---
> 	if (task) {
> 		ctx->task = task;
> 		get_task_struct(task);
> 	}
4637,4639c3779,3780
< 		err = perf_allow_cpu(&event->attr);
< 		if (err)
< 			return ERR_PTR(err);
---
> 		if (perf_paranoid_cpu() && !capable(CAP_SYS_ADMIN))
> 			return ERR_PTR(-EACCES);
4644d3784
< 		raw_spin_lock_irqsave(&ctx->lock, flags);
4646d3785
< 		raw_spin_unlock_irqrestore(&ctx->lock, flags);
4657c3796
< 		task_ctx_data = alloc_task_ctx_data(pmu);
---
> 		task_ctx_data = kzalloc(pmu->task_ctx_size, GFP_KERNEL);
4715c3854
< 	free_task_ctx_data(pmu, task_ctx_data);
---
> 	kfree(task_ctx_data);
4719c3858
< 	free_task_ctx_data(pmu, task_ctx_data);
---
> 	kfree(task_ctx_data);
4723a3863
> static void perf_event_free_bpf_prog(struct perf_event *event);
4733c3873
< 	kmem_cache_free(perf_event_cache, event);
---
> 	kfree(event);
4737c3877
< 			       struct perf_buffer *rb);
---
> 			       struct ring_buffer *rb);
4760,4762c3900,3901
< 	    attr->task || attr->ksymbol ||
< 	    attr->context_switch || attr->text_poke ||
< 	    attr->bpf_event)
---
> 	    attr->task ||
> 	    attr->context_switch)
4811c3950
< 	if (event->attach_state & (PERF_ATTACH_TASK | PERF_ATTACH_SCHED_CB))
---
> 	if (event->attach_state & PERF_ATTACH_TASK)
4815,4816d3953
< 	if (event->attr.build_id)
< 		atomic_dec(&nr_build_id_events);
4821,4822d3957
< 	if (event->attr.cgroup)
< 		atomic_dec(&nr_cgroup_events);
4835,4840d3969
< 	if (event->attr.ksymbol)
< 		atomic_dec(&nr_ksymbol_events);
< 	if (event->attr.bpf_event)
< 		atomic_dec(&nr_bpf_events);
< 	if (event->attr.text_poke)
< 		atomic_dec(&nr_text_poke_events);
4876c4005
< 	if (!is_exclusive_pmu(pmu))
---
> 	if (!(pmu->capabilities & PERF_PMU_CAP_EXCLUSIVE))
4907c4036
< 	if (!is_exclusive_pmu(pmu))
---
> 	if (!(pmu->capabilities & PERF_PMU_CAP_EXCLUSIVE))
4926a4056
> /* Called under the same ctx::mutex as perf_install_in_context() */
4933,4935c4063
< 	lockdep_assert_held(&ctx->mutex);
< 
< 	if (!is_exclusive_pmu(pmu))
---
> 	if (!(pmu->capabilities & PERF_PMU_CAP_EXCLUSIVE))
4955,4956d4082
< 	security_perf_event_free(event);
< 
4979c4105
< 	kfree(event->addr_filter_ranges);
---
> 	kfree(event->addr_filters_offs);
4984,4994d4109
< 	/*
< 	 * Must be after ->destroy(), due to uprobe_perf_close() using
< 	 * hw.target.
< 	 */
< 	if (event->hw.target)
< 		put_task_struct(event->hw.target);
< 
< 	/*
< 	 * perf_event_free_task() relies on put_ctx() being 'last', in particular
< 	 * all task references must be cleaned up.
< 	 */
5177,5178d4291
< 		void *var = &child->ctx->refcount;
< 
5181,5187d4293
< 
< 		/*
< 		 * Wake any perf_event_free_task() waiting for this event to be
< 		 * freed.
< 		 */
< 		smp_mb(); /* pairs with wait_var_event() */
< 		wake_up_var(var);
5284c4390
< 	for_each_sibling_event(sub, leader) {
---
> 	list_for_each_entry(sub, &leader->sibling_list, group_entry) {
5384c4490
< 	 * Return end-of-file for a read on an event that is in
---
> 	 * Return end-of-file for a read on a event that is in
5410,5413d4515
< 	ret = security_perf_event_read(event);
< 	if (ret)
< 		return ret;
< 
5421c4523
< static __poll_t perf_poll(struct file *file, poll_table *wait)
---
> static unsigned int perf_poll(struct file *file, poll_table *wait)
5424,5425c4526,4527
< 	struct perf_buffer *rb;
< 	__poll_t events = EPOLLHUP;
---
> 	struct ring_buffer *rb;
> 	unsigned int events = POLLHUP;
5451,5468d4552
< /* Assume it's not an event with inherit set. */
< u64 perf_event_pause(struct perf_event *event, bool reset)
< {
< 	struct perf_event_context *ctx;
< 	u64 count;
< 
< 	ctx = perf_event_ctx_lock(event);
< 	WARN_ON_ONCE(event->attr.inherit);
< 	_perf_event_disable(event);
< 	count = local64_read(&event->count);
< 	if (reset)
< 		local64_set(&event->count, 0);
< 	perf_event_ctx_unlock(event, ctx);
< 
< 	return count;
< }
< EXPORT_SYMBOL_GPL(perf_event_pause);
< 
5500c4584
< 	for_each_sibling_event(sibling, event)
---
> 	list_for_each_entry(sibling, &event->sibling_list, group_entry)
5541c4625
< static int perf_event_check_period(struct perf_event *event, u64 value)
---
> static int perf_event_period(struct perf_event *event, u64 __user *arg)
5543,5544c4627
< 	return event->pmu->check_period(event, value);
< }
---
> 	u64 value;
5546,5547d4628
< static int _perf_event_period(struct perf_event *event, u64 value)
< {
5550a4632,4634
> 	if (copy_from_user(&value, arg, sizeof(value)))
> 		return -EFAULT;
> 
5557,5562d4640
< 	if (perf_event_check_period(event, value))
< 		return -EINVAL;
< 
< 	if (!event->attr.freq && (value & (1ULL << 63)))
< 		return -EINVAL;
< 
5568,5580d4645
< int perf_event_period(struct perf_event *event, u64 value)
< {
< 	struct perf_event_context *ctx;
< 	int ret;
< 
< 	ctx = perf_event_ctx_lock(event);
< 	ret = _perf_event_period(event, value);
< 	perf_event_ctx_unlock(event, ctx);
< 
< 	return ret;
< }
< EXPORT_SYMBOL_GPL(perf_event_period);
< 
5600,5601c4665
< static int perf_copy_attr(struct perf_event_attr __user *uattr,
< 			  struct perf_event_attr *attr);
---
> static int perf_event_set_bpf_prog(struct perf_event *event, u32 prog_fd);
5623,5624c4687
< 	{
< 		u64 value;
---
> 		return perf_event_period(event, (u64 __user *)arg);
5626,5630d4688
< 		if (copy_from_user(&value, (u64 __user *)arg, sizeof(value)))
< 			return -EFAULT;
< 
< 		return _perf_event_period(event, value);
< 	}
5662,5677c4720
< 	{
< 		struct bpf_prog *prog;
< 		int err;
< 
< 		prog = bpf_prog_get(arg);
< 		if (IS_ERR(prog))
< 			return PTR_ERR(prog);
< 
< 		err = perf_event_set_bpf_prog(event, prog, 0);
< 		if (err) {
< 			bpf_prog_put(prog);
< 			return err;
< 		}
< 
< 		return 0;
< 	}
---
> 		return perf_event_set_bpf_prog(event, arg);
5680c4723
< 		struct perf_buffer *rb;
---
> 		struct ring_buffer *rb;
5692,5705d4734
< 
< 	case PERF_EVENT_IOC_QUERY_BPF:
< 		return perf_event_query_prog_array(event, (void __user *)arg);
< 
< 	case PERF_EVENT_IOC_MODIFY_ATTRIBUTES: {
< 		struct perf_event_attr new_attr;
< 		int err = perf_copy_attr((struct perf_event_attr __user *)arg,
< 					 &new_attr);
< 
< 		if (err)
< 			return err;
< 
< 		return perf_event_modify_attr(event,  &new_attr);
< 	}
5724,5728d4752
< 	/* Treat ioctl like writes as it is likely a mutating operation. */
< 	ret = security_perf_event_write(event);
< 	if (ret)
< 		return ret;
< 
5743,5744d4766
< 	case _IOC_NR(PERF_EVENT_IOC_QUERY_BPF):
< 	case _IOC_NR(PERF_EVENT_IOC_MODIFY_ATTRIBUTES):
5816c4838
< 	struct perf_buffer *rb;
---
> 	struct ring_buffer *rb;
5848c4870
< 	struct perf_buffer *rb;
---
> 	struct ring_buffer *rb;
5869,5870c4891,4892
< 	 * Disable preemption to guarantee consistent time stamps are stored to
< 	 * the user page.
---
> 	 * Disable preemption so as to not let the corresponding user-space
> 	 * spin too long if we get preempted.
5894d4915
< EXPORT_SYMBOL_GPL(perf_event_update_userpage);
5896c4917
< static vm_fault_t perf_mmap_fault(struct vm_fault *vmf)
---
> static int perf_mmap_fault(struct vm_fault *vmf)
5899,5900c4920,4921
< 	struct perf_buffer *rb;
< 	vm_fault_t ret = VM_FAULT_SIGBUS;
---
> 	struct ring_buffer *rb;
> 	int ret = VM_FAULT_SIGBUS;
5932c4953
< 			       struct perf_buffer *rb)
---
> 			       struct ring_buffer *rb)
5934c4955
< 	struct perf_buffer *old_rb = NULL;
---
> 	struct ring_buffer *old_rb = NULL;
5992c5013
< 	struct perf_buffer *rb;
---
> 	struct ring_buffer *rb;
6003c5024
< struct perf_buffer *ring_buffer_get(struct perf_event *event)
---
> struct ring_buffer *ring_buffer_get(struct perf_event *event)
6005c5026
< 	struct perf_buffer *rb;
---
> 	struct ring_buffer *rb;
6010c5031
< 		if (!refcount_inc_not_zero(&rb->refcount))
---
> 		if (!atomic_inc_not_zero(&rb->refcount))
6018c5039
< void ring_buffer_put(struct perf_buffer *rb)
---
> void ring_buffer_put(struct ring_buffer *rb)
6020c5041
< 	if (!refcount_dec_and_test(&rb->refcount))
---
> 	if (!atomic_dec_and_test(&rb->refcount))
6055c5076,5077
< 	struct perf_buffer *rb = ring_buffer_get(event);
---
> 
> 	struct ring_buffer *rb = ring_buffer_get(event);
6059d5080
< 	bool detach_rest = false;
6080,6081c5101,5102
< 		atomic_long_sub(rb->aux_nr_pages - rb->aux_mmap_locked, &mmap_user->locked_vm);
< 		atomic64_sub(rb->aux_mmap_locked, &vma->vm_mm->pinned_vm);
---
> 		atomic_long_sub(rb->aux_nr_pages, &mmap_user->locked_vm);
> 		vma->vm_mm->pinned_vm -= rb->aux_mmap_locked;
6085c5106
< 		WARN_ON_ONCE(refcount_read(&rb->aux_refcount));
---
> 		WARN_ON_ONCE(atomic_read(&rb->aux_refcount));
6090,6091c5111
< 	if (atomic_dec_and_test(&rb->mmap_count))
< 		detach_rest = true;
---
> 	atomic_dec(&rb->mmap_count);
6100c5120
< 	if (!detach_rest)
---
> 	if (atomic_read(&rb->mmap_count))
6154,6156c5174,5175
< 	atomic_long_sub((size >> PAGE_SHIFT) + 1 - mmap_locked,
< 			&mmap_user->locked_vm);
< 	atomic64_sub(mmap_locked, &vma->vm_mm->pinned_vm);
---
> 	atomic_long_sub((size >> PAGE_SHIFT) + 1, &mmap_user->locked_vm);
> 	vma->vm_mm->pinned_vm -= mmap_locked;
6165c5184
< 	.close		= perf_mmap_close, /* non mergeable */
---
> 	.close		= perf_mmap_close, /* non mergable */
6175d5193
< 	struct perf_buffer *rb = NULL;
6176a5195
> 	struct ring_buffer *rb = NULL;
6193,6196d5211
< 	ret = security_perf_event_read(event);
< 	if (ret)
< 		return ret;
< 
6301c5316
< 	user_locked = atomic_long_read(&user->locked_vm);
---
> 	user_locked = atomic_long_read(&user->locked_vm) + user_extra;
6303,6306d5317
< 	/*
< 	 * sysctl_perf_event_mlock may have changed, so that
< 	 *     user->locked_vm > user_lock_limit
< 	 */
6308,6315d5318
< 		user_locked = user_lock_limit;
< 	user_locked += user_extra;
< 
< 	if (user_locked > user_lock_limit) {
< 		/*
< 		 * charge locked_vm until it hits user_lock_limit;
< 		 * charge the rest from pinned_vm
< 		 */
6317,6318d5319
< 		user_extra -= extra;
< 	}
6322c5323
< 	locked = atomic64_read(&vma->vm_mm->pinned_vm) + extra;
---
> 	locked = vma->vm_mm->pinned_vm + extra;
6324c5325
< 	if ((locked > lock_limit) && perf_is_paranoid() &&
---
> 	if ((locked > lock_limit) && perf_paranoid_tracepoint_raw() &&
6351,6352d5351
< 		perf_event_update_time(event);
< 		perf_set_shadow_time(event, event->ctx);
6365c5364
< 		atomic64_add(extra, &vma->vm_mm->pinned_vm);
---
> 		vma->vm_mm->pinned_vm += extra;
6439,6501d5437
< static void perf_sigtrap(struct perf_event *event)
< {
< 	/*
< 	 * We'd expect this to only occur if the irq_work is delayed and either
< 	 * ctx->task or current has changed in the meantime. This can be the
< 	 * case on architectures that do not implement arch_irq_work_raise().
< 	 */
< 	if (WARN_ON_ONCE(event->ctx->task != current))
< 		return;
< 
< 	/*
< 	 * perf_pending_event() can race with the task exiting.
< 	 */
< 	if (current->flags & PF_EXITING)
< 		return;
< 
< 	force_sig_perf((void __user *)event->pending_addr,
< 		       event->attr.type, event->attr.sig_data);
< }
< 
< static void perf_pending_event_disable(struct perf_event *event)
< {
< 	int cpu = READ_ONCE(event->pending_disable);
< 
< 	if (cpu < 0)
< 		return;
< 
< 	if (cpu == smp_processor_id()) {
< 		WRITE_ONCE(event->pending_disable, -1);
< 
< 		if (event->attr.sigtrap) {
< 			perf_sigtrap(event);
< 			atomic_set_release(&event->event_limit, 1); /* rearm event */
< 			return;
< 		}
< 
< 		perf_event_disable_local(event);
< 		return;
< 	}
< 
< 	/*
< 	 *  CPU-A			CPU-B
< 	 *
< 	 *  perf_event_disable_inatomic()
< 	 *    @pending_disable = CPU-A;
< 	 *    irq_work_queue();
< 	 *
< 	 *  sched-out
< 	 *    @pending_disable = -1;
< 	 *
< 	 *				sched-in
< 	 *				perf_event_disable_inatomic()
< 	 *				  @pending_disable = CPU-B;
< 	 *				  irq_work_queue(); // FAILS
< 	 *
< 	 *  irq_work_run()
< 	 *    perf_pending_event()
< 	 *
< 	 * But the event runs on CPU-B and wants disabling there.
< 	 */
< 	irq_work_queue_on(&event->pending, cpu);
< }
< 
6504c5440,5441
< 	struct perf_event *event = container_of(entry, struct perf_event, pending);
---
> 	struct perf_event *event = container_of(entry,
> 			struct perf_event, pending);
6513c5450,5453
< 	perf_pending_event_disable(event);
---
> 	if (event->pending_disable) {
> 		event->pending_disable = 0;
> 		perf_event_disable_local(event);
> 	}
6562c5502,5503
< 				  struct pt_regs *regs)
---
> 				  struct pt_regs *regs,
> 				  struct pt_regs *regs_user_copy)
6567,6568c5508,5509
< 	} else if (!(current->flags & PF_KTHREAD)) {
< 		perf_get_regs_user(regs_user, regs);
---
> 	} else if (current->mm) {
> 		perf_get_regs_user(regs_user, regs, regs_user_copy);
6587c5528
<  * precisely, but there's no way to get it safely under interrupt,
---
>  * precisly, but there's no way to get it safely under interrupt,
6651d5591
< 		mm_segment_t fs;
6669d5608
< 		fs = force_uaccess_begin();
6671d5609
< 		force_uaccess_end(fs);
6681,6796d5618
< static unsigned long perf_prepare_sample_aux(struct perf_event *event,
< 					  struct perf_sample_data *data,
< 					  size_t size)
< {
< 	struct perf_event *sampler = event->aux_event;
< 	struct perf_buffer *rb;
< 
< 	data->aux_size = 0;
< 
< 	if (!sampler)
< 		goto out;
< 
< 	if (WARN_ON_ONCE(READ_ONCE(sampler->state) != PERF_EVENT_STATE_ACTIVE))
< 		goto out;
< 
< 	if (WARN_ON_ONCE(READ_ONCE(sampler->oncpu) != smp_processor_id()))
< 		goto out;
< 
< 	rb = ring_buffer_get(sampler->parent ? sampler->parent : sampler);
< 	if (!rb)
< 		goto out;
< 
< 	/*
< 	 * If this is an NMI hit inside sampling code, don't take
< 	 * the sample. See also perf_aux_sample_output().
< 	 */
< 	if (READ_ONCE(rb->aux_in_sampling)) {
< 		data->aux_size = 0;
< 	} else {
< 		size = min_t(size_t, size, perf_aux_size(rb));
< 		data->aux_size = ALIGN(size, sizeof(u64));
< 	}
< 	ring_buffer_put(rb);
< 
< out:
< 	return data->aux_size;
< }
< 
< static long perf_pmu_snapshot_aux(struct perf_buffer *rb,
<                                  struct perf_event *event,
<                                  struct perf_output_handle *handle,
<                                  unsigned long size)
< {
< 	unsigned long flags;
< 	long ret;
< 
< 	/*
< 	 * Normal ->start()/->stop() callbacks run in IRQ mode in scheduler
< 	 * paths. If we start calling them in NMI context, they may race with
< 	 * the IRQ ones, that is, for example, re-starting an event that's just
< 	 * been stopped, which is why we're using a separate callback that
< 	 * doesn't change the event state.
< 	 *
< 	 * IRQs need to be disabled to prevent IPIs from racing with us.
< 	 */
< 	local_irq_save(flags);
< 	/*
< 	 * Guard against NMI hits inside the critical section;
< 	 * see also perf_prepare_sample_aux().
< 	 */
< 	WRITE_ONCE(rb->aux_in_sampling, 1);
< 	barrier();
< 
< 	ret = event->pmu->snapshot_aux(event, handle, size);
< 
< 	barrier();
< 	WRITE_ONCE(rb->aux_in_sampling, 0);
< 	local_irq_restore(flags);
< 
< 	return ret;
< }
< 
< static void perf_aux_sample_output(struct perf_event *event,
< 				   struct perf_output_handle *handle,
< 				   struct perf_sample_data *data)
< {
< 	struct perf_event *sampler = event->aux_event;
< 	struct perf_buffer *rb;
< 	unsigned long pad;
< 	long size;
< 
< 	if (WARN_ON_ONCE(!sampler || !data->aux_size))
< 		return;
< 
< 	rb = ring_buffer_get(sampler->parent ? sampler->parent : sampler);
< 	if (!rb)
< 		return;
< 
< 	size = perf_pmu_snapshot_aux(rb, sampler, handle, data->aux_size);
< 
< 	/*
< 	 * An error here means that perf_output_copy() failed (returned a
< 	 * non-zero surplus that it didn't copy), which in its current
< 	 * enlightened implementation is not possible. If that changes, we'd
< 	 * like to know.
< 	 */
< 	if (WARN_ON_ONCE(size < 0))
< 		goto out_put;
< 
< 	/*
< 	 * The pad comes from ALIGN()ing data->aux_size up to u64 in
< 	 * perf_prepare_sample_aux(), so should not be more than that.
< 	 */
< 	pad = data->aux_size - size;
< 	if (WARN_ON_ONCE(pad >= sizeof(u64)))
< 		pad = 8;
< 
< 	if (pad) {
< 		u64 zero = 0;
< 		perf_output_copy(handle, &zero, pad);
< 	}
< 
< out_put:
< 	ring_buffer_put(rb);
< }
< 
6907,6908c5729
< 	if ((leader != event) &&
< 	    (leader->state == PERF_EVENT_STATE_ACTIVE))
---
> 	if (leader != event)
6917c5738
< 	for_each_sibling_event(sub, leader) {
---
> 	list_for_each_entry(sub, &leader->sibling_list, group_entry) {
6966,6970d5786
< static inline bool perf_sample_save_hw_index(struct perf_event *event)
< {
< 	return event->attr.branch_sample_type & PERF_SAMPLE_BRANCH_HW_INDEX;
< }
< 
7011c5827,5833
< 		int size = 1;
---
> 		if (data->callchain) {
> 			int size = 1;
> 
> 			if (data->callchain)
> 				size += data->callchain->nr;
> 
> 			size *= sizeof(u64);
7013,7015c5835,5839
< 		size += data->callchain->nr;
< 		size *= sizeof(u64);
< 		__output_copy(handle, data->callchain, size);
---
> 			__output_copy(handle, data->callchain, size);
> 		} else {
> 			u64 nr = 0;
> 			perf_output_put(handle, nr);
> 		}
7059,7060d5882
< 			if (perf_sample_save_hw_index(event))
< 				perf_output_put(handle, data->br_stack->hw_idx);
7094,7095c5916,5917
< 	if (sample_type & PERF_SAMPLE_WEIGHT_TYPE)
< 		perf_output_put(handle, data->weight.full);
---
> 	if (sample_type & PERF_SAMPLE_WEIGHT)
> 		perf_output_put(handle, data->weight);
7123,7138d5944
< 	if (sample_type & PERF_SAMPLE_CGROUP)
< 		perf_output_put(handle, data->cgroup);
< 
< 	if (sample_type & PERF_SAMPLE_DATA_PAGE_SIZE)
< 		perf_output_put(handle, data->data_page_size);
< 
< 	if (sample_type & PERF_SAMPLE_CODE_PAGE_SIZE)
< 		perf_output_put(handle, data->code_page_size);
< 
< 	if (sample_type & PERF_SAMPLE_AUX) {
< 		perf_output_put(handle, data->aux_size);
< 
< 		if (data->aux_size)
< 			perf_aux_sample_output(event, handle, data);
< 	}
< 
7143c5949
< 			struct perf_buffer *rb = handle->rb;
---
> 			struct ring_buffer *rb = handle->rb;
7172c5978
< 		 * Try IRQ-safe get_user_page_fast_only first.
---
> 		 * Try IRQ-safe __get_user_pages_fast first.
7175,7180c5981,5983
< 		if (current->mm != NULL) {
< 			pagefault_disable();
< 			if (get_user_page_fast_only(virt, 0, &p))
< 				phys_addr = page_to_phys(p) + virt % PAGE_SIZE;
< 			pagefault_enable();
< 		}
---
> 		if ((current->mm != NULL) &&
> 		    (__get_user_pages_fast(virt, 1, 0, &p) == 1))
> 			phys_addr = page_to_phys(p) + virt % PAGE_SIZE;
7189,7295d5991
< /*
<  * Return the pagetable size of a given virtual address.
<  */
< static u64 perf_get_pgtable_size(struct mm_struct *mm, unsigned long addr)
< {
< 	u64 size = 0;
< 
< #ifdef CONFIG_HAVE_FAST_GUP
< 	pgd_t *pgdp, pgd;
< 	p4d_t *p4dp, p4d;
< 	pud_t *pudp, pud;
< 	pmd_t *pmdp, pmd;
< 	pte_t *ptep, pte;
< 
< 	pgdp = pgd_offset(mm, addr);
< 	pgd = READ_ONCE(*pgdp);
< 	if (pgd_none(pgd))
< 		return 0;
< 
< 	if (pgd_leaf(pgd))
< 		return pgd_leaf_size(pgd);
< 
< 	p4dp = p4d_offset_lockless(pgdp, pgd, addr);
< 	p4d = READ_ONCE(*p4dp);
< 	if (!p4d_present(p4d))
< 		return 0;
< 
< 	if (p4d_leaf(p4d))
< 		return p4d_leaf_size(p4d);
< 
< 	pudp = pud_offset_lockless(p4dp, p4d, addr);
< 	pud = READ_ONCE(*pudp);
< 	if (!pud_present(pud))
< 		return 0;
< 
< 	if (pud_leaf(pud))
< 		return pud_leaf_size(pud);
< 
< 	pmdp = pmd_offset_lockless(pudp, pud, addr);
< 	pmd = READ_ONCE(*pmdp);
< 	if (!pmd_present(pmd))
< 		return 0;
< 
< 	if (pmd_leaf(pmd))
< 		return pmd_leaf_size(pmd);
< 
< 	ptep = pte_offset_map(&pmd, addr);
< 	pte = ptep_get_lockless(ptep);
< 	if (pte_present(pte))
< 		size = pte_leaf_size(pte);
< 	pte_unmap(ptep);
< #endif /* CONFIG_HAVE_FAST_GUP */
< 
< 	return size;
< }
< 
< static u64 perf_get_page_size(unsigned long addr)
< {
< 	struct mm_struct *mm;
< 	unsigned long flags;
< 	u64 size;
< 
< 	if (!addr)
< 		return 0;
< 
< 	/*
< 	 * Software page-table walkers must disable IRQs,
< 	 * which prevents any tear down of the page tables.
< 	 */
< 	local_irq_save(flags);
< 
< 	mm = current->mm;
< 	if (!mm) {
< 		/*
< 		 * For kernel threads and the like, use init_mm so that
< 		 * we can find kernel memory.
< 		 */
< 		mm = &init_mm;
< 	}
< 
< 	size = perf_get_pgtable_size(mm, addr);
< 
< 	local_irq_restore(flags);
< 
< 	return size;
< }
< 
< static struct perf_callchain_entry __empty_callchain = { .nr = 0, };
< 
< struct perf_callchain_entry *
< perf_callchain(struct perf_event *event, struct pt_regs *regs)
< {
< 	bool kernel = !event->attr.exclude_callchain_kernel;
< 	bool user   = !event->attr.exclude_callchain_user;
< 	/* Disallow cross-task user callchains. */
< 	bool crosstask = event->ctx->task && event->ctx->task != current;
< 	const u32 max_stack = event->attr.sample_max_stack;
< 	struct perf_callchain_entry *callchain;
< 
< 	if (!kernel && !user)
< 		return &__empty_callchain;
< 
< 	callchain = get_perf_callchain(regs, 0, kernel, user,
< 				       max_stack, crosstask, true);
< 	return callchain ?: &__empty_callchain;
< }
< 
7311c6007
< 	if (sample_type & (PERF_SAMPLE_IP | PERF_SAMPLE_CODE_PAGE_SIZE))
---
> 	if (sample_type & PERF_SAMPLE_IP)
7317,7318c6013
< 		if (!(sample_type & __PERF_SAMPLE_CALLCHAIN_EARLY))
< 			data->callchain = perf_callchain(event, regs);
---
> 		data->callchain = perf_callchain(event, regs);
7320c6015,6016
< 		size += data->callchain->nr;
---
> 		if (data->callchain)
> 			size += data->callchain->nr;
7353,7355d6048
< 			if (perf_sample_save_hw_index(event))
< 				size += sizeof(u64);
< 
7363c6056,6057
< 		perf_sample_regs_user(&data->regs_user, regs);
---
> 		perf_sample_regs_user(&data->regs_user, regs,
> 				      &data->regs_user_copy);
7379c6073
< 		 * Either we need PERF_SAMPLE_STACK_USER bit to be always
---
> 		 * Either we need PERF_SAMPLE_STACK_USER bit to be allways
7419,7468d6112
< 
< #ifdef CONFIG_CGROUP_PERF
< 	if (sample_type & PERF_SAMPLE_CGROUP) {
< 		struct cgroup *cgrp;
< 
< 		/* protected by RCU */
< 		cgrp = task_css_check(current, perf_event_cgrp_id, 1)->cgroup;
< 		data->cgroup = cgroup_id(cgrp);
< 	}
< #endif
< 
< 	/*
< 	 * PERF_DATA_PAGE_SIZE requires PERF_SAMPLE_ADDR. If the user doesn't
< 	 * require PERF_SAMPLE_ADDR, kernel implicitly retrieve the data->addr,
< 	 * but the value will not dump to the userspace.
< 	 */
< 	if (sample_type & PERF_SAMPLE_DATA_PAGE_SIZE)
< 		data->data_page_size = perf_get_page_size(data->addr);
< 
< 	if (sample_type & PERF_SAMPLE_CODE_PAGE_SIZE)
< 		data->code_page_size = perf_get_page_size(data->ip);
< 
< 	if (sample_type & PERF_SAMPLE_AUX) {
< 		u64 size;
< 
< 		header->size += sizeof(u64); /* size */
< 
< 		/*
< 		 * Given the 16bit nature of header::size, an AUX sample can
< 		 * easily overflow it, what with all the preceding sample bits.
< 		 * Make sure this doesn't happen by using up to U16_MAX bytes
< 		 * per sample in total (rounded down to 8 byte boundary).
< 		 */
< 		size = min_t(size_t, U16_MAX - header->size,
< 			     event->attr.aux_sample_size);
< 		size = rounddown(size, 8);
< 		size = perf_prepare_sample_aux(event, data, size);
< 
< 		WARN_ON_ONCE(size + header->size > U16_MAX);
< 		header->size += size;
< 	}
< 	/*
< 	 * If you're adding more sample types here, you likely need to do
< 	 * something about the overflowing header::size, like repurpose the
< 	 * lowest 3 bits of size, which should be always zero at the moment.
< 	 * This raises a more important question, do we really need 512k sized
< 	 * samples and why, so good argumentation is in order for whatever you
< 	 * do here next.
< 	 */
< 	WARN_ON_ONCE(header->size & 7);
7471c6115
< static __always_inline int
---
> static void __always_inline
7476d6119
< 					struct perf_sample_data *,
7482d6124
< 	int err;
7489,7490c6131
< 	err = output_begin(&handle, data, event, header.size);
< 	if (err)
---
> 	if (output_begin(&handle, event, header.size))
7499d6139
< 	return err;
7518c6158
< int
---
> void
7523c6163
< 	return __perf_event_output(event, data, regs, perf_output_begin);
---
> 	__perf_event_output(event, data, regs, perf_output_begin);
7555c6195
< 	ret = perf_output_begin(&handle, &sample, event, read_event.header.size);
---
> 	ret = perf_output_begin(&handle, event, read_event.header.size);
7663,7665c6303,6304
< 		if (filter->path.dentry) {
< 			event->addr_filter_ranges[count].start = 0;
< 			event->addr_filter_ranges[count].size = 0;
---
> 		if (filter->inode) {
> 			event->addr_filters_offs[count] = 0;
7684a6324
> 	rcu_read_lock();
7685a6326,6329
> 		ctx = current->perf_event_ctxp[ctxn];
> 		if (!ctx)
> 			continue;
> 
7687d6330
< 		perf_event_remove_on_exec(ctxn);
7689,7695c6332,6333
< 		rcu_read_lock();
< 		ctx = rcu_dereference(current->perf_event_ctxp[ctxn]);
< 		if (ctx) {
< 			perf_iterate_ctx(ctx, perf_event_addr_filters_exec,
< 					 NULL, true);
< 		}
< 		rcu_read_unlock();
---
> 		perf_iterate_ctx(ctx, perf_event_addr_filters_exec, NULL,
> 				   true);
7696a6335
> 	rcu_read_unlock();
7700c6339
< 	struct perf_buffer	*rb;
---
> 	struct ring_buffer	*rb;
7708c6347
< 	struct perf_buffer *rb = ro->rb;
---
> 	struct ring_buffer *rb = ro->rb;
7736c6375
< 	struct pmu *pmu = event->ctx->pmu;
---
> 	struct pmu *pmu = event->pmu;
7824c6463
< 	ret = perf_output_begin(&handle, &sample, event,
---
> 	ret = perf_output_begin(&handle, event,
7830c6469
< 	task_event->event_id.tid = perf_event_tid(event, task);
---
> 	task_event->event_id.ppid = perf_event_pid(event, current);
7832,7840c6471,6472
< 	if (task_event->event_id.header.type == PERF_RECORD_EXIT) {
< 		task_event->event_id.ppid = perf_event_pid(event,
< 							task->real_parent);
< 		task_event->event_id.ptid = perf_event_pid(event,
< 							task->real_parent);
< 	} else {  /* PERF_RECORD_FORK */
< 		task_event->event_id.ppid = perf_event_pid(event, current);
< 		task_event->event_id.ptid = perf_event_tid(event, current);
< 	}
---
> 	task_event->event_id.tid = perf_event_tid(event, task);
> 	task_event->event_id.ptid = perf_event_tid(event, current);
7927c6559
< 	ret = perf_output_begin(&handle, &sample, event,
---
> 	ret = perf_output_begin(&handle, event,
8027c6659
< 	ret = perf_output_begin(&handle, &sample, event,
---
> 	ret = perf_output_begin(&handle, event,
8052c6684
< 	int error;
---
> 	void *error;
8122,8220d6753
<  * cgroup tracking
<  */
< #ifdef CONFIG_CGROUP_PERF
< 
< struct perf_cgroup_event {
< 	char				*path;
< 	int				path_size;
< 	struct {
< 		struct perf_event_header	header;
< 		u64				id;
< 		char				path[];
< 	} event_id;
< };
< 
< static int perf_event_cgroup_match(struct perf_event *event)
< {
< 	return event->attr.cgroup;
< }
< 
< static void perf_event_cgroup_output(struct perf_event *event, void *data)
< {
< 	struct perf_cgroup_event *cgroup_event = data;
< 	struct perf_output_handle handle;
< 	struct perf_sample_data sample;
< 	u16 header_size = cgroup_event->event_id.header.size;
< 	int ret;
< 
< 	if (!perf_event_cgroup_match(event))
< 		return;
< 
< 	perf_event_header__init_id(&cgroup_event->event_id.header,
< 				   &sample, event);
< 	ret = perf_output_begin(&handle, &sample, event,
< 				cgroup_event->event_id.header.size);
< 	if (ret)
< 		goto out;
< 
< 	perf_output_put(&handle, cgroup_event->event_id);
< 	__output_copy(&handle, cgroup_event->path, cgroup_event->path_size);
< 
< 	perf_event__output_id_sample(event, &handle, &sample);
< 
< 	perf_output_end(&handle);
< out:
< 	cgroup_event->event_id.header.size = header_size;
< }
< 
< static void perf_event_cgroup(struct cgroup *cgrp)
< {
< 	struct perf_cgroup_event cgroup_event;
< 	char path_enomem[16] = "//enomem";
< 	char *pathname;
< 	size_t size;
< 
< 	if (!atomic_read(&nr_cgroup_events))
< 		return;
< 
< 	cgroup_event = (struct perf_cgroup_event){
< 		.event_id  = {
< 			.header = {
< 				.type = PERF_RECORD_CGROUP,
< 				.misc = 0,
< 				.size = sizeof(cgroup_event.event_id),
< 			},
< 			.id = cgroup_id(cgrp),
< 		},
< 	};
< 
< 	pathname = kmalloc(PATH_MAX, GFP_KERNEL);
< 	if (pathname == NULL) {
< 		cgroup_event.path = path_enomem;
< 	} else {
< 		/* just to be sure to have enough space for alignment */
< 		cgroup_path(cgrp, pathname, PATH_MAX - sizeof(u64));
< 		cgroup_event.path = pathname;
< 	}
< 
< 	/*
< 	 * Since our buffer works in 8 byte units we need to align our string
< 	 * size to a multiple of 8. However, we must guarantee the tail end is
< 	 * zero'd out to avoid leaking random bits to userspace.
< 	 */
< 	size = strlen(cgroup_event.path) + 1;
< 	while (!IS_ALIGNED(size, sizeof(u64)))
< 		cgroup_event.path[size++] = '\0';
< 
< 	cgroup_event.event_id.header.size += size;
< 	cgroup_event.path_size = size;
< 
< 	perf_iterate_sb(perf_event_cgroup_output,
< 			&cgroup_event,
< 			NULL);
< 
< 	kfree(pathname);
< }
< 
< #endif
< 
< /*
8233,8234d6765
< 	u8			build_id[BUILD_ID_SIZE_MAX];
< 	u32			build_id_size;
8265,8266d6795
< 	u32 type = mmap_event->event_id.header.type;
< 	bool use_build_id;
8283c6812
< 	ret = perf_output_begin(&handle, &sample, event,
---
> 	ret = perf_output_begin(&handle, event,
8291,8295d6819
< 	use_build_id = event->attr.build_id && mmap_event->build_id_size;
< 
< 	if (event->attr.mmap2 && use_build_id)
< 		mmap_event->event_id.header.misc |= PERF_RECORD_MISC_MMAP_BUILD_ID;
< 
8299,8309c6823,6826
< 		if (use_build_id) {
< 			u8 size[4] = { (u8) mmap_event->build_id_size, 0, 0, 0 };
< 
< 			__output_copy(&handle, size, 4);
< 			__output_copy(&handle, mmap_event->build_id, BUILD_ID_SIZE_MAX);
< 		} else {
< 			perf_output_put(&handle, mmap_event->maj);
< 			perf_output_put(&handle, mmap_event->min);
< 			perf_output_put(&handle, mmap_event->ino);
< 			perf_output_put(&handle, mmap_event->ino_generation);
< 		}
---
> 		perf_output_put(&handle, mmap_event->maj);
> 		perf_output_put(&handle, mmap_event->min);
> 		perf_output_put(&handle, mmap_event->ino);
> 		perf_output_put(&handle, mmap_event->ino_generation);
8322d6838
< 	mmap_event->event_id.header.type = type;
8348a6865,6868
> 	if (vma->vm_flags & VM_DENYWRITE)
> 		flags |= MAP_DENYWRITE;
> 	if (vma->vm_flags & VM_MAYEXEC)
> 		flags |= MAP_EXECUTABLE;
8351c6871
< 	if (is_vm_hugetlb_page(vma))
---
> 	if (vma->vm_flags & VM_HUGETLB)
8434,8436d6953
< 	if (atomic_read(&nr_build_id_events))
< 		build_id_parse(vma, mmap_event->build_id, &mmap_event->build_id_size);
< 
8451,8455c6968
< 	/* d_inode(NULL) won't be equal to any mapped user-space file */
< 	if (!filter->path.dentry)
< 		return false;
< 
< 	if (d_inode(filter->path.dentry) != file_inode(file))
---
> 	if (filter->inode != file_inode(file))
8467,8488d6979
< static bool perf_addr_filter_vma_adjust(struct perf_addr_filter *filter,
< 					struct vm_area_struct *vma,
< 					struct perf_addr_filter_range *fr)
< {
< 	unsigned long vma_size = vma->vm_end - vma->vm_start;
< 	unsigned long off = vma->vm_pgoff << PAGE_SHIFT;
< 	struct file *file = vma->vm_file;
< 
< 	if (!perf_addr_filter_match(filter, file, off, vma_size))
< 		return false;
< 
< 	if (filter->offset < off) {
< 		fr->start = vma->vm_start;
< 		fr->size = min(vma_size, filter->size - (off - filter->offset));
< 	} else {
< 		fr->start = vma->vm_start + filter->offset - off;
< 		fr->size = min(vma->vm_end - fr->start, filter->size);
< 	}
< 
< 	return true;
< }
< 
8492a6984,6985
> 	unsigned long off = vma->vm_pgoff << PAGE_SHIFT, flags;
> 	struct file *file = vma->vm_file;
8495d6987
< 	unsigned long flags;
8500c6992
< 	if (!vma->vm_file)
---
> 	if (!file)
8505,8506c6997,6999
< 		if (perf_addr_filter_vma_adjust(filter, vma,
< 						&event->addr_filter_ranges[count]))
---
> 		if (perf_addr_filter_match(filter, file, off,
> 					     vma->vm_end - vma->vm_start)) {
> 			event->addr_filters_offs[count] = vma->vm_start;
8507a7001
> 		}
8604c7098
< 	ret = perf_output_begin(&handle, &sample, event, rec.header.size);
---
> 	ret = perf_output_begin(&handle, event, rec.header.size);
8638c7132
< 	ret = perf_output_begin(&handle, &sample, event,
---
> 	ret = perf_output_begin(&handle, event,
8693c7187
< 	ret = perf_output_begin(&handle, &sample, event, se->event_id.header.size);
---
> 	ret = perf_output_begin(&handle, event, se->event_id.header.size);
8728,8733c7222,7224
< 	if (!sched_in && task->on_rq) {
< 		switch_event.event_id.header.misc |=
< 				PERF_RECORD_MISC_SWITCH_OUT_PREEMPT;
< 	}
< 
< 	perf_iterate_sb(perf_event_switch_output, &switch_event, NULL);
---
> 	perf_iterate_sb(perf_event_switch_output,
> 		       &switch_event,
> 		       NULL);
8767c7258
< 	ret = perf_output_begin(&handle, &sample, event,
---
> 	ret = perf_output_begin(&handle, event,
8777,9060d7267
< /*
<  * ksymbol register/unregister tracking
<  */
< 
< struct perf_ksymbol_event {
< 	const char	*name;
< 	int		name_len;
< 	struct {
< 		struct perf_event_header        header;
< 		u64				addr;
< 		u32				len;
< 		u16				ksym_type;
< 		u16				flags;
< 	} event_id;
< };
< 
< static int perf_event_ksymbol_match(struct perf_event *event)
< {
< 	return event->attr.ksymbol;
< }
< 
< static void perf_event_ksymbol_output(struct perf_event *event, void *data)
< {
< 	struct perf_ksymbol_event *ksymbol_event = data;
< 	struct perf_output_handle handle;
< 	struct perf_sample_data sample;
< 	int ret;
< 
< 	if (!perf_event_ksymbol_match(event))
< 		return;
< 
< 	perf_event_header__init_id(&ksymbol_event->event_id.header,
< 				   &sample, event);
< 	ret = perf_output_begin(&handle, &sample, event,
< 				ksymbol_event->event_id.header.size);
< 	if (ret)
< 		return;
< 
< 	perf_output_put(&handle, ksymbol_event->event_id);
< 	__output_copy(&handle, ksymbol_event->name, ksymbol_event->name_len);
< 	perf_event__output_id_sample(event, &handle, &sample);
< 
< 	perf_output_end(&handle);
< }
< 
< void perf_event_ksymbol(u16 ksym_type, u64 addr, u32 len, bool unregister,
< 			const char *sym)
< {
< 	struct perf_ksymbol_event ksymbol_event;
< 	char name[KSYM_NAME_LEN];
< 	u16 flags = 0;
< 	int name_len;
< 
< 	if (!atomic_read(&nr_ksymbol_events))
< 		return;
< 
< 	if (ksym_type >= PERF_RECORD_KSYMBOL_TYPE_MAX ||
< 	    ksym_type == PERF_RECORD_KSYMBOL_TYPE_UNKNOWN)
< 		goto err;
< 
< 	strlcpy(name, sym, KSYM_NAME_LEN);
< 	name_len = strlen(name) + 1;
< 	while (!IS_ALIGNED(name_len, sizeof(u64)))
< 		name[name_len++] = '\0';
< 	BUILD_BUG_ON(KSYM_NAME_LEN % sizeof(u64));
< 
< 	if (unregister)
< 		flags |= PERF_RECORD_KSYMBOL_FLAGS_UNREGISTER;
< 
< 	ksymbol_event = (struct perf_ksymbol_event){
< 		.name = name,
< 		.name_len = name_len,
< 		.event_id = {
< 			.header = {
< 				.type = PERF_RECORD_KSYMBOL,
< 				.size = sizeof(ksymbol_event.event_id) +
< 					name_len,
< 			},
< 			.addr = addr,
< 			.len = len,
< 			.ksym_type = ksym_type,
< 			.flags = flags,
< 		},
< 	};
< 
< 	perf_iterate_sb(perf_event_ksymbol_output, &ksymbol_event, NULL);
< 	return;
< err:
< 	WARN_ONCE(1, "%s: Invalid KSYMBOL type 0x%x\n", __func__, ksym_type);
< }
< 
< /*
<  * bpf program load/unload tracking
<  */
< 
< struct perf_bpf_event {
< 	struct bpf_prog	*prog;
< 	struct {
< 		struct perf_event_header        header;
< 		u16				type;
< 		u16				flags;
< 		u32				id;
< 		u8				tag[BPF_TAG_SIZE];
< 	} event_id;
< };
< 
< static int perf_event_bpf_match(struct perf_event *event)
< {
< 	return event->attr.bpf_event;
< }
< 
< static void perf_event_bpf_output(struct perf_event *event, void *data)
< {
< 	struct perf_bpf_event *bpf_event = data;
< 	struct perf_output_handle handle;
< 	struct perf_sample_data sample;
< 	int ret;
< 
< 	if (!perf_event_bpf_match(event))
< 		return;
< 
< 	perf_event_header__init_id(&bpf_event->event_id.header,
< 				   &sample, event);
< 	ret = perf_output_begin(&handle, data, event,
< 				bpf_event->event_id.header.size);
< 	if (ret)
< 		return;
< 
< 	perf_output_put(&handle, bpf_event->event_id);
< 	perf_event__output_id_sample(event, &handle, &sample);
< 
< 	perf_output_end(&handle);
< }
< 
< static void perf_event_bpf_emit_ksymbols(struct bpf_prog *prog,
< 					 enum perf_bpf_event_type type)
< {
< 	bool unregister = type == PERF_BPF_EVENT_PROG_UNLOAD;
< 	int i;
< 
< 	if (prog->aux->func_cnt == 0) {
< 		perf_event_ksymbol(PERF_RECORD_KSYMBOL_TYPE_BPF,
< 				   (u64)(unsigned long)prog->bpf_func,
< 				   prog->jited_len, unregister,
< 				   prog->aux->ksym.name);
< 	} else {
< 		for (i = 0; i < prog->aux->func_cnt; i++) {
< 			struct bpf_prog *subprog = prog->aux->func[i];
< 
< 			perf_event_ksymbol(
< 				PERF_RECORD_KSYMBOL_TYPE_BPF,
< 				(u64)(unsigned long)subprog->bpf_func,
< 				subprog->jited_len, unregister,
< 				prog->aux->ksym.name);
< 		}
< 	}
< }
< 
< void perf_event_bpf_event(struct bpf_prog *prog,
< 			  enum perf_bpf_event_type type,
< 			  u16 flags)
< {
< 	struct perf_bpf_event bpf_event;
< 
< 	if (type <= PERF_BPF_EVENT_UNKNOWN ||
< 	    type >= PERF_BPF_EVENT_MAX)
< 		return;
< 
< 	switch (type) {
< 	case PERF_BPF_EVENT_PROG_LOAD:
< 	case PERF_BPF_EVENT_PROG_UNLOAD:
< 		if (atomic_read(&nr_ksymbol_events))
< 			perf_event_bpf_emit_ksymbols(prog, type);
< 		break;
< 	default:
< 		break;
< 	}
< 
< 	if (!atomic_read(&nr_bpf_events))
< 		return;
< 
< 	bpf_event = (struct perf_bpf_event){
< 		.prog = prog,
< 		.event_id = {
< 			.header = {
< 				.type = PERF_RECORD_BPF_EVENT,
< 				.size = sizeof(bpf_event.event_id),
< 			},
< 			.type = type,
< 			.flags = flags,
< 			.id = prog->aux->id,
< 		},
< 	};
< 
< 	BUILD_BUG_ON(BPF_TAG_SIZE % sizeof(u64));
< 
< 	memcpy(bpf_event.event_id.tag, prog->tag, BPF_TAG_SIZE);
< 	perf_iterate_sb(perf_event_bpf_output, &bpf_event, NULL);
< }
< 
< struct perf_text_poke_event {
< 	const void		*old_bytes;
< 	const void		*new_bytes;
< 	size_t			pad;
< 	u16			old_len;
< 	u16			new_len;
< 
< 	struct {
< 		struct perf_event_header	header;
< 
< 		u64				addr;
< 	} event_id;
< };
< 
< static int perf_event_text_poke_match(struct perf_event *event)
< {
< 	return event->attr.text_poke;
< }
< 
< static void perf_event_text_poke_output(struct perf_event *event, void *data)
< {
< 	struct perf_text_poke_event *text_poke_event = data;
< 	struct perf_output_handle handle;
< 	struct perf_sample_data sample;
< 	u64 padding = 0;
< 	int ret;
< 
< 	if (!perf_event_text_poke_match(event))
< 		return;
< 
< 	perf_event_header__init_id(&text_poke_event->event_id.header, &sample, event);
< 
< 	ret = perf_output_begin(&handle, &sample, event,
< 				text_poke_event->event_id.header.size);
< 	if (ret)
< 		return;
< 
< 	perf_output_put(&handle, text_poke_event->event_id);
< 	perf_output_put(&handle, text_poke_event->old_len);
< 	perf_output_put(&handle, text_poke_event->new_len);
< 
< 	__output_copy(&handle, text_poke_event->old_bytes, text_poke_event->old_len);
< 	__output_copy(&handle, text_poke_event->new_bytes, text_poke_event->new_len);
< 
< 	if (text_poke_event->pad)
< 		__output_copy(&handle, &padding, text_poke_event->pad);
< 
< 	perf_event__output_id_sample(event, &handle, &sample);
< 
< 	perf_output_end(&handle);
< }
< 
< void perf_event_text_poke(const void *addr, const void *old_bytes,
< 			  size_t old_len, const void *new_bytes, size_t new_len)
< {
< 	struct perf_text_poke_event text_poke_event;
< 	size_t tot, pad;
< 
< 	if (!atomic_read(&nr_text_poke_events))
< 		return;
< 
< 	tot  = sizeof(text_poke_event.old_len) + old_len;
< 	tot += sizeof(text_poke_event.new_len) + new_len;
< 	pad  = ALIGN(tot, sizeof(u64)) - tot;
< 
< 	text_poke_event = (struct perf_text_poke_event){
< 		.old_bytes    = old_bytes,
< 		.new_bytes    = new_bytes,
< 		.pad          = pad,
< 		.old_len      = old_len,
< 		.new_len      = new_len,
< 		.event_id  = {
< 			.header = {
< 				.type = PERF_RECORD_TEXT_POKE,
< 				.misc = PERF_RECORD_MISC_KERNEL,
< 				.size = sizeof(text_poke_event.event_id) + tot + pad,
< 			},
< 			.addr = (unsigned long)addr,
< 		},
< 	};
< 
< 	perf_iterate_sb(perf_event_text_poke_output, &text_poke_event, NULL);
< }
< 
9091c7298
< 	ret = perf_output_begin(&handle, &sample, event, rec.header.size);
---
> 	ret = perf_output_begin(&handle, event, rec.header.size);
9172d7378
< 		event->pending_addr = data->addr;
9660c7866
< 	 * If exclude_kernel, only trace user-space tracepoints (uprobes)
---
> 	 * All tracepoints are from kernel-space.
9662c7868
< 	if (event->attr.exclude_kernel && !user_mode(regs))
---
> 	if (event->attr.exclude_kernel)
9726,9727d7931
< 			if (event->cpu != smp_processor_id())
< 				continue;
9781,9921d7984
< #if defined(CONFIG_KPROBE_EVENTS) || defined(CONFIG_UPROBE_EVENTS)
< /*
<  * Flags in config, used by dynamic PMU kprobe and uprobe
<  * The flags should match following PMU_FORMAT_ATTR().
<  *
<  * PERF_PROBE_CONFIG_IS_RETPROBE if set, create kretprobe/uretprobe
<  *                               if not set, create kprobe/uprobe
<  *
<  * The following values specify a reference counter (or semaphore in the
<  * terminology of tools like dtrace, systemtap, etc.) Userspace Statically
<  * Defined Tracepoints (USDT). Currently, we use 40 bit for the offset.
<  *
<  * PERF_UPROBE_REF_CTR_OFFSET_BITS	# of bits in config as th offset
<  * PERF_UPROBE_REF_CTR_OFFSET_SHIFT	# of bits to shift left
<  */
< enum perf_probe_config {
< 	PERF_PROBE_CONFIG_IS_RETPROBE = 1U << 0,  /* [k,u]retprobe */
< 	PERF_UPROBE_REF_CTR_OFFSET_BITS = 32,
< 	PERF_UPROBE_REF_CTR_OFFSET_SHIFT = 64 - PERF_UPROBE_REF_CTR_OFFSET_BITS,
< };
< 
< PMU_FORMAT_ATTR(retprobe, "config:0");
< #endif
< 
< #ifdef CONFIG_KPROBE_EVENTS
< static struct attribute *kprobe_attrs[] = {
< 	&format_attr_retprobe.attr,
< 	NULL,
< };
< 
< static struct attribute_group kprobe_format_group = {
< 	.name = "format",
< 	.attrs = kprobe_attrs,
< };
< 
< static const struct attribute_group *kprobe_attr_groups[] = {
< 	&kprobe_format_group,
< 	NULL,
< };
< 
< static int perf_kprobe_event_init(struct perf_event *event);
< static struct pmu perf_kprobe = {
< 	.task_ctx_nr	= perf_sw_context,
< 	.event_init	= perf_kprobe_event_init,
< 	.add		= perf_trace_add,
< 	.del		= perf_trace_del,
< 	.start		= perf_swevent_start,
< 	.stop		= perf_swevent_stop,
< 	.read		= perf_swevent_read,
< 	.attr_groups	= kprobe_attr_groups,
< };
< 
< static int perf_kprobe_event_init(struct perf_event *event)
< {
< 	int err;
< 	bool is_retprobe;
< 
< 	if (event->attr.type != perf_kprobe.type)
< 		return -ENOENT;
< 
< 	if (!perfmon_capable())
< 		return -EACCES;
< 
< 	/*
< 	 * no branch sampling for probe events
< 	 */
< 	if (has_branch_stack(event))
< 		return -EOPNOTSUPP;
< 
< 	is_retprobe = event->attr.config & PERF_PROBE_CONFIG_IS_RETPROBE;
< 	err = perf_kprobe_init(event, is_retprobe);
< 	if (err)
< 		return err;
< 
< 	event->destroy = perf_kprobe_destroy;
< 
< 	return 0;
< }
< #endif /* CONFIG_KPROBE_EVENTS */
< 
< #ifdef CONFIG_UPROBE_EVENTS
< PMU_FORMAT_ATTR(ref_ctr_offset, "config:32-63");
< 
< static struct attribute *uprobe_attrs[] = {
< 	&format_attr_retprobe.attr,
< 	&format_attr_ref_ctr_offset.attr,
< 	NULL,
< };
< 
< static struct attribute_group uprobe_format_group = {
< 	.name = "format",
< 	.attrs = uprobe_attrs,
< };
< 
< static const struct attribute_group *uprobe_attr_groups[] = {
< 	&uprobe_format_group,
< 	NULL,
< };
< 
< static int perf_uprobe_event_init(struct perf_event *event);
< static struct pmu perf_uprobe = {
< 	.task_ctx_nr	= perf_sw_context,
< 	.event_init	= perf_uprobe_event_init,
< 	.add		= perf_trace_add,
< 	.del		= perf_trace_del,
< 	.start		= perf_swevent_start,
< 	.stop		= perf_swevent_stop,
< 	.read		= perf_swevent_read,
< 	.attr_groups	= uprobe_attr_groups,
< };
< 
< static int perf_uprobe_event_init(struct perf_event *event)
< {
< 	int err;
< 	unsigned long ref_ctr_offset;
< 	bool is_retprobe;
< 
< 	if (event->attr.type != perf_uprobe.type)
< 		return -ENOENT;
< 
< 	if (!perfmon_capable())
< 		return -EACCES;
< 
< 	/*
< 	 * no branch sampling for probe events
< 	 */
< 	if (has_branch_stack(event))
< 		return -EOPNOTSUPP;
< 
< 	is_retprobe = event->attr.config & PERF_PROBE_CONFIG_IS_RETPROBE;
< 	ref_ctr_offset = event->attr.config >> PERF_UPROBE_REF_CTR_OFFSET_SHIFT;
< 	err = perf_uprobe_init(event, ref_ctr_offset, is_retprobe);
< 	if (err)
< 		return err;
< 
< 	event->destroy = perf_uprobe_destroy;
< 
< 	return 0;
< }
< #endif /* CONFIG_UPROBE_EVENTS */
< 
9925,9930d7987
< #ifdef CONFIG_KPROBE_EVENTS
< 	perf_pmu_register(&perf_kprobe, "kprobe", -1);
< #endif
< #ifdef CONFIG_UPROBE_EVENTS
< 	perf_pmu_register(&perf_uprobe, "uprobe", -1);
< #endif
9947d8003
< 	struct bpf_prog *prog;
9950a8007
> 	preempt_disable();
9954,9956c8011
< 	prog = READ_ONCE(event->prog);
< 	if (prog)
< 		ret = bpf_prog_run(prog, &ctx);
---
> 	ret = BPF_PROG_RUN(event->prog, &ctx);
9959a8015
> 	preempt_enable();
9966,9968c8022
< static int perf_event_set_bpf_handler(struct perf_event *event,
< 				      struct bpf_prog *prog,
< 				      u64 bpf_cookie)
---
> static int perf_event_set_bpf_handler(struct perf_event *event, u32 prog_fd)
9969a8024,8025
> 	struct bpf_prog *prog;
> 
9977,9995c8033,8035
< 	if (prog->type != BPF_PROG_TYPE_PERF_EVENT)
< 		return -EINVAL;
< 
< 	if (event->attr.precise_ip &&
< 	    prog->call_get_stack &&
< 	    (!(event->attr.sample_type & __PERF_SAMPLE_CALLCHAIN_EARLY) ||
< 	     event->attr.exclude_callchain_kernel ||
< 	     event->attr.exclude_callchain_user)) {
< 		/*
< 		 * On perf_event with precise_ip, calling bpf_get_stack()
< 		 * may trigger unwinder warnings and occasional crashes.
< 		 * bpf_get_[stack|stackid] works around this issue by using
< 		 * callchain attached to perf_sample_data. If the
< 		 * perf_event does not full (kernel and user) callchain
< 		 * attached to perf_sample_data, do not allow attaching BPF
< 		 * program that calls bpf_get_[stack|stackid].
< 		 */
< 		return -EPROTO;
< 	}
---
> 	prog = bpf_prog_get_type(prog_fd, BPF_PROG_TYPE_PERF_EVENT);
> 	if (IS_ERR(prog))
> 		return PTR_ERR(prog);
9998d8037
< 	event->bpf_cookie = bpf_cookie;
10016,10018c8055
< static int perf_event_set_bpf_handler(struct perf_event *event,
< 				      struct bpf_prog *prog,
< 				      u64 bpf_cookie)
---
> static int perf_event_set_bpf_handler(struct perf_event *event, u32 prog_fd)
10027,10047c8064
< /*
<  * returns true if the event is a tracepoint, or a kprobe/upprobe created
<  * with perf_event_open()
<  */
< static inline bool perf_event_is_tracing(struct perf_event *event)
< {
< 	if (event->pmu == &perf_tracepoint)
< 		return true;
< #ifdef CONFIG_KPROBE_EVENTS
< 	if (event->pmu == &perf_kprobe)
< 		return true;
< #endif
< #ifdef CONFIG_UPROBE_EVENTS
< 	if (event->pmu == &perf_uprobe)
< 		return true;
< #endif
< 	return false;
< }
< 
< int perf_event_set_bpf_prog(struct perf_event *event, struct bpf_prog *prog,
< 			    u64 bpf_cookie)
---
> static int perf_event_set_bpf_prog(struct perf_event *event, u32 prog_fd)
10049a8067,8068
> 	struct bpf_prog *prog;
> 	int ret;
10051,10052c8070,8071
< 	if (!perf_event_is_tracing(event))
< 		return perf_event_set_bpf_handler(event, prog, bpf_cookie);
---
> 	if (event->attr.type != PERF_TYPE_TRACEPOINT)
> 		return perf_event_set_bpf_handler(event, prog_fd);
10060a8080,8083
> 	prog = bpf_prog_get(prog_fd);
> 	if (IS_ERR(prog))
> 		return PTR_ERR(prog);
> 
10063,10068c8086,8088
< 	    (is_syscall_tp && prog->type != BPF_PROG_TYPE_TRACEPOINT))
< 		return -EINVAL;
< 
< 	/* Kprobe override only works for kprobes, not uprobes. */
< 	if (prog->kprobe_override &&
< 	    !(event->tp_event->flags & TRACE_EVENT_FL_KPROBE))
---
> 	    (is_syscall_tp && prog->type != BPF_PROG_TYPE_TRACEPOINT)) {
> 		/* valid fd, but invalid bpf program type */
> 		bpf_prog_put(prog);
10069a8090
> 	}
10074c8095,8096
< 		if (prog->aux->max_ctx_offset > off)
---
> 		if (prog->aux->max_ctx_offset > off) {
> 			bpf_prog_put(prog);
10075a8098
> 		}
10078c8101,8104
< 	return perf_event_attach_bpf_prog(event, prog, bpf_cookie);
---
> 	ret = perf_event_attach_bpf_prog(event, prog);
> 	if (ret)
> 		bpf_prog_put(prog);
> 	return ret;
10081c8107
< void perf_event_free_bpf_prog(struct perf_event *event)
---
> static void perf_event_free_bpf_prog(struct perf_event *event)
10083c8109
< 	if (!perf_event_is_tracing(event)) {
---
> 	if (event->attr.type != PERF_TYPE_TRACEPOINT) {
10100,10101c8126
< int perf_event_set_bpf_prog(struct perf_event *event, struct bpf_prog *prog,
< 			    u64 bpf_cookie)
---
> static int perf_event_set_bpf_prog(struct perf_event *event, u32 prog_fd)
10106c8131
< void perf_event_free_bpf_prog(struct perf_event *event)
---
> static void perf_event_free_bpf_prog(struct perf_event *event)
10148c8173,8174
< 		path_put(&filter->path);
---
> 		if (filter->inode)
> 			iput(filter->inode);
10184c8210
<  * Called with mm::mmap_lock down for reading.
---
>  * Called with mm::mmap_sem down for reading.
10186,10188c8212,8213
< static void perf_addr_filter_apply(struct perf_addr_filter *filter,
< 				   struct mm_struct *mm,
< 				   struct perf_addr_filter_range *fr)
---
> static unsigned long perf_addr_filter_apply(struct perf_addr_filter *filter,
> 					    struct mm_struct *mm)
10193c8218,8222
< 		if (!vma->vm_file)
---
> 		struct file *file = vma->vm_file;
> 		unsigned long off = vma->vm_pgoff << PAGE_SHIFT;
> 		unsigned long vma_size = vma->vm_end - vma->vm_start;
> 
> 		if (!file)
10196,10197c8225,8228
< 		if (perf_addr_filter_vma_adjust(filter, vma, fr))
< 			return;
---
> 		if (!perf_addr_filter_match(filter, file, off, vma_size))
> 			continue;
> 
> 		return vma->vm_start;
10198a8230,8231
> 
> 	return 0;
10221,10224c8254,8255
< 	if (ifh->nr_file_filters) {
< 		mm = get_task_mm(task);
< 		if (!mm)
< 			goto restart;
---
> 	if (!ifh->nr_file_filters)
> 		return;
10226,10227c8257,8261
< 		mmap_read_lock(mm);
< 	}
---
> 	mm = get_task_mm(event->ctx->task);
> 	if (!mm)
> 		goto restart;
> 
> 	down_read(&mm->mmap_sem);
10231,10237c8265
< 		if (filter->path.dentry) {
< 			/*
< 			 * Adjust base offset if the filter is associated to a
< 			 * binary that needs to be mapped:
< 			 */
< 			event->addr_filter_ranges[count].start = 0;
< 			event->addr_filter_ranges[count].size = 0;
---
> 		event->addr_filters_offs[count] = 0;
10239,10243c8267,8273
< 			perf_addr_filter_apply(filter, mm, &event->addr_filter_ranges[count]);
< 		} else {
< 			event->addr_filter_ranges[count].start = filter->offset;
< 			event->addr_filter_ranges[count].size  = filter->size;
< 		}
---
> 		/*
> 		 * Adjust base offset if the filter is associated to a binary
> 		 * that needs to be mapped:
> 		 */
> 		if (filter->inode)
> 			event->addr_filters_offs[count] =
> 				perf_addr_filter_apply(filter, mm);
10251,10252c8281
< 	if (ifh->nr_file_filters) {
< 		mmap_read_unlock(mm);
---
> 	up_read(&mm->mmap_sem);
10254,10255c8283
< 		mmput(mm);
< 	}
---
> 	mmput(mm);
10277,10278c8305
<  * if <size> is not specified or is zero, the range is treated as a single
<  * address; not valid for ACTION=="filter".
---
>  * if <size> is not specified, the range is treated as a single address.
10316a8344
> 	struct path path;
10327,10331d8354
< 		static const enum perf_addr_filter_action_t actions[] = {
< 			[IF_ACT_FILTER]	= PERF_ADDR_FILTER_ACTION_FILTER,
< 			[IF_ACT_START]	= PERF_ADDR_FILTER_ACTION_START,
< 			[IF_ACT_STOP]	= PERF_ADDR_FILTER_ACTION_STOP,
< 		};
10347a8371,8372
> 			filter->filter = 1;
> 
10352d8376
< 			filter->action = actions[token];
10359d8382
< 			fallthrough;
10365a8389,8391
> 			if (token == IF_SRC_FILE || token == IF_SRC_KERNEL)
> 				filter->range = 1;
> 
10371c8397
< 			if (token == IF_SRC_KERNEL || token == IF_SRC_FILE) {
---
> 			if (filter->range) {
10379c8405
< 				int fpos = token == IF_SRC_FILE ? 2 : 1;
---
> 				int fpos = filter->range ? 2 : 1;
10381d8406
< 				kfree(filename);
10406,10413d8430
< 			/*
< 			 * ACTION "filter" must have a non-zero length region
< 			 * specified.
< 			 */
< 			if (filter->action == PERF_ADDR_FILTER_ACTION_FILTER &&
< 			    !filter->size)
< 				goto fail;
< 
10428c8445
< 					goto fail;
---
> 					goto fail_free_name;
10431,10432c8448
< 				ret = kern_path(filename, LOOKUP_FOLLOW,
< 						&filter->path);
---
> 				ret = kern_path(filename, LOOKUP_FOLLOW, &path);
10434c8450,8455
< 					goto fail;
---
> 					goto fail_free_name;
> 
> 				filter->inode = igrab(d_inode(path.dentry));
> 				path_put(&path);
> 				kfree(filename);
> 				filename = NULL;
10437,10439c8458,8460
< 				if (!filter->path.dentry ||
< 				    !S_ISREG(d_inode(filter->path.dentry)
< 					     ->i_mode))
---
> 				if (!filter->inode ||
> 				    !S_ISREG(filter->inode->i_mode))
> 					/* free_filters_list() will iput() */
10454d8474
< 	kfree(filename);
10459c8479
< fail:
---
> fail_free_name:
10460a8481
> fail:
10506a8528,8550
> static int
> perf_tracepoint_set_filter(struct perf_event *event, char *filter_str)
> {
> 	struct perf_event_context *ctx = event->ctx;
> 	int ret;
> 
> 	/*
> 	 * Beware, here be dragons!!
> 	 *
> 	 * the tracepoint muck will deadlock against ctx->mutex, but the tracepoint
> 	 * stuff does not actually need it. So temporarily drop ctx->mutex. As per
> 	 * perf_event_ctx_lock() we already have a reference on ctx.
> 	 *
> 	 * This can result in event getting moved to a different ctx, but that
> 	 * does not affect the tracepoint state.
> 	 */
> 	mutex_unlock(&ctx->mutex);
> 	ret = ftrace_profile_set_filter(event, event->attr.config, filter_str);
> 	mutex_lock(&ctx->mutex);
> 
> 	return ret;
> }
> 
10509d8552
< 	int ret = -EINVAL;
10510a8554,8559
> 	int ret = -EINVAL;
> 
> 	if ((event->attr.type != PERF_TYPE_TRACEPOINT ||
> 	    !IS_ENABLED(CONFIG_EVENT_TRACING)) &&
> 	    !has_addr_filter(event))
> 		return -EINVAL;
10516,10536c8565,8568
< #ifdef CONFIG_EVENT_TRACING
< 	if (perf_event_is_tracing(event)) {
< 		struct perf_event_context *ctx = event->ctx;
< 
< 		/*
< 		 * Beware, here be dragons!!
< 		 *
< 		 * the tracepoint muck will deadlock against ctx->mutex, but
< 		 * the tracepoint stuff does not actually need it. So
< 		 * temporarily drop ctx->mutex. As per perf_event_ctx_lock() we
< 		 * already have a reference on ctx.
< 		 *
< 		 * This can result in event getting moved to a different ctx,
< 		 * but that does not affect the tracepoint state.
< 		 */
< 		mutex_unlock(&ctx->mutex);
< 		ret = ftrace_profile_set_filter(event, event->attr.config, filter_str);
< 		mutex_lock(&ctx->mutex);
< 	} else
< #endif
< 	if (has_addr_filter(event))
---
> 	if (IS_ENABLED(CONFIG_EVENT_TRACING) &&
> 	    event->attr.type == PERF_TYPE_TRACEPOINT)
> 		ret = perf_tracepoint_set_filter(event, filter_str);
> 	else if (has_addr_filter(event))
10595c8627
< 		      HRTIMER_MODE_REL_PINNED_HARD);
---
> 		      HRTIMER_MODE_REL_PINNED);
10617c8649
< 	hrtimer_init(&hwc->hrtimer, CLOCK_MONOTONIC, HRTIMER_MODE_REL_HARD);
---
> 	hrtimer_init(&hwc->hrtimer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);
10806,10810d8837
< static int perf_event_nop_int(struct perf_event *event, u64 value)
< {
< 	return 0;
< }
< 
10881a8909
> 	mutex_lock(&pmus_lock);
10882a8911
> 	mutex_unlock(&pmus_lock);
11006,11011d9034
< 	if (pmu->attr_update)
< 		ret = sysfs_update_groups(&pmu->dev->kobj, pmu->attr_update);
< 
< 	if (ret)
< 		goto del_dev;
< 
11028c9051
< 	int cpu, ret, max = PERF_TYPE_MAX;
---
> 	int cpu, ret;
11041,11046c9064,9067
< 	if (type != PERF_TYPE_SOFTWARE) {
< 		if (type >= 0)
< 			max = type;
< 
< 		ret = idr_alloc(&pmu_idr, pmu, max, 0, GFP_KERNEL);
< 		if (ret < 0)
---
> 	if (type < 0) {
> 		type = idr_alloc(&pmu_idr, pmu, PERF_TYPE_MAX, 0, GFP_KERNEL);
> 		if (type < 0) {
> 			ret = type;
11048,11051c9069
< 
< 		WARN_ON(type >= 0 && ret != type);
< 
< 		type = ret;
---
> 		}
11097,11099d9114
< 
< 		cpuctx->heap_size = ARRAY_SIZE(cpuctx->heap_default);
< 		cpuctx->heap = cpuctx->heap_default;
11125,11127d9139
< 	if (!pmu->check_period)
< 		pmu->check_period = perf_event_nop_int;
< 
11131,11140c9143
< 	/*
< 	 * Ensure the TYPE_SOFTWARE PMUs are at the head of the list,
< 	 * since these cannot be in the IDR. This way the linear search
< 	 * is fast, provided a valid software event is provided.
< 	 */
< 	if (type == PERF_TYPE_SOFTWARE || !name)
< 		list_add_rcu(&pmu->entry, &pmus);
< 	else
< 		list_add_tail_rcu(&pmu->entry, &pmus);
< 
---
> 	list_add_rcu(&pmu->entry, &pmus);
11153c9156
< 	if (pmu->type != PERF_TYPE_SOFTWARE)
---
> 	if (pmu->type >= PERF_TYPE_MAX)
11163a9167,9168
> 	int remove_device;
> 
11164a9170
> 	remove_device = pmu_bus_running;
11165a9172
> 	mutex_unlock(&pmus_lock);
11175c9182
< 	if (pmu->type != PERF_TYPE_SOFTWARE)
---
> 	if (pmu->type >= PERF_TYPE_MAX)
11177c9184
< 	if (pmu_bus_running) {
---
> 	if (remove_device) {
11184d9190
< 	mutex_unlock(&pmus_lock);
11188,11193d9193
< static inline bool has_extended_regs(struct perf_event *event)
< {
< 	return (event->attr.sample_regs_user & PERF_REG_EXTENDED_MASK) ||
< 	       (event->attr.sample_regs_intr & PERF_REG_EXTENDED_MASK);
< }
< 
11224,11236d9223
< 	if (!ret) {
< 		if (!(pmu->capabilities & PERF_PMU_CAP_EXTENDED_REGS) &&
< 		    has_extended_regs(event))
< 			ret = -EOPNOTSUPP;
< 
< 		if (pmu->capabilities & PERF_PMU_CAP_NO_EXCLUDE &&
< 		    event_has_any_exclude_flag(event))
< 			ret = -EINVAL;
< 
< 		if (ret && event->destroy)
< 			event->destroy(event);
< 	}
< 
11245,11246d9231
< 	bool extended_type = false;
< 	int idx, type, ret;
11247a9233,9234
> 	int idx;
> 	int ret;
11259,11274d9245
< 	/*
< 	 * PERF_TYPE_HARDWARE and PERF_TYPE_HW_CACHE
< 	 * are often aliases for PERF_TYPE_RAW.
< 	 */
< 	type = event->attr.type;
< 	if (type == PERF_TYPE_HARDWARE || type == PERF_TYPE_HW_CACHE) {
< 		type = event->attr.config >> PERF_PMU_TYPE_SHIFT;
< 		if (!type) {
< 			type = PERF_TYPE_RAW;
< 		} else {
< 			extended_type = true;
< 			event->attr.config &= PERF_HW_EVENT_MASK;
< 		}
< 	}
< 
< again:
11276c9247
< 	pmu = idr_find(&pmu_idr, type);
---
> 	pmu = idr_find(&pmu_idr, event->attr.type);
11279,11282d9249
< 		if (event->attr.type != type && type != PERF_TYPE_RAW &&
< 		    !(pmu->capabilities & PERF_PMU_CAP_EXTENDED_HW_TYPE))
< 			goto fail;
< 
11284,11288d9250
< 		if (ret == -ENOENT && event->attr.type != type && !extended_type) {
< 			type = event->attr.type;
< 			goto again;
< 		}
< 
11291d9252
< 
11295c9256
< 	list_for_each_entry_rcu(pmu, &pmus, entry, lockdep_is_held(&pmus_srcu)) {
---
> 	list_for_each_entry_rcu(pmu, &pmus, entry) {
11305d9265
< fail:
11372c9332
< 	if (event->attach_state & (PERF_ATTACH_TASK | PERF_ATTACH_SCHED_CB))
---
> 	if (event->attach_state & PERF_ATTACH_TASK)
11376,11377d9335
< 	if (event->attr.build_id)
< 		atomic_inc(&nr_build_id_events);
11382,11383d9339
< 	if (event->attr.cgroup)
< 		atomic_inc(&nr_cgroup_events);
11396,11401d9351
< 	if (event->attr.ksymbol)
< 		atomic_inc(&nr_ksymbol_events);
< 	if (event->attr.bpf_event)
< 		atomic_inc(&nr_bpf_events);
< 	if (event->attr.text_poke)
< 		atomic_inc(&nr_text_poke_events);
11420c9370
< 			synchronize_rcu();
---
> 			synchronize_sched();
11437c9387
<  * Allocate and initialize an event structure
---
>  * Allocate and initialize a event structure
11451d9400
< 	int node;
11457,11460d9405
< 	if (attr->sigtrap && !task) {
< 		/* Requires a task: avoid signalling random tasks. */
< 		return ERR_PTR(-EINVAL);
< 	}
11462,11464c9407
< 	node = (cpu >= 0) ? cpu_to_node(cpu) : -1;
< 	event = kmem_cache_alloc_node(perf_event_cache, GFP_KERNEL | __GFP_ZERO,
< 				      node);
---
> 	event = kzalloc(sizeof(*event), GFP_KERNEL);
11477a9421
> 	INIT_LIST_HEAD(&event->group_entry);
11480,11481d9423
< 	INIT_LIST_HEAD(&event->active_list);
< 	init_event_group(event);
11489d9430
< 	event->pending_disable = -1;
11509,11511d9449
< 	if (event->attr.sigtrap)
< 		atomic_set(&event->event_limit, 1);
< 
11519c9457
< 		event->hw.target = get_task_struct(task);
---
> 		event->hw.target = task;
11531c9469
< 			struct bpf_prog *prog = parent_event->prog;
---
> 			struct bpf_prog *prog = bpf_prog_inc(parent_event->prog);
11533c9471,9474
< 			bpf_prog_inc(prog);
---
> 			if (IS_ERR(prog)) {
> 				err = PTR_ERR(prog);
> 				goto err_ns;
> 			}
11573a9515,9520
> 	if (cgroup_fd != -1) {
> 		err = perf_cgroup_connect(cgroup_fd, event, attr, group_leader);
> 		if (err)
> 			goto err_ns;
> 	}
> 
11580,11600d9526
< 	/*
< 	 * Disallow uncore-cgroup events, they don't make sense as the cgroup will
< 	 * be different on other CPUs in the uncore mask.
< 	 */
< 	if (pmu->task_ctx_nr == perf_invalid_context && cgroup_fd != -1) {
< 		err = -EINVAL;
< 		goto err_pmu;
< 	}
< 
< 	if (event->attr.aux_output &&
< 	    !(pmu->capabilities & PERF_PMU_CAP_AUX_OUTPUT)) {
< 		err = -EOPNOTSUPP;
< 		goto err_pmu;
< 	}
< 
< 	if (cgroup_fd != -1) {
< 		err = perf_cgroup_connect(cgroup_fd, event, attr, group_leader);
< 		if (err)
< 			goto err_pmu;
< 	}
< 
11606,11609c9532,9535
< 		event->addr_filter_ranges = kcalloc(pmu->nr_addr_filters,
< 						    sizeof(struct perf_addr_filter_range),
< 						    GFP_KERNEL);
< 		if (!event->addr_filter_ranges) {
---
> 		event->addr_filters_offs = kcalloc(pmu->nr_addr_filters,
> 						   sizeof(unsigned long),
> 						   GFP_KERNEL);
> 		if (!event->addr_filters_offs) {
11614,11627d9539
< 		/*
< 		 * Clone the parent's vma offsets: they are valid until exec()
< 		 * even if the mm is not shared with the parent.
< 		 */
< 		if (event->parent) {
< 			struct perf_addr_filters_head *ifh = perf_event_addr_filters(event);
< 
< 			raw_spin_lock_irq(&ifh->lock);
< 			memcpy(event->addr_filter_ranges,
< 			       event->parent->addr_filter_ranges,
< 			       pmu->nr_addr_filters * sizeof(struct perf_addr_filter_range));
< 			raw_spin_unlock_irq(&ifh->lock);
< 		}
< 
11640,11643d9551
< 	err = security_perf_event_alloc(event);
< 	if (err)
< 		goto err_callchain_buffer;
< 
11649,11653d9556
< err_callchain_buffer:
< 	if (!event->parent) {
< 		if (event->attr.sample_type & PERF_SAMPLE_CALLCHAIN)
< 			put_callchain_buffers();
< 	}
11655c9558
< 	kfree(event->addr_filter_ranges);
---
> 	kfree(event->addr_filters_offs);
11661,11662d9563
< 	if (is_cgroup_event(event))
< 		perf_detach_cgroup(event);
11666a9568,9569
> 	if (is_cgroup_event(event))
> 		perf_detach_cgroup(event);
11669,11671c9572
< 	if (event->hw.target)
< 		put_task_struct(event->hw.target);
< 	kmem_cache_free(perf_event_cache, event);
---
> 	kfree(event);
11682c9583,9588
< 	/* Zero the full structure, so that a short copy will be nice. */
---
> 	if (!access_ok(VERIFY_WRITE, uattr, PERF_ATTR_SIZE_VER0))
> 		return -EFAULT;
> 
> 	/*
> 	 * zero the full structure, so that a short copy will be nice.
> 	 */
11689,11690c9595,9598
< 	/* ABI compatibility quirk: */
< 	if (!size)
---
> 	if (size > PAGE_SIZE)	/* silly large */
> 		goto err_size;
> 
> 	if (!size)		/* abi compat */
11692c9600,9601
< 	if (size < PERF_ATTR_SIZE_VER0 || size > PAGE_SIZE)
---
> 
> 	if (size < PERF_ATTR_SIZE_VER0)
11695,11699c9604,9625
< 	ret = copy_struct_from_user(attr, sizeof(*attr), uattr, size);
< 	if (ret) {
< 		if (ret == -E2BIG)
< 			goto err_size;
< 		return ret;
---
> 	/*
> 	 * If we're handed a bigger struct than we know of,
> 	 * ensure all the unknown bits are 0 - i.e. new
> 	 * user-space does not rely on any kernel feature
> 	 * extensions we dont know about yet.
> 	 */
> 	if (size > sizeof(*attr)) {
> 		unsigned char __user *addr;
> 		unsigned char __user *end;
> 		unsigned char val;
> 
> 		addr = (void __user *)uattr + sizeof(*attr);
> 		end  = (void __user *)uattr + size;
> 
> 		for (; addr < end; addr++) {
> 			ret = get_user(val, addr);
> 			if (ret)
> 				return ret;
> 			if (val)
> 				goto err_size;
> 		}
> 		size = sizeof(*attr);
11701a9628,9631
> 	ret = copy_from_user(attr, uattr, size);
> 	if (ret)
> 		return -EFAULT;
> 
11704c9634
< 	if (attr->__reserved_1 || attr->__reserved_2 || attr->__reserved_3)
---
> 	if (attr->__reserved_1)
11742,11746c9672,9674
< 		if (mask & PERF_SAMPLE_BRANCH_PERM_PLM) {
< 			ret = perf_allow_kernel(attr);
< 			if (ret)
< 				return ret;
< 		}
---
> 		if ((mask & PERF_SAMPLE_BRANCH_PERM_PLM)
> 		    && perf_paranoid_kernel() && !capable(CAP_SYS_ADMIN))
> 			return -EACCES;
11765c9693
< 			return -EINVAL;
---
> 			ret = -EINVAL;
11767c9695
< 			return -EINVAL;
---
> 			ret = -EINVAL;
11770,11772d9697
< 	if (!attr->sample_max_stack)
< 		attr->sample_max_stack = sysctl_perf_event_max_stack;
< 
11775,11792d9699
< 
< #ifndef CONFIG_CGROUP_PERF
< 	if (attr->sample_type & PERF_SAMPLE_CGROUP)
< 		return -EINVAL;
< #endif
< 	if ((attr->sample_type & PERF_SAMPLE_WEIGHT) &&
< 	    (attr->sample_type & PERF_SAMPLE_WEIGHT_STRUCT))
< 		return -EINVAL;
< 
< 	if (!attr->inherit && attr->inherit_thread)
< 		return -EINVAL;
< 
< 	if (attr->remove_on_exec && attr->enable_on_exec)
< 		return -EINVAL;
< 
< 	if (attr->sigtrap && !attr->remove_on_exec)
< 		return -EINVAL;
< 
11805c9712
< 	struct perf_buffer *rb = NULL;
---
> 	struct ring_buffer *rb = NULL;
11899c9806
< 		event->clock = &ktime_get_boottime_ns;
---
> 		event->clock = &ktime_get_boot_ns;
11903c9810
< 		event->clock = &ktime_get_clocktai_ns;
---
> 		event->clock = &ktime_get_tai_ns;
11929c9836
< 	if (!refcount_inc_not_zero(&gctx->refcount)) {
---
> 	if (!atomic_inc_not_zero(&gctx->refcount)) {
11947,11977d9853
< static bool
< perf_check_permission(struct perf_event_attr *attr, struct task_struct *task)
< {
< 	unsigned int ptrace_mode = PTRACE_MODE_READ_REALCREDS;
< 	bool is_capable = perfmon_capable();
< 
< 	if (attr->sigtrap) {
< 		/*
< 		 * perf_event_attr::sigtrap sends signals to the other task.
< 		 * Require the current task to also have CAP_KILL.
< 		 */
< 		rcu_read_lock();
< 		is_capable &= ns_capable(__task_cred(task)->user_ns, CAP_KILL);
< 		rcu_read_unlock();
< 
< 		/*
< 		 * If the required capabilities aren't available, checks for
< 		 * ptrace permissions: upgrade to ATTACH, since sending signals
< 		 * can effectively change the target task.
< 		 */
< 		ptrace_mode = PTRACE_MODE_ATTACH_REALCREDS;
< 	}
< 
< 	/*
< 	 * Preserve ptrace permission check for backwards compatibility. The
< 	 * ptrace check also includes checks that the current task and other
< 	 * task have matching uids, and is therefore not done here explicitly.
< 	 */
< 	return is_capable || ptrace_may_access(task, ptrace_mode);
< }
< 
11985d9860
<  * @flags:		perf event open flags
11994c9869
< 	struct perf_event_context *ctx, *gctx;
---
> 	struct perf_event_context *ctx, *uninitialized_var(gctx);
12009,12013d9883
< 	/* Do we allow access to perf_event_open(2) ? */
< 	err = security_perf_event_open(&attr, PERF_SECURITY_OPEN);
< 	if (err)
< 		return err;
< 
12019,12021c9889,9890
< 		err = perf_allow_kernel(&attr);
< 		if (err)
< 			return err;
---
> 		if (perf_paranoid_kernel() && !capable(CAP_SYS_ADMIN))
> 			return -EACCES;
12025c9894
< 		if (!perfmon_capable())
---
> 		if (!capable(CAP_SYS_ADMIN))
12038,12042c9907,9909
< 	if ((attr.sample_type & PERF_SAMPLE_PHYS_ADDR)) {
< 		err = perf_allow_kernel(&attr);
< 		if (err)
< 			return err;
< 	}
---
> 	if ((attr.sample_type & PERF_SAMPLE_PHYS_ADDR) &&
> 	    perf_paranoid_kernel() && !capable(CAP_SYS_ADMIN))
> 		return -EACCES;
12044,12049c9911,9912
< 	/* REGS_INTR can leak data, lockdown must prevent this */
< 	if (attr.sample_type & PERF_SAMPLE_REGS_INTR) {
< 		err = security_locked_down(LOCKDOWN_PERF);
< 		if (err)
< 			return err;
< 	}
---
> 	if (!attr.sample_max_stack)
> 		attr.sample_max_stack = sysctl_perf_event_max_stack;
12091a9955,9972
> 	if (task) {
> 		err = mutex_lock_interruptible(&task->signal->cred_guard_mutex);
> 		if (err)
> 			goto err_task;
> 
> 		/*
> 		 * Reuse ptrace permission checks for now.
> 		 *
> 		 * We must hold cred_guard_mutex across this and any potential
> 		 * perf_install_in_context() call for this new event to
> 		 * serialize against exec() altering our credentials (and the
> 		 * perf_event_exit_task() that could imply).
> 		 */
> 		err = -EACCES;
> 		if (!ptrace_may_access(task, PTRACE_MODE_READ_REALCREDS))
> 			goto err_cred;
> 	}
> 
12099c9980
< 		goto err_task;
---
> 		goto err_cred;
12124,12126c10005,10007
< 	if (group_leader) {
< 		if (is_software_event(event) &&
< 		    !in_software_context(group_leader)) {
---
> 	if (group_leader &&
> 	    (is_software_event(event) != is_software_event(group_leader))) {
> 		if (is_software_event(event)) {
12128,12129c10009,10010
< 			 * If the event is a sw event, but the group_leader
< 			 * is on hw context.
---
> 			 * If event and group_leader are not both a software
> 			 * event, and event is, then group leader is not.
12131,12133c10012,10014
< 			 * Allow the addition of software events to hw
< 			 * groups, this is safe because software events
< 			 * never fail to schedule.
---
> 			 * Allow the addition of software events to !software
> 			 * groups, this is safe because software events never
> 			 * fail to schedule.
12135,12137c10016,10017
< 			pmu = group_leader->ctx->pmu;
< 		} else if (!is_software_event(event) &&
< 			   is_software_event(group_leader) &&
---
> 			pmu = group_leader->pmu;
> 		} else if (is_software_event(group_leader) &&
12156a10037,10041
> 	if ((pmu->capabilities & PERF_PMU_CAP_EXCLUSIVE) && group_leader) {
> 		err = -EBUSY;
> 		goto err_context;
> 	}
> 
12218,12233d10102
< 	if (task) {
< 		err = down_read_interruptible(&task->signal->exec_update_lock);
< 		if (err)
< 			goto err_file;
< 
< 		/*
< 		 * We must hold exec_update_lock across this and any potential
< 		 * perf_install_in_context() call for this new event to
< 		 * serialize against exec() altering our credentials (and the
< 		 * perf_event_exit_task() that could imply).
< 		 */
< 		err = -EACCES;
< 		if (!perf_check_permission(&attr, task))
< 			goto err_cred;
< 	}
< 
12260,12271d10128
< 
< 		/*
< 		 * Failure to create exclusive events returns -EBUSY.
< 		 */
< 		err = -EBUSY;
< 		if (!exclusive_event_installable(group_leader, ctx))
< 			goto err_locked;
< 
< 		for_each_sibling_event(sibling, group_leader) {
< 			if (!exclusive_event_installable(sibling, ctx))
< 				goto err_locked;
< 		}
12302,12305d10158
< 	if (perf_need_aux_event(event) && !perf_get_aux_event(event, group_leader)) {
< 		err = -EINVAL;
< 		goto err_locked;
< 	}
12311a10165,10167
> 		/* exclusive and group stuff are assumed mutually exclusive */
> 		WARN_ON_ONCE(move_group);
> 
12331c10187,10188
< 		for_each_sibling_event(sibling, group_leader) {
---
> 		list_for_each_entry(sibling, &group_leader->sibling_list,
> 				    group_entry) {
12352c10209,10210
< 		for_each_sibling_event(sibling, group_leader) {
---
> 		list_for_each_entry(sibling, &group_leader->sibling_list,
> 				    group_entry) {
12387c10245
< 		up_read(&task->signal->exec_update_lock);
---
> 		mutex_unlock(&task->signal->cred_guard_mutex);
12409,12412c10267
< err_cred:
< 	if (task)
< 		up_read(&task->signal->exec_update_lock);
< err_file:
---
> /* err_file: */
12423a10279,10281
> err_cred:
> 	if (task)
> 		mutex_unlock(&task->signal->cred_guard_mutex);
12440,12441d10297
<  * @overflow_handler: callback to trigger when we hit the event
<  * @context: context data could be used in overflow_handler callback
12454,12455c10310
< 	 * Grouping is not supported for kernel events, neither is 'AUX',
< 	 * make sure the caller's intentions are adjusted.
---
> 	 * Get the target context (task or percpu):
12457,12458d10311
< 	if (attr->aux_output)
< 		return ERR_PTR(-EINVAL);
12470,12472d10322
< 	/*
< 	 * Get the target context (task or percpu):
< 	 */
12506c10356
< 	perf_install_in_context(ctx, event, event->cpu);
---
> 	perf_install_in_context(ctx, event, cpu);
12588c10438,10439
< static void sync_child_event(struct perf_event *child_event)
---
> static void sync_child_event(struct perf_event *child_event,
> 			       struct task_struct *child)
12593,12598c10444,10445
< 	if (child_event->attr.inherit_stat) {
< 		struct task_struct *task = child_event->ctx->task;
< 
< 		if (task && task != TASK_TOMBSTONE)
< 			perf_event_read_event(child_event, task);
< 	}
---
> 	if (child_event->attr.inherit_stat)
> 		perf_event_read_event(child_event, child);
12613c10460,10462
< perf_event_exit_event(struct perf_event *event, struct perf_event_context *ctx)
---
> perf_event_exit_event(struct perf_event *child_event,
> 		      struct perf_event_context *child_ctx,
> 		      struct task_struct *child)
12615,12633c10464
< 	struct perf_event *parent_event = event->parent;
< 	unsigned long detach_flags = 0;
< 
< 	if (parent_event) {
< 		/*
< 		 * Do not destroy the 'original' grouping; because of the
< 		 * context switch optimization the original events could've
< 		 * ended up in a random child task.
< 		 *
< 		 * If we were to destroy the original group, all group related
< 		 * operations would cease to function properly after this
< 		 * random child dies.
< 		 *
< 		 * Do destroy all inherited groups, we don't care about those
< 		 * and being thorough is better.
< 		 */
< 		detach_flags = DETACH_GROUP | DETACH_CHILD;
< 		mutex_lock(&parent_event->child_mutex);
< 	}
---
> 	struct perf_event *parent_event = child_event->parent;
12635c10466,10479
< 	perf_remove_from_context(event, detach_flags);
---
> 	/*
> 	 * Do not destroy the 'original' grouping; because of the context
> 	 * switch optimization the original events could've ended up in a
> 	 * random child task.
> 	 *
> 	 * If we were to destroy the original group, all group related
> 	 * operations would cease to function properly after this random
> 	 * child dies.
> 	 *
> 	 * Do destroy all inherited groups, we don't care about those
> 	 * and being thorough is better.
> 	 */
> 	raw_spin_lock_irq(&child_ctx->lock);
> 	WARN_ON_ONCE(child_ctx->is_active);
12637,12640c10481,10485
< 	raw_spin_lock_irq(&ctx->lock);
< 	if (event->state > PERF_EVENT_STATE_EXIT)
< 		perf_event_set_state(event, PERF_EVENT_STATE_EXIT);
< 	raw_spin_unlock_irq(&ctx->lock);
---
> 	if (parent_event)
> 		perf_group_detach(child_event);
> 	list_del_event(child_event, child_ctx);
> 	perf_event_set_state(child_event, PERF_EVENT_STATE_EXIT); /* is_event_hup() */
> 	raw_spin_unlock_irq(&child_ctx->lock);
12643c10488
< 	 * Child events can be freed.
---
> 	 * Parent events are governed by their filedesc, retain them.
12645,12652c10490,10491
< 	if (parent_event) {
< 		mutex_unlock(&parent_event->child_mutex);
< 		/*
< 		 * Kick perf_poll() for is_event_hup();
< 		 */
< 		perf_event_wakeup(parent_event);
< 		free_event(event);
< 		put_event(parent_event);
---
> 	if (!parent_event) {
> 		perf_event_wakeup(child_event);
12654a10494,10498
> 	/*
> 	 * Child events can be cleaned up.
> 	 */
> 
> 	sync_child_event(child_event, child);
12657c10501,10509
< 	 * Parent events are governed by their filedesc, retain them.
---
> 	 * Remove this event from the parent's list
> 	 */
> 	WARN_ON_ONCE(parent_event->ctx->parent_ctx);
> 	mutex_lock(&parent_event->child_mutex);
> 	list_del_init(&child_event->child_list);
> 	mutex_unlock(&parent_event->child_mutex);
> 
> 	/*
> 	 * Kick perf_poll() for is_event_hup().
12659c10511,10513
< 	perf_event_wakeup(event);
---
> 	perf_event_wakeup(parent_event);
> 	free_event(child_event);
> 	put_event(parent_event);
12716c10570
< 		perf_event_exit_event(child_event, child_ctx);
---
> 		perf_event_exit_event(child_event, child_ctx, child);
12726,12727c10580,10581
<  * Can be called with exec_update_lock held when called from
<  * setup_new_exec().
---
>  * Can be called with cred_guard_mutex held when called from
>  * install_exec_creds().
12782,12783c10636,10637
<  * Free a context as created by inheritance by perf_event_init_task() below,
<  * used by fork() in case of fail.
---
>  * Free an unexposed, unused context as created by inheritance by
>  * perf_event_init_task below, used by fork() in case of fail.
12785,12786c10639,10640
<  * Even though the task has never lived, the context and events have been
<  * exposed through the child_list, so we must take care tearing it all down.
---
>  * Not all locks are strictly required, but take them anyway to be nice and
>  * help out with the lockdep assertions.
12816,12832c10670
< 
< 		/*
< 		 * perf_event_release_kernel() could've stolen some of our
< 		 * child events and still have them on its free_list. In that
< 		 * case we must wait for these events to have been freed (in
< 		 * particular all their references to this task must've been
< 		 * dropped).
< 		 *
< 		 * Without this copy_process() will unconditionally free this
< 		 * task (irrespective of its reference count) and
< 		 * _free_event()'s put_task_struct(event->hw.target) will be a
< 		 * use-after-free.
< 		 *
< 		 * Wait for all events to drop their context reference.
< 		 */
< 		wait_var_event(&ctx->refcount, refcount_read(&ctx->refcount) == 1);
< 		put_ctx(ctx); /* must be last */
---
> 		put_ctx(ctx);
12846c10684,10686
< 	struct file *file = fget(fd);
---
> 	struct file *file;
> 
> 	file = fget_raw(fd);
12858,12865d10697
< const struct perf_event *perf_get_event(struct file *file)
< {
< 	if (file->f_op != &perf_fops)
< 		return ERR_PTR(-EINVAL);
< 
< 	return file->private_data;
< }
< 
12875c10707
<  * Inherit an event from parent task to child task.
---
>  * Inherit a event from parent task to child task.
12911,12922d10742
< 
< 	if ((child_event->attach_state & PERF_ATTACH_TASK_DATA) &&
< 	    !child_ctx->task_ctx_data) {
< 		struct pmu *pmu = child_event->pmu;
< 
< 		child_ctx->task_ctx_data = alloc_task_ctx_data(pmu);
< 		if (!child_ctx->task_ctx_data) {
< 			free_event(child_event);
< 			return ERR_PTR(-ENOMEM);
< 		}
< 	}
< 
12933d10752
< 		/* task_ctx_data is freed with child_ctx */
12976d10794
< 	child_event->attach_state |= PERF_ATTACH_CHILD;
13017c10835
< 	for_each_sibling_event(sub, parent_event) {
---
> 	list_for_each_entry(sub, &parent_event->sibling_list, group_entry) {
13022,13025d10839
< 
< 		if (sub->aux_event == parent_event && child_ctr &&
< 		    !perf_get_aux_event(child_ctr, leader))
< 			return -EINVAL;
13045c10859
< 		   u64 clone_flags, int *inherited_all)
---
> 		   int *inherited_all)
13050,13053c10864
< 	if (!event->attr.inherit ||
< 	    (event->attr.inherit_thread && !(clone_flags & CLONE_THREAD)) ||
< 	    /* Do not inherit if sigtrap and signal handlers were cleared. */
< 	    (event->attr.sigtrap && (clone_flags & CLONE_CLEAR_SIGHAND))) {
---
> 	if (!event->attr.inherit) {
13085,13086c10896
< static int perf_event_init_context(struct task_struct *child, int ctxn,
< 				   u64 clone_flags)
---
> static int perf_event_init_context(struct task_struct *child, int ctxn)
13124c10934
< 	perf_event_groups_for_each(event, &parent_ctx->pinned_groups) {
---
> 	list_for_each_entry(event, &parent_ctx->pinned_groups, group_entry) {
13126,13127c10936
< 					 child, ctxn, clone_flags,
< 					 &inherited_all);
---
> 					 child, ctxn, &inherited_all);
13141c10950
< 	perf_event_groups_for_each(event, &parent_ctx->flexible_groups) {
---
> 	list_for_each_entry(event, &parent_ctx->flexible_groups, group_entry) {
13143,13144c10952
< 					 child, ctxn, clone_flags,
< 					 &inherited_all);
---
> 					 child, ctxn, &inherited_all);
13186c10994
< int perf_event_init_task(struct task_struct *child, u64 clone_flags)
---
> int perf_event_init_task(struct task_struct *child)
13195c11003
< 		ret = perf_event_init_context(child, ctxn, clone_flags);
---
> 		ret = perf_event_init_context(child, ctxn);
13227c11035
< static void perf_swevent_init_cpu(unsigned int cpu)
---
> void perf_swevent_init_cpu(unsigned int cpu)
13348,13349d11155
< 	perf_event_cache = KMEM_CACHE(perf_event, SLAB_PANIC);
< 
13426,13431d11231
< static int perf_cgroup_css_online(struct cgroup_subsys_state *css)
< {
< 	perf_event_cgroup(css->cgroup);
< 	return 0;
< }
< 
13453d11252
< 	.css_online	= perf_cgroup_css_online,
