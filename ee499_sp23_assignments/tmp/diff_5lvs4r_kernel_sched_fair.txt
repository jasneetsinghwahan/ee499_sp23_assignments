22a23,39
> 
> #include <linux/sched/mm.h>
> #include <linux/sched/topology.h>
> 
> #include <linux/latencytop.h>
> #include <linux/cpumask.h>
> #include <linux/cpuidle.h>
> #include <linux/slab.h>
> #include <linux/profile.h>
> #include <linux/interrupt.h>
> #include <linux/mempolicy.h>
> #include <linux/migrate.h>
> #include <linux/task_work.h>
> #include <linux/sched/isolation.h>
> 
> #include <trace/events/sched.h>
> 
39c56
< static unsigned int normalized_sysctl_sched_latency	= 6000000ULL;
---
> unsigned int normalized_sysctl_sched_latency		= 6000000ULL;
52c69
< unsigned int sysctl_sched_tunable_scaling = SCHED_TUNABLESCALING_LOG;
---
> enum sched_tunable_scaling sysctl_sched_tunable_scaling = SCHED_TUNABLESCALING_LOG;
59,60c76,77
< unsigned int sysctl_sched_min_granularity			= 750000ULL;
< static unsigned int normalized_sysctl_sched_min_granularity	= 750000ULL;
---
> unsigned int sysctl_sched_min_granularity		= 750000ULL;
> unsigned int normalized_sysctl_sched_min_granularity	= 750000ULL;
82,83c99,100
< unsigned int sysctl_sched_wakeup_granularity			= 1000000UL;
< static unsigned int normalized_sysctl_sched_wakeup_granularity	= 1000000UL;
---
> unsigned int sysctl_sched_wakeup_granularity		= 1000000UL;
> unsigned int normalized_sysctl_sched_wakeup_granularity	= 1000000UL;
87,99d103
< int sched_thermal_decay_shift;
< static int __init setup_sched_thermal_decay_shift(char *str)
< {
< 	int _shift = 0;
< 
< 	if (kstrtoint(str, 0, &_shift))
< 		pr_warn("Unable to set scheduler thermal pressure decay shift parameter\n");
< 
< 	sched_thermal_decay_shift = clamp(_shift, 0, 10);
< 	return 1;
< }
< __setup("sched_thermal_decay_shift=", setup_sched_thermal_decay_shift);
< 
102c106
<  * For asym packing, by default the lower numbered CPU has higher priority.
---
>  * For asym packing, by default the lower numbered cpu has higher priority.
108,122d111
< 
< /*
<  * The margin used when comparing utilization with CPU capacity.
<  *
<  * (default: ~20%)
<  */
< #define fits_capacity(cap, max)	((cap) * 1280 < (max) * 1024)
< 
< /*
<  * The margin used when comparing CPU capacities.
<  * is 'cap1' noticeably greater than 'cap2'
<  *
<  * (default: ~5%)
<  */
< #define capacity_greater(cap1, cap2) ((cap1) * 1024 > (cap2) * 1078)
138a128,135
> /*
>  * The margin used when comparing utilization with CPU capacity:
>  * util * margin < capacity * 1024
>  *
>  * (default: ~20%)
>  */
> unsigned int capacity_margin				= 1280;
> 
199c196
< void __init sched_init_granularity(void)
---
> void sched_init_granularity(void)
239d235
< 	u32 fact_hi = (u32)(fact >> 32);
241d236
< 	int fs;
245,248c240,244
< 	if (unlikely(fact_hi)) {
< 		fs = fls(fact_hi);
< 		shift -= fs;
< 		fact >>= fs;
---
> 	if (unlikely(fact >> 32)) {
> 		while (fact >> 32) {
> 			fact >>= 1;
> 			shift--;
> 		}
251,257c247,252
< 	fact = mul_u32_u32(fact, lw->inv_weight);
< 
< 	fact_hi = (u32)(fact >> 32);
< 	if (fact_hi) {
< 		fs = fls(fact_hi);
< 		shift -= fs;
< 		fact >>= fs;
---
> 	/* hint to use a 32x32->64 mul */
> 	fact = (u64)(u32)fact * lw->inv_weight;
> 
> 	while (fact >> 32) {
> 		fact >>= 1;
> 		shift--;
271a267,281
> /* cpu runqueue to which this cfs_rq is attached */
> static inline struct rq *rq_of(struct cfs_rq *cfs_rq)
> {
> 	return cfs_rq->rq;
> }
> 
> /* An entity is a task if it doesn't "own" a runqueue */
> #define entity_is_task(se)	(!se->my_q)
> 
> static inline struct task_struct *task_of(struct sched_entity *se)
> {
> 	SCHED_WARN_ON(!entity_is_task(se));
> 	return container_of(se, struct task_struct, se);
> }
> 
276c286
< static inline void cfs_rq_tg_path(struct cfs_rq *cfs_rq, char *path, int len)
---
> static inline struct cfs_rq *task_cfs_rq(struct task_struct *p)
278,286c288
< 	if (!path)
< 		return;
< 
< 	if (cfs_rq && task_group_is_autogroup(cfs_rq->tg))
< 		autogroup_path(cfs_rq->tg, path, len);
< 	else if (cfs_rq && cfs_rq->tg->css.cgroup)
< 		cgroup_path(cfs_rq->tg->css.cgroup, path, len);
< 	else
< 		strlcpy(path, "(null)", len);
---
> 	return p->se.cfs_rq;
289c291,292
< static inline bool list_add_leaf_cfs_rq(struct cfs_rq *cfs_rq)
---
> /* runqueue on which this entity is (to be) queued */
> static inline struct cfs_rq *cfs_rq_of(struct sched_entity *se)
291,295c294,295
< 	struct rq *rq = rq_of(cfs_rq);
< 	int cpu = cpu_of(rq);
< 
< 	if (cfs_rq->on_list)
< 		return rq->tmp_alone_branch == &rq->leaf_cfs_rq_list;
---
> 	return se->cfs_rq;
> }
297c297,301
< 	cfs_rq->on_list = 1;
---
> /* runqueue "owned" by this group */
> static inline struct cfs_rq *group_cfs_rq(struct sched_entity *grp)
> {
> 	return grp->my_q;
> }
299,317c303,307
< 	/*
< 	 * Ensure we either appear before our parent (if already
< 	 * enqueued) or force our parent to appear after us when it is
< 	 * enqueued. The fact that we always enqueue bottom-up
< 	 * reduces this to two cases and a special case for the root
< 	 * cfs_rq. Furthermore, it also means that we will always reset
< 	 * tmp_alone_branch either when the branch is connected
< 	 * to a tree or when we reach the top of the tree
< 	 */
< 	if (cfs_rq->tg->parent &&
< 	    cfs_rq->tg->parent->cfs_rq[cpu]->on_list) {
< 		/*
< 		 * If parent is already on the list, we add the child
< 		 * just before. Thanks to circular linked property of
< 		 * the list, this means to put the child at the tail
< 		 * of the list that starts by parent.
< 		 */
< 		list_add_tail_rcu(&cfs_rq->leaf_cfs_rq_list,
< 			&(cfs_rq->tg->parent->cfs_rq[cpu]->leaf_cfs_rq_list));
---
> static inline void list_add_leaf_cfs_rq(struct cfs_rq *cfs_rq)
> {
> 	if (!cfs_rq->on_list) {
> 		struct rq *rq = rq_of(cfs_rq);
> 		int cpu = cpu_of(rq);
319,321c309,315
< 		 * The branch is now connected to its tree so we can
< 		 * reset tmp_alone_branch to the beginning of the
< 		 * list.
---
> 		 * Ensure we either appear before our parent (if already
> 		 * enqueued) or force our parent to appear after us when it is
> 		 * enqueued. The fact that we always enqueue bottom-up
> 		 * reduces this to two cases and a special case for the root
> 		 * cfs_rq. Furthermore, it also means that we will always reset
> 		 * tmp_alone_branch either when the branch is connected
> 		 * to a tree or when we reach the beg of the tree
323,325c317,359
< 		rq->tmp_alone_branch = &rq->leaf_cfs_rq_list;
< 		return true;
< 	}
---
> 		if (cfs_rq->tg->parent &&
> 		    cfs_rq->tg->parent->cfs_rq[cpu]->on_list) {
> 			/*
> 			 * If parent is already on the list, we add the child
> 			 * just before. Thanks to circular linked property of
> 			 * the list, this means to put the child at the tail
> 			 * of the list that starts by parent.
> 			 */
> 			list_add_tail_rcu(&cfs_rq->leaf_cfs_rq_list,
> 				&(cfs_rq->tg->parent->cfs_rq[cpu]->leaf_cfs_rq_list));
> 			/*
> 			 * The branch is now connected to its tree so we can
> 			 * reset tmp_alone_branch to the beginning of the
> 			 * list.
> 			 */
> 			rq->tmp_alone_branch = &rq->leaf_cfs_rq_list;
> 		} else if (!cfs_rq->tg->parent) {
> 			/*
> 			 * cfs rq without parent should be put
> 			 * at the tail of the list.
> 			 */
> 			list_add_tail_rcu(&cfs_rq->leaf_cfs_rq_list,
> 				&rq->leaf_cfs_rq_list);
> 			/*
> 			 * We have reach the beg of a tree so we can reset
> 			 * tmp_alone_branch to the beginning of the list.
> 			 */
> 			rq->tmp_alone_branch = &rq->leaf_cfs_rq_list;
> 		} else {
> 			/*
> 			 * The parent has not already been added so we want to
> 			 * make sure that it will be put after us.
> 			 * tmp_alone_branch points to the beg of the branch
> 			 * where we will add parent.
> 			 */
> 			list_add_rcu(&cfs_rq->leaf_cfs_rq_list,
> 				rq->tmp_alone_branch);
> 			/*
> 			 * update tmp_alone_branch to points to the new beg
> 			 * of the branch
> 			 */
> 			rq->tmp_alone_branch = &cfs_rq->leaf_cfs_rq_list;
> 		}
327,339c361
< 	if (!cfs_rq->tg->parent) {
< 		/*
< 		 * cfs rq without parent should be put
< 		 * at the tail of the list.
< 		 */
< 		list_add_tail_rcu(&cfs_rq->leaf_cfs_rq_list,
< 			&rq->leaf_cfs_rq_list);
< 		/*
< 		 * We have reach the top of a tree so we can reset
< 		 * tmp_alone_branch to the beginning of the list.
< 		 */
< 		rq->tmp_alone_branch = &rq->leaf_cfs_rq_list;
< 		return true;
---
> 		cfs_rq->on_list = 1;
341,354d362
< 
< 	/*
< 	 * The parent has not already been added so we want to
< 	 * make sure that it will be put after us.
< 	 * tmp_alone_branch points to the begin of the branch
< 	 * where we will add parent.
< 	 */
< 	list_add_rcu(&cfs_rq->leaf_cfs_rq_list, rq->tmp_alone_branch);
< 	/*
< 	 * update tmp_alone_branch to points to the new begin
< 	 * of the branch
< 	 */
< 	rq->tmp_alone_branch = &cfs_rq->leaf_cfs_rq_list;
< 	return false;
360,371d367
< 		struct rq *rq = rq_of(cfs_rq);
< 
< 		/*
< 		 * With cfs_rq being unthrottled/throttled during an enqueue,
< 		 * it can happen the tmp_alone_branch points the a leaf that
< 		 * we finally want to del. In this case, tmp_alone_branch moves
< 		 * to the prev element but it will point to rq->leaf_cfs_rq_list
< 		 * at the end of the enqueue.
< 		 */
< 		if (rq->tmp_alone_branch == &cfs_rq->leaf_cfs_rq_list)
< 			rq->tmp_alone_branch = cfs_rq->leaf_cfs_rq_list.prev;
< 
377,381d372
< static inline void assert_list_leaf_cfs_rq(struct rq *rq)
< {
< 	SCHED_WARN_ON(rq->tmp_alone_branch != &rq->leaf_cfs_rq_list);
< }
< 
434,437c425
< static int tg_is_idle(struct task_group *tg)
< {
< 	return tg->idle > 0;
< }
---
> #else	/* !CONFIG_FAIR_GROUP_SCHED */
439c427
< static int cfs_rq_is_idle(struct cfs_rq *cfs_rq)
---
> static inline struct task_struct *task_of(struct sched_entity *se)
441c429
< 	return cfs_rq->idle > 0;
---
> 	return container_of(se, struct task_struct, se);
444c432
< static int se_is_idle(struct sched_entity *se)
---
> static inline struct rq *rq_of(struct cfs_rq *cfs_rq)
446,448c434
< 	if (entity_is_task(se))
< 		return task_has_idle_policy(task_of(se));
< 	return cfs_rq_is_idle(group_cfs_rq(se));
---
> 	return container_of(cfs_rq, struct rq, cfs);
451c437
< #else	/* !CONFIG_FAIR_GROUP_SCHED */
---
> #define entity_is_task(se)	1
456c442
< static inline void cfs_rq_tg_path(struct cfs_rq *cfs_rq, char *path, int len)
---
> static inline struct cfs_rq *task_cfs_rq(struct task_struct *p)
458,459c444
< 	if (path)
< 		strlcpy(path, "(null)", len);
---
> 	return &task_rq(p)->cfs;
462c447
< static inline bool list_add_leaf_cfs_rq(struct cfs_rq *cfs_rq)
---
> static inline struct cfs_rq *cfs_rq_of(struct sched_entity *se)
464c449,452
< 	return true;
---
> 	struct task_struct *p = task_of(se);
> 	struct rq *rq = task_rq(p);
> 
> 	return &rq->cfs;
467c455,461
< static inline void list_del_leaf_cfs_rq(struct cfs_rq *cfs_rq)
---
> /* runqueue "owned" by this group */
> static inline struct cfs_rq *group_cfs_rq(struct sched_entity *grp)
> {
> 	return NULL;
> }
> 
> static inline void list_add_leaf_cfs_rq(struct cfs_rq *cfs_rq)
471c465
< static inline void assert_list_leaf_cfs_rq(struct rq *rq)
---
> static inline void list_del_leaf_cfs_rq(struct cfs_rq *cfs_rq)
488,502d481
< static inline int tg_is_idle(struct task_group *tg)
< {
< 	return 0;
< }
< 
< static int cfs_rq_is_idle(struct cfs_rq *cfs_rq)
< {
< 	return 0;
< }
< 
< static int se_is_idle(struct sched_entity *se)
< {
< 	return 0;
< }
< 
530c509
< static inline bool entity_before(struct sched_entity *a,
---
> static inline int entity_before(struct sched_entity *a,
536,538d514
< #define __node_2_se(node) \
< 	rb_entry((node), struct sched_entity, run_node)
< 
554c530,531
< 		struct sched_entity *se = __node_2_se(leftmost);
---
> 		struct sched_entity *se;
> 		se = rb_entry(leftmost, struct sched_entity, run_node);
570,574d546
< static inline bool __entity_less(struct rb_node *a, const struct rb_node *b)
< {
< 	return entity_before(__node_2_se(a), __node_2_se(b));
< }
< 
580c552,577
< 	rb_add_cached(&se->run_node, &cfs_rq->tasks_timeline, __entity_less);
---
> 	struct rb_node **link = &cfs_rq->tasks_timeline.rb_root.rb_node;
> 	struct rb_node *parent = NULL;
> 	struct sched_entity *entry;
> 	bool leftmost = true;
> 
> 	/*
> 	 * Find the right place in the rbtree:
> 	 */
> 	while (*link) {
> 		parent = *link;
> 		entry = rb_entry(parent, struct sched_entity, run_node);
> 		/*
> 		 * We dont care about collisions. Nodes with
> 		 * the same key stay together.
> 		 */
> 		if (entity_before(se, entry)) {
> 			link = &parent->rb_left;
> 		} else {
> 			link = &parent->rb_right;
> 			leftmost = false;
> 		}
> 	}
> 
> 	rb_link_node(&se->run_node, parent, link);
> 	rb_insert_color_cached(&se->run_node,
> 			       &cfs_rq->tasks_timeline, leftmost);
595c592
< 	return __node_2_se(left);
---
> 	return rb_entry(left, struct sched_entity, run_node);
605c602
< 	return __node_2_se(next);
---
> 	return rb_entry(next, struct sched_entity, run_node);
616c613
< 	return __node_2_se(last);
---
> 	return rb_entry(last, struct sched_entity, run_node);
623c620,622
< int sched_update_scaling(void)
---
> int sched_proc_update_handler(struct ctl_table *table, int write,
> 		void __user *buffer, size_t *lenp,
> 		loff_t *ppos)
624a624
> 	int ret = proc_dointvec_minmax(table, write, buffer, lenp, ppos);
626a627,629
> 	if (ret || !write)
> 		return ret;
> 
676,682c679
< 	unsigned int nr_running = cfs_rq->nr_running;
< 	u64 slice;
< 
< 	if (sched_feat(ALT_PERIOD))
< 		nr_running = rq_of(cfs_rq)->cfs.h_nr_running;
< 
< 	slice = __sched_period(nr_running + !se->on_rq);
---
> 	u64 slice = __sched_period(cfs_rq->nr_running + !se->on_rq);
699,702d695
< 
< 	if (sched_feat(BASE_SLICE))
< 		slice = max(slice, (u64)sysctl_sched_min_granularity);
< 
716d708
< #include "pelt.h"
718a711,712
> #include "sched-pelt.h"
> 
721d714
< static unsigned long capacity_of(int cpu);
731c724
< 	 * Tasks are initialized with full load to be seen as heavy tasks until
---
> 	 * Tasks are intialized with full load to be seen as heavy tasks until
733c726
< 	 * Group entities are initialized with zero load to reflect the fact that
---
> 	 * Group entities are intialized with zero load to reflect the fact that
737c730,732
< 		sa->load_avg = scale_load_down(se->load.weight);
---
> 		sa->runnable_load_avg = sa->load_avg = scale_load_down(se->load.weight);
> 
> 	se->runnable_weight = se->load.weight;
741a737
> static inline u64 cfs_rq_clock_task(struct cfs_rq *cfs_rq);
757c753
<  *   util_avg_cap = (cpu_scale - cfs_rq->avg.util_avg) / 2^n
---
>  *   util_avg_cap = (1024 - cfs_rq->avg.util_avg) / 2^n
759c755
<  * where n denotes the nth task and cpu_scale the CPU capacity.
---
>  * where n denotes the nth task.
761,762c757
<  * For example, for a CPU with 1024 of capacity, a simplest series from
<  * the beginning would be like:
---
>  * For example, a simplest series from the beginning would be like:
770c765
< void post_init_entity_util_avg(struct task_struct *p)
---
> void post_init_entity_util_avg(struct sched_entity *se)
772d766
< 	struct sched_entity *se = &p->se;
775,776c769
< 	long cpu_scale = arch_scale_cpu_capacity(cpu_of(rq_of(cfs_rq)));
< 	long cap = (long)(cpu_scale - cfs_rq->avg.util_avg) / 2;
---
> 	long cap = (long)(SCHED_CAPACITY_SCALE - cfs_rq->avg.util_avg) / 2;
790,804c783,798
< 	sa->runnable_avg = sa->util_avg;
< 
< 	if (p->sched_class != &fair_sched_class) {
< 		/*
< 		 * For !fair tasks do:
< 		 *
< 		update_cfs_rq_load_avg(now, cfs_rq);
< 		attach_entity_load_avg(cfs_rq, se);
< 		switched_from_fair(rq, p);
< 		 *
< 		 * such that the next switched_to_fair() has the
< 		 * expected state.
< 		 */
< 		se->avg.last_update_time = cfs_rq_clock_pelt(cfs_rq);
< 		return;
---
> 	if (entity_is_task(se)) {
> 		struct task_struct *p = task_of(se);
> 		if (p->sched_class != &fair_sched_class) {
> 			/*
> 			 * For !fair tasks do:
> 			 *
> 			update_cfs_rq_load_avg(now, cfs_rq);
> 			attach_entity_load_avg(cfs_rq, se);
> 			switched_from_fair(rq, p);
> 			 *
> 			 * such that the next switched_to_fair() has the
> 			 * expected state.
> 			 */
> 			se->avg.last_update_time = cfs_rq_clock_task(cfs_rq);
> 			return;
> 		}
814c808
< void post_init_entity_util_avg(struct task_struct *p)
---
> void post_init_entity_util_avg(struct sched_entity *se)
817c811
< static void update_tg_load_avg(struct cfs_rq *cfs_rq)
---
> static void update_tg_load_avg(struct cfs_rq *cfs_rq, int force)
880c874
< 	__schedstat_set(se->statistics.wait_start, wait_start);
---
> 	schedstat_set(se->statistics.wait_start, wait_start);
892,900d885
< 	/*
< 	 * When the sched_schedstat changes from 0 to 1, some sched se
< 	 * maybe already in the runqueue, the se->statistics.wait_start
< 	 * will be 0.So it will let the delta wrong. We need to avoid this
< 	 * scenario.
< 	 */
< 	if (unlikely(!schedstat_val(se->statistics.wait_start)))
< 		return;
< 
911c896
< 			__schedstat_set(se->statistics.wait_start, delta);
---
> 			schedstat_set(se->statistics.wait_start, delta);
917c902
< 	__schedstat_set(se->statistics.wait_max,
---
> 	schedstat_set(se->statistics.wait_max,
919,921c904,906
< 	__schedstat_inc(se->statistics.wait_count);
< 	__schedstat_add(se->statistics.wait_sum, delta);
< 	__schedstat_set(se->statistics.wait_start, 0);
---
> 	schedstat_inc(se->statistics.wait_count);
> 	schedstat_add(se->statistics.wait_sum, delta);
> 	schedstat_set(se->statistics.wait_start, 0);
946c931
< 			__schedstat_set(se->statistics.sleep_max, delta);
---
> 			schedstat_set(se->statistics.sleep_max, delta);
948,949c933,934
< 		__schedstat_set(se->statistics.sleep_start, 0);
< 		__schedstat_add(se->statistics.sum_sleep_runtime, delta);
---
> 		schedstat_set(se->statistics.sleep_start, 0);
> 		schedstat_add(se->statistics.sum_sleep_runtime, delta);
963c948
< 			__schedstat_set(se->statistics.block_max, delta);
---
> 			schedstat_set(se->statistics.block_max, delta);
965,966c950,951
< 		__schedstat_set(se->statistics.block_start, 0);
< 		__schedstat_add(se->statistics.sum_sleep_runtime, delta);
---
> 		schedstat_set(se->statistics.block_start, 0);
> 		schedstat_add(se->statistics.sum_sleep_runtime, delta);
970,971c955,956
< 				__schedstat_add(se->statistics.iowait_sum, delta);
< 				__schedstat_inc(se->statistics.iowait_count);
---
> 				schedstat_add(se->statistics.iowait_sum, delta);
> 				schedstat_inc(se->statistics.iowait_count);
1028d1012
< 		unsigned int state;
1030,1033c1014,1015
< 		/* XXX racy against TTWU */
< 		state = READ_ONCE(tsk->__state);
< 		if (state & TASK_INTERRUPTIBLE)
< 			__schedstat_set(se->statistics.sleep_start,
---
> 		if (tsk->state & TASK_INTERRUPTIBLE)
> 			schedstat_set(se->statistics.sleep_start,
1035,1036c1017,1018
< 		if (state & TASK_UNINTERRUPTIBLE)
< 			__schedstat_set(se->statistics.block_start,
---
> 		if (tsk->state & TASK_UNINTERRUPTIBLE)
> 			schedstat_set(se->statistics.block_start,
1073c1055
< 	refcount_t refcount;
---
> 	atomic_t refcount;
1089c1071
< 	unsigned long faults[];
---
> 	unsigned long faults[0];
1092,1106d1073
< /*
<  * For functions that can be called in multiple contexts that permit reading
<  * ->numa_group (see struct task_struct for locking rules).
<  */
< static struct numa_group *deref_task_numa_group(struct task_struct *p)
< {
< 	return rcu_dereference_check(p->numa_group, p == current ||
< 		(lockdep_is_held(__rq_lockp(task_rq(p))) && !READ_ONCE(p->on_cpu)));
< }
< 
< static struct numa_group *deref_curr_numa_group(struct task_struct *p)
< {
< 	return rcu_dereference_protected(p->numa_group, p == current);
< }
< 
1129c1096
< /* For sanity's sake, never scan more PTEs than MAX_SCAN_WINDOW MB/sec. */
---
> /* For sanitys sake, never scan more PTEs than MAX_SCAN_WINDOW MB/sec. */
1150d1116
< 	struct numa_group *ng;
1153,1155c1119,1120
< 	rcu_read_lock();
< 	ng = rcu_dereference(p->numa_group);
< 	if (ng) {
---
> 	if (p->numa_group) {
> 		struct numa_group *ng = p->numa_group;
1159c1124
< 		period *= refcount_read(&ng->refcount);
---
> 		period *= atomic_read(&ng->refcount);
1163d1127
< 	rcu_read_unlock();
1172d1135
< 	struct numa_group *ng;
1178,1179c1141,1142
< 	ng = deref_curr_numa_group(p);
< 	if (ng) {
---
> 	if (p->numa_group) {
> 		struct numa_group *ng = p->numa_group;
1184c1147
< 		period *= refcount_read(&ng->refcount);
---
> 		period *= atomic_read(&ng->refcount);
1196c1159
< 	rq->nr_numa_running += (p->numa_preferred_nid != NUMA_NO_NODE);
---
> 	rq->nr_numa_running += (p->numa_preferred_nid != -1);
1202c1165
< 	rq->nr_numa_running -= (p->numa_preferred_nid != NUMA_NO_NODE);
---
> 	rq->nr_numa_running -= (p->numa_preferred_nid != -1);
1217,1226c1180
< 	struct numa_group *ng;
< 	pid_t gid = 0;
< 
< 	rcu_read_lock();
< 	ng = rcu_dereference(p->numa_group);
< 	if (ng)
< 		gid = ng->gid;
< 	rcu_read_unlock();
< 
< 	return gid;
---
> 	return p->numa_group ? p->numa_group->gid : 0;
1230c1184
<  * The averaged statistics, shared & private, memory & CPU,
---
>  * The averaged statistics, shared & private, memory & cpu,
1251,1253c1205
< 	struct numa_group *ng = deref_task_numa_group(p);
< 
< 	if (!ng)
---
> 	if (!p->numa_group)
1256,1257c1208,1209
< 	return ng->faults[task_faults_idx(NUMA_MEM, nid, 0)] +
< 		ng->faults[task_faults_idx(NUMA_MEM, nid, 1)];
---
> 	return p->numa_group->faults[task_faults_idx(NUMA_MEM, nid, 0)] +
> 		p->numa_group->faults[task_faults_idx(NUMA_MEM, nid, 1)];
1339c1291
< 					dist >= maxdist)
---
> 					dist > maxdist)
1395d1346
< 	struct numa_group *ng = deref_task_numa_group(p);
1398c1349
< 	if (!ng)
---
> 	if (!p->numa_group)
1401c1352
< 	total_faults = ng->total_faults;
---
> 	total_faults = p->numa_group->total_faults;
1415c1366
< 	struct numa_group *ng = deref_curr_numa_group(p);
---
> 	struct numa_group *ng = p->numa_group;
1420,1430d1370
< 	last_cpupid = page_cpupid_xchg_last(page, this_cpupid);
< 
< 	/*
< 	 * Allow first faults or private faults to migrate immediately early in
< 	 * the lifetime of a task. The magic number 4 is based on waiting for
< 	 * two full passes of the "multi-stage node selection" test that is
< 	 * executed below.
< 	 */
< 	if ((p->numa_preferred_nid == NUMA_NO_NODE || p->numa_scan_seq <= 4) &&
< 	    (cpupid_pid_unset(last_cpupid) || cpupid_match_pid(p, last_cpupid)))
< 		return true;
1448a1389
> 	last_cpupid = page_cpupid_xchg_last(page, this_cpupid);
1481,1497c1422,1425
< /*
<  * 'numa_type' describes the node at the moment of load balancing.
<  */
< enum numa_type {
< 	/* The node has spare capacity that can be used to run more tasks.  */
< 	node_has_spare = 0,
< 	/*
< 	 * The node is fully used and the tasks don't compete for more CPU
< 	 * cycles. Nevertheless, some tasks might wait before running.
< 	 */
< 	node_fully_busy,
< 	/*
< 	 * The node is overloaded and can't provide expected CPU cycles to all
< 	 * tasks.
< 	 */
< 	node_overloaded
< };
---
> static unsigned long weighted_cpuload(struct rq *rq);
> static unsigned long source_load(int cpu, int type);
> static unsigned long target_load(int cpu, int type);
> static unsigned long capacity_of(int cpu);
1500a1429
> 	unsigned long nr_running;
1502,1503c1431
< 	unsigned long runnable;
< 	unsigned long util;
---
> 
1506,1509c1434,1437
< 	unsigned int nr_running;
< 	unsigned int weight;
< 	enum numa_type node_type;
< 	int idle_cpu;
---
> 
> 	/* Approximate capacity in terms of runnable tasks on a node */
> 	unsigned long task_capacity;
> 	int has_free_capacity;
1512c1440,1443
< static inline bool is_core_idle(int cpu)
---
> /*
>  * XXX borrowed from update_sg_lb_stats
>  */
> static void update_numa_stats(struct numa_stats *ns, int nid)
1514,1515c1445,1446
< #ifdef CONFIG_SCHED_SMT
< 	int sibling;
---
> 	int smt, cpu, cpus = 0;
> 	unsigned long capacity;
1517,1519c1448,1450
< 	for_each_cpu(sibling, cpu_smt_mask(cpu)) {
< 		if (cpu == sibling)
< 			continue;
---
> 	memset(ns, 0, sizeof(*ns));
> 	for_each_cpu(cpu, cpumask_of_node(nid)) {
> 		struct rq *rq = cpu_rq(cpu);
1521,1522c1452,1456
< 		if (!idle_cpu(sibling))
< 			return false;
---
> 		ns->nr_running += rq->nr_running;
> 		ns->load += weighted_cpuload(rq);
> 		ns->compute_capacity += capacity_of(cpu);
> 
> 		cpus++;
1524d1457
< #endif
1526c1459,1476
< 	return true;
---
> 	/*
> 	 * If we raced with hotplug and there are no CPUs left in our mask
> 	 * the @ns structure is NULL'ed and task_numa_compare() will
> 	 * not find this node attractive.
> 	 *
> 	 * We'll either bail at !has_free_capacity, or we'll detect a huge
> 	 * imbalance and bail there.
> 	 */
> 	if (!cpus)
> 		return;
> 
> 	/* smt := ceil(cpus / capacity), assumes: 1 < smt_power < 2 */
> 	smt = DIV_ROUND_UP(SCHED_CAPACITY_SCALE * cpus, ns->compute_capacity);
> 	capacity = cpus / smt; /* cores */
> 
> 	ns->task_capacity = min_t(unsigned, capacity,
> 		DIV_ROUND_CLOSEST(ns->compute_capacity, SCHED_CAPACITY_SCALE));
> 	ns->has_free_capacity = (ns->nr_running < ns->task_capacity);
1545,1638d1494
< static unsigned long cpu_load(struct rq *rq);
< static unsigned long cpu_runnable(struct rq *rq);
< static unsigned long cpu_util(int cpu);
< static inline long adjust_numa_imbalance(int imbalance,
< 					int dst_running, int dst_weight);
< 
< static inline enum
< numa_type numa_classify(unsigned int imbalance_pct,
< 			 struct numa_stats *ns)
< {
< 	if ((ns->nr_running > ns->weight) &&
< 	    (((ns->compute_capacity * 100) < (ns->util * imbalance_pct)) ||
< 	     ((ns->compute_capacity * imbalance_pct) < (ns->runnable * 100))))
< 		return node_overloaded;
< 
< 	if ((ns->nr_running < ns->weight) ||
< 	    (((ns->compute_capacity * 100) > (ns->util * imbalance_pct)) &&
< 	     ((ns->compute_capacity * imbalance_pct) > (ns->runnable * 100))))
< 		return node_has_spare;
< 
< 	return node_fully_busy;
< }
< 
< #ifdef CONFIG_SCHED_SMT
< /* Forward declarations of select_idle_sibling helpers */
< static inline bool test_idle_cores(int cpu, bool def);
< static inline int numa_idle_core(int idle_core, int cpu)
< {
< 	if (!static_branch_likely(&sched_smt_present) ||
< 	    idle_core >= 0 || !test_idle_cores(cpu, false))
< 		return idle_core;
< 
< 	/*
< 	 * Prefer cores instead of packing HT siblings
< 	 * and triggering future load balancing.
< 	 */
< 	if (is_core_idle(cpu))
< 		idle_core = cpu;
< 
< 	return idle_core;
< }
< #else
< static inline int numa_idle_core(int idle_core, int cpu)
< {
< 	return idle_core;
< }
< #endif
< 
< /*
<  * Gather all necessary information to make NUMA balancing placement
<  * decisions that are compatible with standard load balancer. This
<  * borrows code and logic from update_sg_lb_stats but sharing a
<  * common implementation is impractical.
<  */
< static void update_numa_stats(struct task_numa_env *env,
< 			      struct numa_stats *ns, int nid,
< 			      bool find_idle)
< {
< 	int cpu, idle_core = -1;
< 
< 	memset(ns, 0, sizeof(*ns));
< 	ns->idle_cpu = -1;
< 
< 	rcu_read_lock();
< 	for_each_cpu(cpu, cpumask_of_node(nid)) {
< 		struct rq *rq = cpu_rq(cpu);
< 
< 		ns->load += cpu_load(rq);
< 		ns->runnable += cpu_runnable(rq);
< 		ns->util += cpu_util(cpu);
< 		ns->nr_running += rq->cfs.h_nr_running;
< 		ns->compute_capacity += capacity_of(cpu);
< 
< 		if (find_idle && !rq->nr_running && idle_cpu(cpu)) {
< 			if (READ_ONCE(rq->numa_migrate_on) ||
< 			    !cpumask_test_cpu(cpu, env->p->cpus_ptr))
< 				continue;
< 
< 			if (ns->idle_cpu == -1)
< 				ns->idle_cpu = cpu;
< 
< 			idle_core = numa_idle_core(idle_core, cpu);
< 		}
< 	}
< 	rcu_read_unlock();
< 
< 	ns->weight = cpumask_weight(cpumask_of_node(nid));
< 
< 	ns->node_type = numa_classify(env->imbalance_pct, ns);
< 
< 	if (idle_core >= 0)
< 		ns->idle_cpu = idle_core;
< }
< 
1642,1675d1497
< 	struct rq *rq = cpu_rq(env->dst_cpu);
< 
< 	/* Check if run-queue part of active NUMA balance. */
< 	if (env->best_cpu != env->dst_cpu && xchg(&rq->numa_migrate_on, 1)) {
< 		int cpu;
< 		int start = env->dst_cpu;
< 
< 		/* Find alternative idle CPU. */
< 		for_each_cpu_wrap(cpu, cpumask_of_node(env->dst_nid), start) {
< 			if (cpu == env->best_cpu || !idle_cpu(cpu) ||
< 			    !cpumask_test_cpu(cpu, env->p->cpus_ptr)) {
< 				continue;
< 			}
< 
< 			env->dst_cpu = cpu;
< 			rq = cpu_rq(env->dst_cpu);
< 			if (!xchg(&rq->numa_migrate_on, 1))
< 				goto assign;
< 		}
< 
< 		/* Failed to find an alternative idle CPU */
< 		return;
< 	}
< 
< assign:
< 	/*
< 	 * Clear previous best_cpu/rq numa-migrate flag, since task now
< 	 * found a better CPU to move/swap.
< 	 */
< 	if (env->best_cpu != -1 && env->best_cpu != env->dst_cpu) {
< 		rq = cpu_rq(env->best_cpu);
< 		WRITE_ONCE(rq->numa_migrate_on, 0);
< 	}
< 
1703c1525,1533
< 	imb = abs(dst_load * src_capacity - src_load * dst_capacity);
---
> 	/* We care about the slope of the imbalance, not the direction. */
> 	if (dst_load < src_load)
> 		swap(dst_load, src_load);
> 
> 	/* Is the difference below the threshold? */
> 	imb = dst_load * src_capacity * 100 -
> 	      src_load * dst_capacity * env->imbalance_pct;
> 	if (imb <= 0)
> 		return false;
1704a1535,1538
> 	/*
> 	 * The imbalance is above the allowed threshold.
> 	 * Compare it with the old imbalance.
> 	 */
1708c1542,1546
< 	old_imb = abs(orig_dst_load * src_capacity - orig_src_load * dst_capacity);
---
> 	if (orig_dst_load < orig_src_load)
> 		swap(orig_dst_load, orig_src_load);
> 
> 	old_imb = orig_dst_load * src_capacity * 100 -
> 		  orig_src_load * dst_capacity * env->imbalance_pct;
1715,1721d1552
<  * Maximum NUMA importance can be 1998 (2*999);
<  * SMALLIMP @ 30 would be close to 1998/64.
<  * Used to deter task migration.
<  */
< #define SMALLIMP	30
< 
< /*
1727,1728c1558,1559
< static bool task_numa_compare(struct task_numa_env *env,
< 			      long taskimp, long groupimp, bool maymove)
---
> static void task_numa_compare(struct task_numa_env *env,
> 			      long taskimp, long groupimp)
1730c1561
< 	struct numa_group *cur_ng, *p_ng = deref_curr_numa_group(env->p);
---
> 	struct rq *src_rq = cpu_rq(env->src_cpu);
1732d1562
< 	long imp = p_ng ? groupimp : taskimp;
1735,1736d1564
< 	int dist = env->dist;
< 	long moveimp = imp;
1738,1741c1566,1568
< 	bool stopsearch = false;
< 
< 	if (READ_ONCE(dst_rq->numa_migrate_on))
< 		return false;
---
> 	long imp = env->p->numa_group ? groupimp : taskimp;
> 	long moveimp = imp;
> 	int dist = env->dist;
1744c1571
< 	cur = rcu_dereference(dst_rq->curr);
---
> 	cur = task_rcu_dereference(&dst_rq->curr);
1752,1774c1579
< 	if (cur == env->p) {
< 		stopsearch = true;
< 		goto unlock;
< 	}
< 
< 	if (!cur) {
< 		if (maymove && moveimp >= env->best_imp)
< 			goto assign;
< 		else
< 			goto unlock;
< 	}
< 
< 	/* Skip this swap candidate if cannot move to the source cpu. */
< 	if (!cpumask_test_cpu(env->src_cpu, cur->cpus_ptr))
< 		goto unlock;
< 
< 	/*
< 	 * Skip this swap candidate if it is not moving to its preferred
< 	 * node and the best task is.
< 	 */
< 	if (env->best_task &&
< 	    env->best_task->numa_preferred_nid == env->src_nid &&
< 	    cur->numa_preferred_nid != env->src_nid) {
---
> 	if (cur == env->p)
1776d1580
< 	}
1782c1586
< 	 * the value is, the more remote accesses that would be expected to
---
> 	 * the value is, the more rmeote accesses that would be expected to
1784,1786d1587
< 	 *
< 	 * If dst and source tasks are in the same NUMA group, or not
< 	 * in any group then look only at task weights.
1788,1798c1589,1593
< 	cur_ng = rcu_dereference(cur->numa_group);
< 	if (cur_ng == p_ng) {
< 		imp = taskimp + task_weight(cur, env->src_nid, dist) -
< 		      task_weight(cur, env->dst_nid, dist);
< 		/*
< 		 * Add some hysteresis to prevent swapping the
< 		 * tasks within a group over tiny differences.
< 		 */
< 		if (cur_ng)
< 			imp -= imp / 16;
< 	} else {
---
> 	if (cur) {
> 		/* Skip this swap candidate if cannot move to the source cpu */
> 		if (!cpumask_test_cpu(env->src_cpu, &cur->cpus_allowed))
> 			goto unlock;
> 
1800,1801c1595,1596
< 		 * Compare the group weights. If a task is all by itself
< 		 * (not part of a group), use the task weight instead.
---
> 		 * If dst and source tasks are in the same NUMA group, or not
> 		 * in any group then look only at task weights.
1803,1808c1598,1619
< 		if (cur_ng && p_ng)
< 			imp += group_weight(cur, env->src_nid, dist) -
< 			       group_weight(cur, env->dst_nid, dist);
< 		else
< 			imp += task_weight(cur, env->src_nid, dist) -
< 			       task_weight(cur, env->dst_nid, dist);
---
> 		if (cur->numa_group == env->p->numa_group) {
> 			imp = taskimp + task_weight(cur, env->src_nid, dist) -
> 			      task_weight(cur, env->dst_nid, dist);
> 			/*
> 			 * Add some hysteresis to prevent swapping the
> 			 * tasks within a group over tiny differences.
> 			 */
> 			if (cur->numa_group)
> 				imp -= imp/16;
> 		} else {
> 			/*
> 			 * Compare the group weights. If a task is all by
> 			 * itself (not part of a group), use the task weight
> 			 * instead.
> 			 */
> 			if (cur->numa_group)
> 				imp += group_weight(cur, env->src_nid, dist) -
> 				       group_weight(cur, env->dst_nid, dist);
> 			else
> 				imp += task_weight(cur, env->src_nid, dist) -
> 				       task_weight(cur, env->dst_nid, dist);
> 		}
1811,1813c1622,1623
< 	/* Discourage picking a task already on its preferred node */
< 	if (cur->numa_preferred_nid == env->dst_nid)
< 		imp -= imp / 16;
---
> 	if (imp <= env->best_imp && moveimp <= env->best_imp)
> 		goto unlock;
1815,1822c1625,1629
< 	/*
< 	 * Encourage picking a task that moves to its preferred node.
< 	 * This potentially makes imp larger than it's maximum of
< 	 * 1998 (see SMALLIMP and task_weight for why) but in this
< 	 * case, it does not matter.
< 	 */
< 	if (cur->numa_preferred_nid == env->src_nid)
< 		imp += imp / 8;
---
> 	if (!cur) {
> 		/* Is there capacity at our destination? */
> 		if (env->src_stats.nr_running <= env->src_stats.task_capacity &&
> 		    !env->dst_stats.has_free_capacity)
> 			goto unlock;
1824,1827c1631
< 	if (maymove && moveimp > imp && moveimp > env->best_imp) {
< 		imp = moveimp;
< 		cur = NULL;
< 		goto assign;
---
> 		goto balance;
1830,1835c1634,1636
< 	/*
< 	 * Prefer swapping with a task moving to its preferred node over a
< 	 * task that is not.
< 	 */
< 	if (env->best_task && cur->numa_preferred_nid == env->src_nid &&
< 	    env->best_task->numa_preferred_nid != env->src_nid) {
---
> 	/* Balance doesn't matter much if we're running a task per cpu */
> 	if (imp > env->best_imp && src_rq->nr_running == 1 &&
> 			dst_rq->nr_running == 1)
1837,1846d1637
< 	}
< 
< 	/*
< 	 * If the NUMA importance is less than SMALLIMP,
< 	 * task migration might only result in ping pong
< 	 * of tasks and also hurt performance due to cache
< 	 * misses.
< 	 */
< 	if (imp < SMALLIMP || imp <= env->best_imp + SMALLIMP / 2)
< 		goto unlock;
1851,1854c1642,1643
< 	load = task_h_load(env->p) - task_h_load(cur);
< 	if (!load)
< 		goto assign;
< 
---
> balance:
> 	load = task_h_load(env->p);
1858,1869c1647
< 	if (load_too_imbalanced(src_load, dst_load, env))
< 		goto unlock;
< 
< assign:
< 	/* Evaluate an idle CPU for a task numa move. */
< 	if (!cur) {
< 		int cpu = env->dst_stats.idle_cpu;
< 
< 		/* Nothing cached so current CPU went idle since the search. */
< 		if (cpu < 0)
< 			cpu = env->dst_cpu;
< 
---
> 	if (moveimp > imp && moveimp > env->best_imp) {
1871,1876c1649,1657
< 		 * If the CPU is no longer truly idle and the previous best CPU
< 		 * is, keep using it.
< 		 */
< 		if (!idle_cpu(cpu) && env->best_cpu >= 0 &&
< 		    idle_cpu(env->best_cpu)) {
< 			cpu = env->best_cpu;
---
> 		 * If the improvement from just moving env->p direction is
> 		 * better than swapping tasks around, check if a move is
> 		 * possible. Store a slightly smaller score than moveimp,
> 		 * so an actually idle CPU will win.
> 		 */
> 		if (!load_too_imbalanced(src_load, dst_load, env)) {
> 			imp = moveimp - 1;
> 			cur = NULL;
> 			goto assign;
1878,1879d1658
< 
< 		env->dst_cpu = cpu;
1882c1661,1662
< 	task_numa_assign(env, cur, imp);
---
> 	if (imp <= env->best_imp)
> 		goto unlock;
1884,1890c1664,1671
< 	/*
< 	 * If a move to idle is allowed because there is capacity or load
< 	 * balance improves then stop the search. While a better swap
< 	 * candidate may exist, a search is not free.
< 	 */
< 	if (maymove && !cur && env->best_cpu >= 0 && idle_cpu(env->best_cpu))
< 		stopsearch = true;
---
> 	if (cur) {
> 		load = task_h_load(cur);
> 		dst_load -= load;
> 		src_load += load;
> 	}
> 
> 	if (load_too_imbalanced(src_load, dst_load, env))
> 		goto unlock;
1893,1894c1674,1675
< 	 * If a swap candidate must be identified and the current best task
< 	 * moves its preferred node then stop the search.
---
> 	 * One idle CPU per node is evaluated for a task numa move.
> 	 * Call select_idle_sibling to maybe find a better one.
1896,1898c1677,1685
< 	if (!maymove && env->best_task &&
< 	    env->best_task->numa_preferred_nid == env->src_nid) {
< 		stopsearch = true;
---
> 	if (!cur) {
> 		/*
> 		 * select_idle_siblings() uses an per-cpu cpumask that
> 		 * can be used from IRQ context.
> 		 */
> 		local_irq_disable();
> 		env->dst_cpu = select_idle_sibling(env->p, env->src_cpu,
> 						   env->dst_cpu);
> 		local_irq_enable();
1899a1687,1689
> 
> assign:
> 	task_numa_assign(env, cur, imp);
1902,1903d1691
< 
< 	return stopsearch;
1909d1696
< 	bool maymove = false;
1912,1952d1698
< 	/*
< 	 * If dst node has spare capacity, then check if there is an
< 	 * imbalance that would be overruled by the load balancer.
< 	 */
< 	if (env->dst_stats.node_type == node_has_spare) {
< 		unsigned int imbalance;
< 		int src_running, dst_running;
< 
< 		/*
< 		 * Would movement cause an imbalance? Note that if src has
< 		 * more running tasks that the imbalance is ignored as the
< 		 * move improves the imbalance from the perspective of the
< 		 * CPU load balancer.
< 		 * */
< 		src_running = env->src_stats.nr_running - 1;
< 		dst_running = env->dst_stats.nr_running + 1;
< 		imbalance = max(0, dst_running - src_running);
< 		imbalance = adjust_numa_imbalance(imbalance, dst_running,
< 							env->dst_stats.weight);
< 
< 		/* Use idle CPU if there is no imbalance */
< 		if (!imbalance) {
< 			maymove = true;
< 			if (env->dst_stats.idle_cpu >= 0) {
< 				env->dst_cpu = env->dst_stats.idle_cpu;
< 				task_numa_assign(env, NULL, 0);
< 				return;
< 			}
< 		}
< 	} else {
< 		long src_load, dst_load, load;
< 		/*
< 		 * If the improvement from just moving env->p direction is better
< 		 * than swapping tasks around, check if a move is possible.
< 		 */
< 		load = task_h_load(env->p);
< 		dst_load = env->dst_stats.load + load;
< 		src_load = env->src_stats.load - load;
< 		maymove = !load_too_imbalanced(src_load, dst_load, env);
< 	}
< 
1955c1701
< 		if (!cpumask_test_cpu(cpu, env->p->cpus_ptr))
---
> 		if (!cpumask_test_cpu(cpu, &env->p->cpus_allowed))
1959,1960c1705
< 		if (task_numa_compare(env, taskimp, groupimp, maymove))
< 			break;
---
> 		task_numa_compare(env, taskimp, groupimp);
1963a1709,1733
> /* Only move tasks to a NUMA node less busy than the current node. */
> static bool numa_has_capacity(struct task_numa_env *env)
> {
> 	struct numa_stats *src = &env->src_stats;
> 	struct numa_stats *dst = &env->dst_stats;
> 
> 	if (src->has_free_capacity && !dst->has_free_capacity)
> 		return false;
> 
> 	/*
> 	 * Only consider a task move if the source has a higher load
> 	 * than the destination, corrected for CPU capacity on each node.
> 	 *
> 	 *      src->load                dst->load
> 	 * --------------------- vs ---------------------
> 	 * src->compute_capacity    dst->compute_capacity
> 	 */
> 	if (src->load * dst->compute_capacity * env->imbalance_pct >
> 
> 	    dst->load * src->compute_capacity * 100)
> 		return true;
> 
> 	return false;
> }
> 
1978d1747
< 	unsigned long taskweight, groupweight;
1980,1982c1749
< 	long taskimp, groupimp;
< 	struct numa_group *ng;
< 	struct rq *best_rq;
---
> 	unsigned long taskweight, groupweight;
1983a1751
> 	long taskimp, groupimp;
2006c1774
< 		sched_setnuma(p, task_node(p));
---
> 		p->numa_preferred_nid = task_node(p);
2014c1782
< 	update_numa_stats(&env, &env.src_stats, env.src_nid, false);
---
> 	update_numa_stats(&env.src_stats, env.src_nid);
2017c1785
< 	update_numa_stats(&env, &env.dst_stats, env.dst_nid, true);
---
> 	update_numa_stats(&env.dst_stats, env.dst_nid);
2020c1788,1789
< 	task_numa_find_cpu(&env, taskimp, groupimp);
---
> 	if (numa_has_capacity(&env))
> 		task_numa_find_cpu(&env, taskimp, groupimp);
2029,2030c1798
< 	ng = deref_curr_numa_group(p);
< 	if (env.best_cpu == -1 || (ng && ng->active_nodes > 1)) {
---
> 	if (env.best_cpu == -1 || (p->numa_group && p->numa_group->active_nodes > 1)) {
2050,2051c1818,1820
< 			update_numa_stats(&env, &env.dst_stats, env.dst_nid, true);
< 			task_numa_find_cpu(&env, taskimp, groupimp);
---
> 			update_numa_stats(&env.dst_stats, env.dst_nid);
> 			if (numa_has_capacity(&env))
> 				task_numa_find_cpu(&env, taskimp, groupimp);
2063c1832,1834
< 	if (ng) {
---
> 	if (p->numa_group) {
> 		struct numa_group *ng = p->numa_group;
> 
2067c1838
< 			nid = cpu_to_node(env.best_cpu);
---
> 			nid = env.dst_nid;
2069,2070c1840,1841
< 		if (nid != p->numa_preferred_nid)
< 			sched_setnuma(p, nid);
---
> 		if (ng->active_nodes > 1 && numa_is_active_node(env.dst_nid, ng))
> 			sched_setnuma(p, env.dst_nid);
2074,2075c1845
< 	if (env.best_cpu == -1) {
< 		trace_sched_stick_numa(p, env.src_cpu, NULL, -1);
---
> 	if (env.best_cpu == -1)
2077d1846
< 	}
2079c1848,1853
< 	best_rq = cpu_rq(env.best_cpu);
---
> 	/*
> 	 * Reset the scan period if the task is being rescheduled on an
> 	 * alternative node to recheck if the tasks is now properly placed.
> 	 */
> 	p->numa_scan_period = task_scan_start(p);
> 
2082d1855
< 		WRITE_ONCE(best_rq->numa_migrate_on, 0);
2084c1857
< 			trace_sched_stick_numa(p, env.src_cpu, NULL, env.best_cpu);
---
> 			trace_sched_stick_numa(p, env.src_cpu, env.best_cpu);
2088,2090c1861
< 	ret = migrate_swap(p, env.best_task, env.best_cpu, env.src_cpu);
< 	WRITE_ONCE(best_rq->numa_migrate_on, 0);
< 
---
> 	ret = migrate_swap(p, env.best_task);
2092c1863
< 		trace_sched_stick_numa(p, env.src_cpu, env.best_task, env.best_cpu);
---
> 		trace_sched_stick_numa(p, env.src_cpu, task_cpu(env.best_task));
2103c1874
< 	if (unlikely(p->numa_preferred_nid == NUMA_NO_NODE || !p->numa_faults))
---
> 	if (unlikely(p->numa_preferred_nid == -1 || !p->numa_faults))
2249,2252d2019
< 
< 		/* Avoid time going backwards, prevent potential divide error: */
< 		if (unlikely((s64)*period < 0))
< 			*period = 0;
2354,2355c2121,2122
< 	int seq, nid, max_nid = NUMA_NO_NODE;
< 	unsigned long max_faults = 0;
---
> 	int seq, nid, max_nid = -1, max_group_nid = -1;
> 	unsigned long max_faults = 0, max_group_faults = 0;
2360d2126
< 	struct numa_group *ng;
2378,2380c2144,2145
< 	ng = deref_curr_numa_group(p);
< 	if (ng) {
< 		group_lock = &ng->lock;
---
> 	if (p->numa_group) {
> 		group_lock = &p->numa_group->lock;
2421c2186
< 			if (ng) {
---
> 			if (p->numa_group) {
2429,2432c2194,2197
< 				ng->faults[mem_idx] += diff;
< 				ng->faults_cpu[mem_idx] += f_diff;
< 				ng->total_faults += diff;
< 				group_faults += ng->faults[mem_idx];
---
> 				p->numa_group->faults[mem_idx] += diff;
> 				p->numa_group->faults_cpu[mem_idx] += f_diff;
> 				p->numa_group->total_faults += diff;
> 				group_faults += p->numa_group->faults[mem_idx];
2436,2442c2201,2202
< 		if (!ng) {
< 			if (faults > max_faults) {
< 				max_faults = faults;
< 				max_nid = nid;
< 			}
< 		} else if (group_faults > max_faults) {
< 			max_faults = group_faults;
---
> 		if (faults > max_faults) {
> 			max_faults = faults;
2444a2205,2209
> 
> 		if (group_faults > max_group_faults) {
> 			max_group_faults = group_faults;
> 			max_group_nid = nid;
> 		}
2447,2448c2212,2215
< 	if (ng) {
< 		numa_group_count_active_nodes(ng);
---
> 	update_task_scan_period(p, fault_types[0], fault_types[1]);
> 
> 	if (p->numa_group) {
> 		numa_group_count_active_nodes(p->numa_group);
2450c2217
< 		max_nid = preferred_group_nid(p, max_nid);
---
> 		max_nid = preferred_group_nid(p, max_group_nid);
2457d2223
< 	}
2459c2225,2227
< 	update_task_scan_period(p, fault_types[0], fault_types[1]);
---
> 		if (task_node(p) != p->numa_preferred_nid)
> 			numa_migrate_preferred(p);
> 	}
2464c2232
< 	return refcount_inc_not_zero(&grp->refcount);
---
> 	return atomic_inc_not_zero(&grp->refcount);
2469c2237
< 	if (refcount_dec_and_test(&grp->refcount))
---
> 	if (atomic_dec_and_test(&grp->refcount))
2482c2250
< 	if (unlikely(!deref_curr_numa_group(p))) {
---
> 	if (unlikely(!p->numa_group)) {
2490c2258
< 		refcount_set(&grp->refcount, 1);
---
> 		atomic_set(&grp->refcount, 1);
2518c2286
< 	my_grp = deref_curr_numa_group(p);
---
> 	my_grp = p->numa_group;
2580,2591c2348,2351
< /*
<  * Get rid of NUMA statistics associated with a task (either current or dead).
<  * If @final is set, the task is dead and has reached refcount zero, so we can
<  * safely free all relevant data structures. Otherwise, there might be
<  * concurrent reads from places like load balancing and procfs, and we should
<  * reset the data back to default state without freeing ->numa_faults.
<  */
< void task_numa_free(struct task_struct *p, bool final)
< {
< 	/* safe: p either is current or is being freed by current */
< 	struct numa_group *grp = rcu_dereference_raw(p->numa_group);
< 	unsigned long *numa_faults = p->numa_faults;
---
> void task_numa_free(struct task_struct *p)
> {
> 	struct numa_group *grp = p->numa_group;
> 	void *numa_faults = p->numa_faults;
2595,2597d2354
< 	if (!numa_faults)
< 		return;
< 
2610,2617c2367,2368
< 	if (final) {
< 		p->numa_faults = NULL;
< 		kfree(numa_faults);
< 	} else {
< 		p->total_numa_faults = 0;
< 		for (i = 0; i < NR_NUMA_HINT_FAULT_STATS * nr_node_ids; i++)
< 			numa_faults[i] = 0;
< 	}
---
> 	p->numa_faults = NULL;
> 	kfree(numa_faults);
2670c2421
< 	ng = deref_curr_numa_group(p);
---
> 	ng = p->numa_group;
2675a2427,2428
> 	task_numa_placement(p);
> 
2677,2678c2430,2431
< 	 * Retry to migrate task to preferred node periodically, in case it
< 	 * previously failed, or the scheduler moved us.
---
> 	 * Retry task to preferred node migration periodically, in case it
> 	 * case it previously failed, or the scheduler moved us.
2680,2681c2433
< 	if (time_after(jiffies, p->numa_migrate_retry)) {
< 		task_numa_placement(p);
---
> 	if (time_after(jiffies, p->numa_migrate_retry))
2683d2434
< 	}
2713c2464
< static void task_numa_work(struct callback_head *work)
---
> void task_numa_work(struct callback_head *work)
2726c2477
< 	work->next = work;
---
> 	work->next = work; /* protect against double add */
2773c2524
< 	if (!mmap_read_trylock(mm))
---
> 	if (!down_read_trylock(&mm->mmap_sem))
2801c2552
< 		if (!vma_is_accessible(vma))
---
> 		if (!(vma->vm_flags & (VM_READ | VM_EXEC | VM_WRITE)))
2841c2592
< 	mmap_read_unlock(mm);
---
> 	up_read(&mm->mmap_sem);
2855,2898d2605
< void init_numa_balancing(unsigned long clone_flags, struct task_struct *p)
< {
< 	int mm_users = 0;
< 	struct mm_struct *mm = p->mm;
< 
< 	if (mm) {
< 		mm_users = atomic_read(&mm->mm_users);
< 		if (mm_users == 1) {
< 			mm->numa_next_scan = jiffies + msecs_to_jiffies(sysctl_numa_balancing_scan_delay);
< 			mm->numa_scan_seq = 0;
< 		}
< 	}
< 	p->node_stamp			= 0;
< 	p->numa_scan_seq		= mm ? mm->numa_scan_seq : 0;
< 	p->numa_scan_period		= sysctl_numa_balancing_scan_delay;
< 	/* Protect against double add, see task_tick_numa and task_numa_work */
< 	p->numa_work.next		= &p->numa_work;
< 	p->numa_faults			= NULL;
< 	RCU_INIT_POINTER(p->numa_group, NULL);
< 	p->last_task_numa_placement	= 0;
< 	p->last_sum_exec_runtime	= 0;
< 
< 	init_task_work(&p->numa_work, task_numa_work);
< 
< 	/* New address space, reset the preferred nid */
< 	if (!(clone_flags & CLONE_VM)) {
< 		p->numa_preferred_nid = NUMA_NO_NODE;
< 		return;
< 	}
< 
< 	/*
< 	 * New thread, keep existing numa_preferred_nid which should be copied
< 	 * already by arch_dup_task_struct but stagger when scans start.
< 	 */
< 	if (mm) {
< 		unsigned int delay;
< 
< 		delay = min_t(unsigned int, task_scan_max(current),
< 			current->numa_scan_period * mm_users * NSEC_PER_MSEC);
< 		delay += 2 * TICK_NSEC;
< 		p->node_stamp = delay;
< 	}
< }
< 
2902c2609
< static void task_tick_numa(struct rq *rq, struct task_struct *curr)
---
> void task_tick_numa(struct rq *rq, struct task_struct *curr)
2910c2617
< 	if ((curr->flags & (PF_EXITING | PF_KTHREAD)) || work->next != work)
---
> 	if (!curr->mm || (curr->flags & PF_EXITING) || work->next != work)
2927,2960c2634,2637
< 		if (!time_before(jiffies, curr->mm->numa_next_scan))
< 			task_work_add(curr, work, TWA_RESUME);
< 	}
< }
< 
< static void update_scan_period(struct task_struct *p, int new_cpu)
< {
< 	int src_nid = cpu_to_node(task_cpu(p));
< 	int dst_nid = cpu_to_node(new_cpu);
< 
< 	if (!static_branch_likely(&sched_numa_balancing))
< 		return;
< 
< 	if (!p->mm || !p->numa_faults || (p->flags & PF_EXITING))
< 		return;
< 
< 	if (src_nid == dst_nid)
< 		return;
< 
< 	/*
< 	 * Allow resets if faults have been trapped before one scan
< 	 * has completed. This is most likely due to a new task that
< 	 * is pulled cross-node due to wakeups or load balancing.
< 	 */
< 	if (p->numa_scan_seq) {
< 		/*
< 		 * Avoid scan adjustments if moving to the preferred
< 		 * node or if the task was not previously running on
< 		 * the preferred node.
< 		 */
< 		if (dst_nid == p->numa_preferred_nid ||
< 		    (p->numa_preferred_nid != NUMA_NO_NODE &&
< 			src_nid != p->numa_preferred_nid))
< 			return;
---
> 		if (!time_before(jiffies, curr->mm->numa_next_scan)) {
> 			init_task_work(work, task_numa_work); /* TODO: move this into sched_fork() */
> 			task_work_add(curr, work, true);
> 		}
2962,2963d2638
< 
< 	p->numa_scan_period = task_scan_start(p);
2979,2982d2653
< static inline void update_scan_period(struct task_struct *p, int new_cpu)
< {
< }
< 
2988a2660,2661
> 	if (!parent_entity(se))
> 		update_load_add(&rq_of(cfs_rq)->load, se->load.weight);
3003a2677,2678
> 	if (!parent_entity(se))
> 		update_load_sub(&rq_of(cfs_rq)->load, se->load.weight);
3049a2725
> #ifdef CONFIG_SMP
3051,3054c2727
<  * Remove and clamp on negative, from a local variable.
<  *
<  * A variant of sub_positive(), which does not use explicit load-store
<  * and is thus optimized for local variable updates.
---
>  * XXX we want to get rid of these helpers and use the full load resolution.
3056,3059c2729,2756
< #define lsub_positive(_ptr, _val) do {				\
< 	typeof(_ptr) ptr = (_ptr);				\
< 	*ptr -= min_t(typeof(*ptr), *ptr, _val);		\
< } while (0)
---
> static inline long se_weight(struct sched_entity *se)
> {
> 	return scale_load_down(se->load.weight);
> }
> 
> static inline long se_runnable(struct sched_entity *se)
> {
> 	return scale_load_down(se->runnable_weight);
> }
> 
> static inline void
> enqueue_runnable_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se)
> {
> 	cfs_rq->runnable_weight += se->runnable_weight;
> 
> 	cfs_rq->avg.runnable_load_avg += se->avg.runnable_load_avg;
> 	cfs_rq->avg.runnable_load_sum += se_runnable(se) * se->avg.runnable_load_sum;
> }
> 
> static inline void
> dequeue_runnable_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se)
> {
> 	cfs_rq->runnable_weight -= se->runnable_weight;
> 
> 	sub_positive(&cfs_rq->avg.runnable_load_avg, se->avg.runnable_load_avg);
> 	sub_positive(&cfs_rq->avg.runnable_load_sum,
> 		     se_runnable(se) * se->avg.runnable_load_sum);
> }
3061d2757
< #ifdef CONFIG_SMP
3072d2767
< 	u32 divider = get_pelt_divider(&se->avg);
3074c2769
< 	cfs_rq->avg.load_sum = cfs_rq->avg.load_avg * divider;
---
> 	sub_positive(&cfs_rq->avg.load_sum, se_weight(se) * se->avg.load_sum);
3077a2773,2776
> enqueue_runnable_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se) { }
> static inline void
> dequeue_runnable_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se) { }
> static inline void
3084c2783
< 			    unsigned long weight)
---
> 			    unsigned long weight, unsigned long runnable)
3090c2789,2790
< 		update_load_sub(&cfs_rq->load, se->load.weight);
---
> 		account_entity_dequeue(cfs_rq, se);
> 		dequeue_runnable_load_avg(cfs_rq, se);
3093a2794
> 	se->runnable_weight = runnable;
3098c2799
< 		u32 divider = get_pelt_divider(&se->avg);
---
> 		u32 divider = LOAD_AVG_MAX - 1024 + se->avg.period_contrib;
3100a2802,2803
> 		se->avg.runnable_load_avg =
> 			div_u64(se_runnable(se) * se->avg.runnable_load_sum, divider);
3105,3107c2808,2811
< 	if (se->on_rq)
< 		update_load_add(&cfs_rq->load, se->load.weight);
< 
---
> 	if (se->on_rq) {
> 		account_entity_enqueue(cfs_rq, se);
> 		enqueue_runnable_load_avg(cfs_rq, se);
> 	}
3117c2821
< 	reweight_entity(cfs_rq, se, weight);
---
> 	reweight_entity(cfs_rq, se, weight, weight);
3122c2826
< #ifdef CONFIG_SMP
---
> # ifdef CONFIG_SMP
3132c2836
<  *                       \Sum grq->load.weight
---
>  *			  \Sum grq->load.weight
3146c2850
<  *                             tg->load_avg
---
>  *				tg->load_avg
3162c2866
<  *                         grp->load.weight
---
>  *			    grp->load.weight
3181c2885
<  *                             tg_load_avg'
---
>  *				tg_load_avg'
3229c2933,2977
< #endif /* CONFIG_SMP */
---
> 
> /*
>  * This calculates the effective runnable weight for a group entity based on
>  * the group entity weight calculated above.
>  *
>  * Because of the above approximation (2), our group entity weight is
>  * an load_avg based ratio (3). This means that it includes blocked load and
>  * does not represent the runnable weight.
>  *
>  * Approximate the group entity's runnable weight per ratio from the group
>  * runqueue:
>  *
>  *					     grq->avg.runnable_load_avg
>  *   ge->runnable_weight = ge->load.weight * -------------------------- (7)
>  *						 grq->avg.load_avg
>  *
>  * However, analogous to above, since the avg numbers are slow, this leads to
>  * transients in the from-idle case. Instead we use:
>  *
>  *   ge->runnable_weight = ge->load.weight *
>  *
>  *		max(grq->avg.runnable_load_avg, grq->runnable_weight)
>  *		-----------------------------------------------------	(8)
>  *		      max(grq->avg.load_avg, grq->load.weight)
>  *
>  * Where these max() serve both to use the 'instant' values to fix the slow
>  * from-idle and avoid the /0 on to-idle, similar to (6).
>  */
> static long calc_group_runnable(struct cfs_rq *cfs_rq, long shares)
> {
> 	long runnable, load_avg;
> 
> 	load_avg = max(cfs_rq->avg.load_avg,
> 		       scale_load_down(cfs_rq->load.weight));
> 
> 	runnable = max(cfs_rq->avg.runnable_load_avg,
> 		       scale_load_down(cfs_rq->runnable_weight));
> 
> 	runnable *= shares;
> 	if (load_avg)
> 		runnable /= load_avg;
> 
> 	return clamp_t(long, runnable, MIN_SHARES, shares);
> }
> # endif /* CONFIG_SMP */
3240c2988
< 	long shares;
---
> 	long shares, runnable;
3249c2997
< 	shares = READ_ONCE(gcfs_rq->tg->shares);
---
> 	runnable = shares = READ_ONCE(gcfs_rq->tg->shares);
3254a3003
> 	runnable = calc_group_runnable(gcfs_rq, shares);
3257c3006
< 	reweight_entity(cfs_rq_of(se), se, shares);
---
> 	reweight_entity(cfs_rq_of(se), se, shares, runnable);
3266c3015
< static inline void cfs_rq_util_change(struct cfs_rq *cfs_rq, int flags)
---
> static inline void cfs_rq_util_change(struct cfs_rq *cfs_rq)
3274c3023,3025
< 		 * a real problem.
---
> 		 * a real problem -- added to that it only calls on the local
> 		 * CPU, so if we enqueue remotely we'll miss an update, but
> 		 * the next tick/schedule should update.
3285c3036
< 		cpufreq_update_util(rq, flags);
---
> 		cpufreq_update_util(rq, 0);
3290d3040
< #ifdef CONFIG_FAIR_GROUP_SCHED
3292,3297c3042,3043
<  * Because list_add_leaf_cfs_rq always places a child cfs_rq on the list
<  * immediately before a parent cfs_rq, and cfs_rqs are removed from the list
<  * bottom-up, we only have to test whether the cfs_rq before us on the list
<  * is our child.
<  * If cfs_rq is not on the list, test whether a child needs its to be added to
<  * connect a branch to the tree  * (see list_add_leaf_cfs_rq() for details).
---
>  * Approximate:
>  *   val * y^n,    where y^32 ~= 0.5 (~1 scheduling period)
3299c3045
< static inline bool child_cfs_rq_on_list(struct cfs_rq *cfs_rq)
---
> static u64 decay_load(u64 val, u64 n)
3301,3302c3047
< 	struct cfs_rq *prev_cfs_rq;
< 	struct list_head *prev;
---
> 	unsigned int local_n;
3304,3307c3049,3053
< 	if (cfs_rq->on_list) {
< 		prev = cfs_rq->leaf_cfs_rq_list.prev;
< 	} else {
< 		struct rq *rq = rq_of(cfs_rq);
---
> 	if (unlikely(n > LOAD_AVG_PERIOD * 63))
> 		return 0;
> 
> 	/* after bounds checking we can collapse to 32-bit */
> 	local_n = n;
3309c3055,3064
< 		prev = rq->tmp_alone_branch;
---
> 	/*
> 	 * As y^PERIOD = 1/2, we can combine
> 	 *    y^n = 1/2^(n/PERIOD) * y^(n%PERIOD)
> 	 * With a look-up table which covers y^n (n<PERIOD)
> 	 *
> 	 * To achieve constant time decay_load.
> 	 */
> 	if (unlikely(local_n >= LOAD_AVG_PERIOD)) {
> 		val >>= local_n / LOAD_AVG_PERIOD;
> 		local_n %= LOAD_AVG_PERIOD;
3312c3067,3073
< 	prev_cfs_rq = container_of(prev, struct cfs_rq, leaf_cfs_rq_list);
---
> 	val = mul_u64_u32_shr(val, runnable_avg_yN_inv[local_n], 32);
> 	return val;
> }
> 
> static u32 __accumulate_pelt_segments(u64 periods, u32 d1, u32 d3)
> {
> 	u32 c1, c2, c3 = d3; /* y^0 == 1 */
3314c3075,3091
< 	return (prev_cfs_rq->tg->parent == cfs_rq->tg);
---
> 	/*
> 	 * c1 = d1 y^p
> 	 */
> 	c1 = decay_load((u64)d1, periods);
> 
> 	/*
> 	 *            p-1
> 	 * c2 = 1024 \Sum y^n
> 	 *            n=1
> 	 *
> 	 *              inf        inf
> 	 *    = 1024 ( \Sum y^n - \Sum y^n - y^0 )
> 	 *              n=0        n=p
> 	 */
> 	c2 = LOAD_AVG_MAX - decay_load(LOAD_AVG_MAX, periods) - 1024;
> 
> 	return c1 + c2 + c3;
3317c3094,3119
< static inline bool cfs_rq_is_decayed(struct cfs_rq *cfs_rq)
---
> #define cap_scale(v, s) ((v)*(s) >> SCHED_CAPACITY_SHIFT)
> 
> /*
>  * Accumulate the three separate parts of the sum; d1 the remainder
>  * of the last (incomplete) period, d2 the span of full periods and d3
>  * the remainder of the (incomplete) current period.
>  *
>  *           d1          d2           d3
>  *           ^           ^            ^
>  *           |           |            |
>  *         |<->|<----------------->|<--->|
>  * ... |---x---|------| ... |------|-----x (now)
>  *
>  *                           p-1
>  * u' = (u + d1) y^p + 1024 \Sum y^n + d3 y^0
>  *                           n=1
>  *
>  *    = u y^p +					(Step 1)
>  *
>  *                     p-1
>  *      d1 y^p + 1024 \Sum y^n + d3 y^0		(Step 2)
>  *                     n=1
>  */
> static __always_inline u32
> accumulate_sum(u64 delta, int cpu, struct sched_avg *sa,
> 	       unsigned long load, unsigned long runnable, int running)
3319,3320c3121,3123
< 	if (cfs_rq->load.weight)
< 		return false;
---
> 	unsigned long scale_freq, scale_cpu;
> 	u32 contrib = (u32)delta; /* p == 0 -> delta < 1024 */
> 	u64 periods;
3322,3323c3125,3126
< 	if (cfs_rq->avg.load_sum)
< 		return false;
---
> 	scale_freq = arch_scale_freq_capacity(NULL, cpu);
> 	scale_cpu = arch_scale_cpu_capacity(NULL, cpu);
3325,3326c3128,3129
< 	if (cfs_rq->avg.util_sum)
< 		return false;
---
> 	delta += sa->period_contrib;
> 	periods = delta / 1024; /* A period is 1024us (~1ms) */
3328,3329c3131,3138
< 	if (cfs_rq->avg.runnable_sum)
< 		return false;
---
> 	/*
> 	 * Step 1: decay old *_sum if we crossed period boundaries.
> 	 */
> 	if (periods) {
> 		sa->load_sum = decay_load(sa->load_sum, periods);
> 		sa->runnable_load_sum =
> 			decay_load(sa->runnable_load_sum, periods);
> 		sa->util_sum = decay_load((u64)(sa->util_sum), periods);
3331,3332c3140,3155
< 	if (child_cfs_rq_on_list(cfs_rq))
< 		return false;
---
> 		/*
> 		 * Step 2
> 		 */
> 		delta %= 1024;
> 		contrib = __accumulate_pelt_segments(periods,
> 				1024 - sa->period_contrib, delta);
> 	}
> 	sa->period_contrib = delta;
> 
> 	contrib = cap_scale(contrib, scale_freq);
> 	if (load)
> 		sa->load_sum += load * contrib;
> 	if (runnable)
> 		sa->runnable_load_sum += runnable * contrib;
> 	if (running)
> 		sa->util_sum += contrib * scale_cpu;
3333a3157,3194
> 	return periods;
> }
> 
> /*
>  * We can represent the historical contribution to runnable average as the
>  * coefficients of a geometric series.  To do this we sub-divide our runnable
>  * history into segments of approximately 1ms (1024us); label the segment that
>  * occurred N-ms ago p_N, with p_0 corresponding to the current period, e.g.
>  *
>  * [<- 1024us ->|<- 1024us ->|<- 1024us ->| ...
>  *      p0            p1           p2
>  *     (now)       (~1ms ago)  (~2ms ago)
>  *
>  * Let u_i denote the fraction of p_i that the entity was runnable.
>  *
>  * We then designate the fractions u_i as our co-efficients, yielding the
>  * following representation of historical load:
>  *   u_0 + u_1*y + u_2*y^2 + u_3*y^3 + ...
>  *
>  * We choose y based on the with of a reasonably scheduling period, fixing:
>  *   y^32 = 0.5
>  *
>  * This means that the contribution to load ~32ms ago (u_32) will be weighted
>  * approximately half as much as the contribution to load within the last ms
>  * (u_0).
>  *
>  * When a period "rolls over" and we have new u_0`, multiplying the previous
>  * sum again by y is sufficient to update:
>  *   load_avg = u_0` + y*(u_0 + u_1*y + u_2*y^2 + ... )
>  *            = u_0 + u_1*y + u_2*y^2 + ... [re-labeling u_i --> u_{i+1}]
>  */
> static __always_inline int
> ___update_load_sum(u64 now, int cpu, struct sched_avg *sa,
> 		  unsigned long load, unsigned long runnable, int running)
> {
> 	u64 delta;
> 
> 	delta = now - sa->last_update_time;
3335,3341c3196,3202
< 	 * _avg must be null when _sum are null because _avg = _sum / divider
< 	 * Make sure that rounding and/or propagation of PELT values never
< 	 * break this.
< 	 */
< 	SCHED_WARN_ON(cfs_rq->avg.load_avg ||
< 		      cfs_rq->avg.util_avg ||
< 		      cfs_rq->avg.runnable_avg);
---
> 	 * This should only happen when time goes backwards, which it
> 	 * unfortunately does during sched clock init when we swap over to TSC.
> 	 */
> 	if ((s64)delta < 0) {
> 		sa->last_update_time = now;
> 		return 0;
> 	}
3343c3204,3290
< 	return true;
---
> 	/*
> 	 * Use 1024ns as the unit of measurement since it's a reasonable
> 	 * approximation of 1us and fast to compute.
> 	 */
> 	delta >>= 10;
> 	if (!delta)
> 		return 0;
> 
> 	sa->last_update_time += delta << 10;
> 
> 	/*
> 	 * running is a subset of runnable (weight) so running can't be set if
> 	 * runnable is clear. But there are some corner cases where the current
> 	 * se has been already dequeued but cfs_rq->curr still points to it.
> 	 * This means that weight will be 0 but not running for a sched_entity
> 	 * but also for a cfs_rq if the latter becomes idle. As an example,
> 	 * this happens during idle_balance() which calls
> 	 * update_blocked_averages()
> 	 */
> 	if (!load)
> 		runnable = running = 0;
> 
> 	/*
> 	 * Now we know we crossed measurement unit boundaries. The *_avg
> 	 * accrues by two steps:
> 	 *
> 	 * Step 1: accumulate *_sum since last_update_time. If we haven't
> 	 * crossed period boundaries, finish.
> 	 */
> 	if (!accumulate_sum(delta, cpu, sa, load, runnable, running))
> 		return 0;
> 
> 	return 1;
> }
> 
> static __always_inline void
> ___update_load_avg(struct sched_avg *sa, unsigned long load, unsigned long runnable)
> {
> 	u32 divider = LOAD_AVG_MAX - 1024 + sa->period_contrib;
> 
> 	/*
> 	 * Step 2: update *_avg.
> 	 */
> 	sa->load_avg = div_u64(load * sa->load_sum, divider);
> 	sa->runnable_load_avg =	div_u64(runnable * sa->runnable_load_sum, divider);
> 	sa->util_avg = sa->util_sum / divider;
> }
> 
> /*
>  * sched_entity:
>  *
>  *   task:
>  *     se_runnable() == se_weight()
>  *
>  *   group: [ see update_cfs_group() ]
>  *     se_weight()   = tg->weight * grq->load_avg / tg->load_avg
>  *     se_runnable() = se_weight(se) * grq->runnable_load_avg / grq->load_avg
>  *
>  *   load_sum := runnable_sum
>  *   load_avg = se_weight(se) * runnable_avg
>  *
>  *   runnable_load_sum := runnable_sum
>  *   runnable_load_avg = se_runnable(se) * runnable_avg
>  *
>  * XXX collapse load_sum and runnable_load_sum
>  *
>  * cfq_rs:
>  *
>  *   load_sum = \Sum se_weight(se) * se->avg.load_sum
>  *   load_avg = \Sum se->avg.load_avg
>  *
>  *   runnable_load_sum = \Sum se_runnable(se) * se->avg.runnable_load_sum
>  *   runnable_load_avg = \Sum se->avg.runable_load_avg
>  */
> 
> static int
> __update_load_avg_blocked_se(u64 now, int cpu, struct sched_entity *se)
> {
> 	if (entity_is_task(se))
> 		se->runnable_weight = se->load.weight;
> 
> 	if (___update_load_sum(now, cpu, &se->avg, 0, 0, 0)) {
> 		___update_load_avg(&se->avg, se_weight(se), se_runnable(se));
> 		return 1;
> 	}
> 
> 	return 0;
3345a3293,3324
> static int
> __update_load_avg_se(u64 now, int cpu, struct cfs_rq *cfs_rq, struct sched_entity *se)
> {
> 	if (entity_is_task(se))
> 		se->runnable_weight = se->load.weight;
> 
> 	if (___update_load_sum(now, cpu, &se->avg, !!se->on_rq, !!se->on_rq,
> 				cfs_rq->curr == se)) {
> 
> 		___update_load_avg(&se->avg, se_weight(se), se_runnable(se));
> 		return 1;
> 	}
> 
> 	return 0;
> }
> 
> static int
> __update_load_avg_cfs_rq(u64 now, int cpu, struct cfs_rq *cfs_rq)
> {
> 	if (___update_load_sum(now, cpu, &cfs_rq->avg,
> 				scale_load_down(cfs_rq->load.weight),
> 				scale_load_down(cfs_rq->runnable_weight),
> 				cfs_rq->curr != NULL)) {
> 
> 		___update_load_avg(&cfs_rq->avg, 1, 1);
> 		return 1;
> 	}
> 
> 	return 0;
> }
> 
> #ifdef CONFIG_FAIR_GROUP_SCHED
3348a3328
>  * @force: update regardless of how small the difference
3360c3340
< static inline void update_tg_load_avg(struct cfs_rq *cfs_rq)
---
> static inline void update_tg_load_avg(struct cfs_rq *cfs_rq, int force)
3370c3350
< 	if (abs(delta) > cfs_rq->tg_load_avg_contrib / 64) {
---
> 	if (force || abs(delta) > cfs_rq->tg_load_avg_contrib / 64) {
3377c3357
<  * Called within set_task_rq() right before setting a task's CPU. The
---
>  * Called within set_task_rq() right before setting a task's cpu. The
3421c3401
< 	__update_load_avg_blocked_se(p_last_update_time, se);
---
> 	__update_load_avg_blocked_se(p_last_update_time, cpu_of(rq_of(prev)), se);
3436,3438c3416,3418
<  * Per the above update_tg_cfs_util() and update_tg_cfs_runnable() are trivial
<  * and simply copies the running/runnable sum over (but still wrong, because
<  * the group entity and group rq do not have their PELT windows aligned).
---
>  * Per the above update_tg_cfs_util() is trivial and simply copies the running
>  * sum over (but still wrong, because the group entity and group rq do not have
>  * their PELT windows aligned).
3440c3420
<  * However, update_tg_cfs_load() is more complex. So we have:
---
>  * However, update_tg_cfs_runnable() is more complex. So we have:
3498d3477
< 	u32 divider;
3505,3506c3484,3488
< 	 * cfs_rq->avg.period_contrib can be used for both cfs_rq and se.
< 	 * See ___update_load_avg() for details.
---
> 	 * The relation between sum and avg is:
> 	 *
> 	 *   LOAD_AVG_MAX - 1024 + sa->period_contrib
> 	 *
> 	 * however, the PELT windows are not aligned between grq and gse.
3508d3489
< 	divider = get_pelt_divider(&cfs_rq->avg);
3512c3493
< 	se->avg.util_sum = se->avg.util_avg * divider;
---
> 	se->avg.util_sum = se->avg.util_avg * LOAD_AVG_MAX;
3516c3497
< 	cfs_rq->avg.util_sum = cfs_rq->avg.util_avg * divider;
---
> 	cfs_rq->avg.util_sum = cfs_rq->avg.util_avg * LOAD_AVG_MAX;
3522,3550c3503,3506
< 	long delta = gcfs_rq->avg.runnable_avg - se->avg.runnable_avg;
< 	u32 divider;
< 
< 	/* Nothing to update */
< 	if (!delta)
< 		return;
< 
< 	/*
< 	 * cfs_rq->avg.period_contrib can be used for both cfs_rq and se.
< 	 * See ___update_load_avg() for details.
< 	 */
< 	divider = get_pelt_divider(&cfs_rq->avg);
< 
< 	/* Set new sched_entity's runnable */
< 	se->avg.runnable_avg = gcfs_rq->avg.runnable_avg;
< 	se->avg.runnable_sum = se->avg.runnable_avg * divider;
< 
< 	/* Update parent cfs_rq runnable */
< 	add_positive(&cfs_rq->avg.runnable_avg, delta);
< 	cfs_rq->avg.runnable_sum = cfs_rq->avg.runnable_avg * divider;
< }
< 
< static inline void
< update_tg_cfs_load(struct cfs_rq *cfs_rq, struct sched_entity *se, struct cfs_rq *gcfs_rq)
< {
< 	long delta, running_sum, runnable_sum = gcfs_rq->prop_runnable_sum;
< 	unsigned long load_avg;
< 	u64 load_sum = 0;
< 	u32 divider;
---
> 	long delta_avg, running_sum, runnable_sum = gcfs_rq->prop_runnable_sum;
> 	unsigned long runnable_load_avg, load_avg;
> 	u64 runnable_load_sum, load_sum = 0;
> 	s64 delta_sum;
3557,3562d3512
< 	/*
< 	 * cfs_rq->avg.period_contrib can be used for both cfs_rq and se.
< 	 * See ___update_load_avg() for details.
< 	 */
< 	divider = get_pelt_divider(&cfs_rq->avg);
< 
3569c3519
< 		runnable_sum = min_t(long, runnable_sum, divider);
---
> 		runnable_sum = min(runnable_sum, (long)LOAD_AVG_MAX);
3586,3588c3536,3537
< 	 * Rescale running sum to be in the same range as runnable sum
< 	 * running_sum is in [0 : LOAD_AVG_MAX <<  SCHED_CAPACITY_SHIFT]
< 	 * runnable_sum is in [0 : LOAD_AVG_MAX]
---
> 	 * As running sum is scale with cpu capacity wehreas the runnable sum
> 	 * is not we rescale running_sum 1st
3590c3539,3540
< 	running_sum = se->avg.util_sum >> SCHED_CAPACITY_SHIFT;
---
> 	running_sum = se->avg.util_sum /
> 		arch_scale_cpu_capacity(NULL, cpu_of(rq_of(cfs_rq)));
3594c3544,3547
< 	load_avg = div_s64(load_sum, divider);
---
> 	load_avg = div_s64(load_sum, LOAD_AVG_MAX);
> 
> 	delta_sum = load_sum - (s64)se_weight(se) * se->avg.load_sum;
> 	delta_avg = load_avg - se->avg.load_avg;
3596a3550,3552
> 	se->avg.load_avg = load_avg;
> 	add_positive(&cfs_rq->avg.load_avg, delta_avg);
> 	add_positive(&cfs_rq->avg.load_sum, delta_sum);
3598,3600c3554,3557
< 	delta = load_avg - se->avg.load_avg;
< 	if (!delta)
< 		return;
---
> 	runnable_load_sum = (s64)se_runnable(se) * runnable_sum;
> 	runnable_load_avg = div_s64(runnable_load_sum, LOAD_AVG_MAX);
> 	delta_sum = runnable_load_sum - se_weight(se) * se->avg.runnable_load_sum;
> 	delta_avg = runnable_load_avg - se->avg.runnable_load_avg;
3602c3559,3560
< 	se->avg.load_avg = load_avg;
---
> 	se->avg.runnable_load_sum = runnable_sum;
> 	se->avg.runnable_load_avg = runnable_load_avg;
3604,3605c3562,3565
< 	add_positive(&cfs_rq->avg.load_avg, delta);
< 	cfs_rq->avg.load_sum = cfs_rq->avg.load_avg * divider;
---
> 	if (se->on_rq) {
> 		add_positive(&cfs_rq->avg.runnable_load_avg, delta_avg);
> 		add_positive(&cfs_rq->avg.runnable_load_sum, delta_sum);
> 	}
3634,3637d3593
< 	update_tg_cfs_load(cfs_rq, se, gcfs_rq);
< 
< 	trace_pelt_cfs_tp(cfs_rq);
< 	trace_pelt_se_tp(se);
3674c3630
< static inline void update_tg_load_avg(struct cfs_rq *cfs_rq) {}
---
> static inline void update_tg_load_avg(struct cfs_rq *cfs_rq, int force) {}
3687c3643
<  * @now: current time, as per cfs_rq_clock_pelt()
---
>  * @now: current time, as per cfs_rq_clock_task()
3704c3660
< 	unsigned long removed_load = 0, removed_util = 0, removed_runnable = 0;
---
> 	unsigned long removed_load = 0, removed_util = 0, removed_runnable_sum = 0;
3710c3666
< 		u32 divider = get_pelt_divider(&cfs_rq->avg);
---
> 		u32 divider = LOAD_AVG_MAX - 1024 + sa->period_contrib;
3715c3671
< 		swap(cfs_rq->removed.runnable_avg, removed_runnable);
---
> 		swap(cfs_rq->removed.runnable_sum, removed_runnable_sum);
3721c3677
< 		sa->load_sum = sa->load_avg * divider;
---
> 		sub_positive(&sa->load_sum, r * divider);
3725c3681
< 		sa->util_sum = sa->util_avg * divider;
---
> 		sub_positive(&sa->util_sum, r * divider);
3727,3736c3683
< 		r = removed_runnable;
< 		sub_positive(&sa->runnable_avg, r);
< 		sa->runnable_sum = sa->runnable_avg * divider;
< 
< 		/*
< 		 * removed_runnable is the unweighted version of removed_load so we
< 		 * can use it to estimate removed_load_sum.
< 		 */
< 		add_tg_cfs_propagate(cfs_rq,
< 			-(long)(removed_runnable * divider) >> SCHED_CAPACITY_SHIFT);
---
> 		add_tg_cfs_propagate(cfs_rq, -(long)removed_runnable_sum);
3741c3688
< 	decayed |= __update_load_avg_cfs_rq(now, cfs_rq);
---
> 	decayed |= __update_load_avg_cfs_rq(now, cpu_of(rq_of(cfs_rq)), cfs_rq);
3747a3695,3697
> 	if (decayed)
> 		cfs_rq_util_change(cfs_rq);
> 
3761,3765c3711
< 	/*
< 	 * cfs_rq->avg.period_contrib can be used for both cfs_rq and se.
< 	 * See ___update_load_avg() for details.
< 	 */
< 	u32 divider = get_pelt_divider(&cfs_rq->avg);
---
> 	u32 divider = LOAD_AVG_MAX - 1024 + cfs_rq->avg.period_contrib;
3785,3786d3730
< 	se->avg.runnable_sum = se->avg.runnable_avg * divider;
< 
3792a3737,3738
> 	se->avg.runnable_load_sum = se->avg.load_sum;
> 
3796,3797d3741
< 	cfs_rq->avg.runnable_avg += se->avg.runnable_avg;
< 	cfs_rq->avg.runnable_sum += se->avg.runnable_sum;
3801,3803c3745
< 	cfs_rq_util_change(cfs_rq, 0);
< 
< 	trace_pelt_cfs_tp(cfs_rq);
---
> 	cfs_rq_util_change(cfs_rq);
3816,3821d3757
< 	/*
< 	 * cfs_rq->avg.period_contrib can be used for both cfs_rq and se.
< 	 * See ___update_load_avg() for details.
< 	 */
< 	u32 divider = get_pelt_divider(&cfs_rq->avg);
< 
3824,3826c3760
< 	cfs_rq->avg.util_sum = cfs_rq->avg.util_avg * divider;
< 	sub_positive(&cfs_rq->avg.runnable_avg, se->avg.runnable_avg);
< 	cfs_rq->avg.runnable_sum = cfs_rq->avg.runnable_avg * divider;
---
> 	sub_positive(&cfs_rq->avg.util_sum, se->avg.util_sum);
3830,3832c3764
< 	cfs_rq_util_change(cfs_rq, 0);
< 
< 	trace_pelt_cfs_tp(cfs_rq);
---
> 	cfs_rq_util_change(cfs_rq);
3845c3777,3779
< 	u64 now = cfs_rq_clock_pelt(cfs_rq);
---
> 	u64 now = cfs_rq_clock_task(cfs_rq);
> 	struct rq *rq = rq_of(cfs_rq);
> 	int cpu = cpu_of(rq);
3853c3787
< 		__update_load_avg_se(now, cfs_rq, se);
---
> 		__update_load_avg_se(now, cpu, cfs_rq, se);
3860,3866d3793
< 		/*
< 		 * DO_ATTACH means we're here from enqueue_entity().
< 		 * !last_update_time means we've passed through
< 		 * migrate_task_rq_fair() indicating we migrated.
< 		 *
< 		 * IOW we're enqueueing a task on a new CPU.
< 		 */
3868,3871c3795
< 		update_tg_load_avg(cfs_rq);
< 
< 	} else if (decayed) {
< 		cfs_rq_util_change(cfs_rq, 0);
---
> 		update_tg_load_avg(cfs_rq, 0);
3873,3875c3797,3798
< 		if (flags & UPDATE_TG)
< 			update_tg_load_avg(cfs_rq);
< 	}
---
> 	} else if (decayed && (flags & UPDATE_TG))
> 		update_tg_load_avg(cfs_rq, 0);
3903c3826
< static void sync_entity_load_avg(struct sched_entity *se)
---
> void sync_entity_load_avg(struct sched_entity *se)
3909c3832
< 	__update_load_avg_blocked_se(last_update_time, se);
---
> 	__update_load_avg_blocked_se(last_update_time, cpu_of(rq_of(cfs_rq)), se);
3916c3839
< static void remove_entity_load_avg(struct sched_entity *se)
---
> void remove_entity_load_avg(struct sched_entity *se)
3924a3848,3851
> 	 *
> 	 * Similarly for groups, they will have passed through
> 	 * post_init_entity_util_avg() before unregister_sched_fair_group()
> 	 * calls this.
3933c3860
< 	cfs_rq->removed.runnable_avg	+= se->avg.runnable_avg;
---
> 	cfs_rq->removed.runnable_sum	+= se->avg.load_sum; /* == runnable_sum */
3937c3864
< static inline unsigned long cfs_rq_runnable_avg(struct cfs_rq *cfs_rq)
---
> static inline unsigned long cfs_rq_runnable_load_avg(struct cfs_rq *cfs_rq)
3939c3866
< 	return cfs_rq->avg.runnable_avg;
---
> 	return cfs_rq->avg.runnable_load_avg;
3947,4138c3874
< static int newidle_balance(struct rq *this_rq, struct rq_flags *rf);
< 
< static inline unsigned long task_util(struct task_struct *p)
< {
< 	return READ_ONCE(p->se.avg.util_avg);
< }
< 
< static inline unsigned long _task_util_est(struct task_struct *p)
< {
< 	struct util_est ue = READ_ONCE(p->se.avg.util_est);
< 
< 	return max(ue.ewma, (ue.enqueued & ~UTIL_AVG_UNCHANGED));
< }
< 
< static inline unsigned long task_util_est(struct task_struct *p)
< {
< 	return max(task_util(p), _task_util_est(p));
< }
< 
< #ifdef CONFIG_UCLAMP_TASK
< static inline unsigned long uclamp_task_util(struct task_struct *p)
< {
< 	return clamp(task_util_est(p),
< 		     uclamp_eff_value(p, UCLAMP_MIN),
< 		     uclamp_eff_value(p, UCLAMP_MAX));
< }
< #else
< static inline unsigned long uclamp_task_util(struct task_struct *p)
< {
< 	return task_util_est(p);
< }
< #endif
< 
< static inline void util_est_enqueue(struct cfs_rq *cfs_rq,
< 				    struct task_struct *p)
< {
< 	unsigned int enqueued;
< 
< 	if (!sched_feat(UTIL_EST))
< 		return;
< 
< 	/* Update root cfs_rq's estimated utilization */
< 	enqueued  = cfs_rq->avg.util_est.enqueued;
< 	enqueued += _task_util_est(p);
< 	WRITE_ONCE(cfs_rq->avg.util_est.enqueued, enqueued);
< 
< 	trace_sched_util_est_cfs_tp(cfs_rq);
< }
< 
< static inline void util_est_dequeue(struct cfs_rq *cfs_rq,
< 				    struct task_struct *p)
< {
< 	unsigned int enqueued;
< 
< 	if (!sched_feat(UTIL_EST))
< 		return;
< 
< 	/* Update root cfs_rq's estimated utilization */
< 	enqueued  = cfs_rq->avg.util_est.enqueued;
< 	enqueued -= min_t(unsigned int, enqueued, _task_util_est(p));
< 	WRITE_ONCE(cfs_rq->avg.util_est.enqueued, enqueued);
< 
< 	trace_sched_util_est_cfs_tp(cfs_rq);
< }
< 
< #define UTIL_EST_MARGIN (SCHED_CAPACITY_SCALE / 100)
< 
< /*
<  * Check if a (signed) value is within a specified (unsigned) margin,
<  * based on the observation that:
<  *
<  *     abs(x) < y := (unsigned)(x + y - 1) < (2 * y - 1)
<  *
<  * NOTE: this only works when value + margin < INT_MAX.
<  */
< static inline bool within_margin(int value, int margin)
< {
< 	return ((unsigned int)(value + margin - 1) < (2 * margin - 1));
< }
< 
< static inline void util_est_update(struct cfs_rq *cfs_rq,
< 				   struct task_struct *p,
< 				   bool task_sleep)
< {
< 	long last_ewma_diff, last_enqueued_diff;
< 	struct util_est ue;
< 
< 	if (!sched_feat(UTIL_EST))
< 		return;
< 
< 	/*
< 	 * Skip update of task's estimated utilization when the task has not
< 	 * yet completed an activation, e.g. being migrated.
< 	 */
< 	if (!task_sleep)
< 		return;
< 
< 	/*
< 	 * If the PELT values haven't changed since enqueue time,
< 	 * skip the util_est update.
< 	 */
< 	ue = p->se.avg.util_est;
< 	if (ue.enqueued & UTIL_AVG_UNCHANGED)
< 		return;
< 
< 	last_enqueued_diff = ue.enqueued;
< 
< 	/*
< 	 * Reset EWMA on utilization increases, the moving average is used only
< 	 * to smooth utilization decreases.
< 	 */
< 	ue.enqueued = task_util(p);
< 	if (sched_feat(UTIL_EST_FASTUP)) {
< 		if (ue.ewma < ue.enqueued) {
< 			ue.ewma = ue.enqueued;
< 			goto done;
< 		}
< 	}
< 
< 	/*
< 	 * Skip update of task's estimated utilization when its members are
< 	 * already ~1% close to its last activation value.
< 	 */
< 	last_ewma_diff = ue.enqueued - ue.ewma;
< 	last_enqueued_diff -= ue.enqueued;
< 	if (within_margin(last_ewma_diff, UTIL_EST_MARGIN)) {
< 		if (!within_margin(last_enqueued_diff, UTIL_EST_MARGIN))
< 			goto done;
< 
< 		return;
< 	}
< 
< 	/*
< 	 * To avoid overestimation of actual task utilization, skip updates if
< 	 * we cannot grant there is idle time in this CPU.
< 	 */
< 	if (task_util(p) > capacity_orig_of(cpu_of(rq_of(cfs_rq))))
< 		return;
< 
< 	/*
< 	 * Update Task's estimated utilization
< 	 *
< 	 * When *p completes an activation we can consolidate another sample
< 	 * of the task size. This is done by storing the current PELT value
< 	 * as ue.enqueued and by using this value to update the Exponential
< 	 * Weighted Moving Average (EWMA):
< 	 *
< 	 *  ewma(t) = w *  task_util(p) + (1-w) * ewma(t-1)
< 	 *          = w *  task_util(p) +         ewma(t-1)  - w * ewma(t-1)
< 	 *          = w * (task_util(p) -         ewma(t-1)) +     ewma(t-1)
< 	 *          = w * (      last_ewma_diff            ) +     ewma(t-1)
< 	 *          = w * (last_ewma_diff  +  ewma(t-1) / w)
< 	 *
< 	 * Where 'w' is the weight of new samples, which is configured to be
< 	 * 0.25, thus making w=1/4 ( >>= UTIL_EST_WEIGHT_SHIFT)
< 	 */
< 	ue.ewma <<= UTIL_EST_WEIGHT_SHIFT;
< 	ue.ewma  += last_ewma_diff;
< 	ue.ewma >>= UTIL_EST_WEIGHT_SHIFT;
< done:
< 	ue.enqueued |= UTIL_AVG_UNCHANGED;
< 	WRITE_ONCE(p->se.avg.util_est, ue);
< 
< 	trace_sched_util_est_se_tp(&p->se);
< }
< 
< static inline int task_fits_capacity(struct task_struct *p, long capacity)
< {
< 	return fits_capacity(uclamp_task_util(p), capacity);
< }
< 
< static inline void update_misfit_status(struct task_struct *p, struct rq *rq)
< {
< 	if (!static_branch_unlikely(&sched_asym_cpucapacity))
< 		return;
< 
< 	if (!p || p->nr_cpus_allowed == 1) {
< 		rq->misfit_task_load = 0;
< 		return;
< 	}
< 
< 	if (task_fits_capacity(p, capacity_of(cpu_of(rq)))) {
< 		rq->misfit_task_load = 0;
< 		return;
< 	}
< 
< 	/*
< 	 * Make sure that misfit_task_load will not be null even if
< 	 * task_h_load() returns 0.
< 	 */
< 	rq->misfit_task_load = max_t(unsigned long, task_h_load(p), 1);
< }
---
> static int idle_balance(struct rq *this_rq, struct rq_flags *rf);
4142c3878,3879
< static inline bool cfs_rq_is_decayed(struct cfs_rq *cfs_rq)
---
> static inline int
> update_cfs_rq_load_avg(u64 now, struct cfs_rq *cfs_rq)
4144c3881
< 	return true;
---
> 	return 0;
4153c3890
< 	cfs_rq_util_change(cfs_rq, 0);
---
> 	cfs_rq_util_change(cfs_rq);
4163c3900
< static inline int newidle_balance(struct rq *rq, struct rq_flags *rf)
---
> static inline int idle_balance(struct rq *rq, struct rq_flags *rf)
4168,4178d3904
< static inline void
< util_est_enqueue(struct cfs_rq *cfs_rq, struct task_struct *p) {}
< 
< static inline void
< util_est_dequeue(struct cfs_rq *cfs_rq, struct task_struct *p) {}
< 
< static inline void
< util_est_update(struct cfs_rq *cfs_rq, struct task_struct *p,
< 		bool task_sleep) {}
< static inline void update_misfit_status(struct task_struct *p, struct rq *rq) {}
< 
4248d3973
< static inline bool cfs_bandwidth_used(void);
4313d4037
< 	se_update_runnable(se);
4314a4039
> 	enqueue_runnable_load_avg(cfs_rq, se);
4327,4332c4052
< 	/*
< 	 * When bandwidth control is enabled, cfs might have been removed
< 	 * because of a parent been throttled but cfs->nr_running > 1. Try to
< 	 * add it unconditionally.
< 	 */
< 	if (cfs_rq->nr_running == 1 || cfs_bandwidth_used())
---
> 	if (cfs_rq->nr_running == 1) {
4334,4335d4053
< 
< 	if (cfs_rq->nr_running == 1)
4336a4055
> 	}
4397,4398c4116,4117
< 	 *   - Subtract its load from the cfs_rq->runnable_avg.
< 	 *   - Subtract its previous weight from cfs_rq->load.weight.
---
> 	 *   - Substract its load from the cfs_rq->runnable_avg.
> 	 *   - Substract its previous weight from cfs_rq->load.weight.
4403c4122
< 	se_update_runnable(se);
---
> 	dequeue_runnable_load_avg(cfs_rq, se);
4434c4153
< 	if ((flags & (DEQUEUE_SAVE | DEQUEUE_MOVE)) != DEQUEUE_SAVE)
---
> 	if ((flags & (DEQUEUE_SAVE | DEQUEUE_MOVE)) == DEQUEUE_SAVE)
4481,4482d4199
< 	clear_buddies(cfs_rq, se);
< 
4503,4504c4220
< 	if (schedstat_enabled() &&
< 	    rq_of(cfs_rq)->cfs.load.weight >= 2*se->load.weight) {
---
> 	if (schedstat_enabled() && rq_of(cfs_rq)->load.weight >= 2*se->load.weight) {
4542c4258
< 	if (cfs_rq->skip && cfs_rq->skip == se) {
---
> 	if (cfs_rq->skip == se) {
4557,4565c4273,4276
< 	if (cfs_rq->next && wakeup_preempt_entity(cfs_rq->next, left) < 1) {
< 		/*
< 		 * Someone really wants this to run. If it's not unfair, run it.
< 		 */
< 		se = cfs_rq->next;
< 	} else if (cfs_rq->last && wakeup_preempt_entity(cfs_rq->last, left) < 1) {
< 		/*
< 		 * Prefer last buddy, try to return the CPU to a preempted task.
< 		 */
---
> 	/*
> 	 * Prefer last buddy, try to return the CPU to a preempted task.
> 	 */
> 	if (cfs_rq->last && wakeup_preempt_entity(cfs_rq->last, left) < 1)
4567c4278,4285
< 	}
---
> 
> 	/*
> 	 * Someone really wants this to run. If it's not unfair, run it.
> 	 */
> 	if (cfs_rq->next && wakeup_preempt_entity(cfs_rq->next, left) < 1)
> 		se = cfs_rq->next;
> 
> 	clear_buddies(cfs_rq, se);
4640c4358
< #ifdef CONFIG_JUMP_LABEL
---
> #ifdef HAVE_JUMP_LABEL
4657c4375
< #else /* CONFIG_JUMP_LABEL */
---
> #else /* HAVE_JUMP_LABEL */
4665c4383
< #endif /* CONFIG_JUMP_LABEL */
---
> #endif /* HAVE_JUMP_LABEL */
4682,4684c4400,4402
<  * Replenish runtime according to assigned quota. We use sched_clock_cpu
<  * directly instead of rq->clock to avoid adding additional synchronization
<  * around rq->lock.
---
>  * Replenish runtime according to assigned quota and update expiration time.
>  * We use sched_clock_cpu directly instead of rq->clock to avoid adding
>  * additional synchronization around rq->lock.
4690c4408,4410
< 	if (unlikely(cfs_b->quota == RUNTIME_INF))
---
> 	u64 now;
> 
> 	if (cfs_b->quota == RUNTIME_INF)
4693,4694c4413,4415
< 	cfs_b->runtime += cfs_b->quota;
< 	cfs_b->runtime = min(cfs_b->runtime, cfs_b->quota + cfs_b->burst);
---
> 	now = sched_clock_cpu(smp_processor_id());
> 	cfs_b->runtime = cfs_b->quota;
> 	cfs_b->runtime_expires = now + ktime_to_ns(cfs_b->period);
4702,4704c4423,4424
< /* returns 0 on failure to allocate runtime */
< static int __assign_cfs_rq_runtime(struct cfs_bandwidth *cfs_b,
< 				   struct cfs_rq *cfs_rq, u64 target_runtime)
---
> /* rq->task_clock normalized against any time this cfs_rq has spent throttled */
> static inline u64 cfs_rq_clock_task(struct cfs_rq *cfs_rq)
4706c4426,4427
< 	u64 min_amount, amount = 0;
---
> 	if (unlikely(cfs_rq->throttle_count))
> 		return cfs_rq->throttled_clock_task - cfs_rq->throttled_clock_task_time;
4708c4429,4437
< 	lockdep_assert_held(&cfs_b->lock);
---
> 	return rq_clock_task(rq_of(cfs_rq)) - cfs_rq->throttled_clock_task_time;
> }
> 
> /* returns 0 on failure to allocate runtime */
> static int assign_cfs_rq_runtime(struct cfs_rq *cfs_rq)
> {
> 	struct task_group *tg = cfs_rq->tg;
> 	struct cfs_bandwidth *cfs_b = tg_cfs_bandwidth(tg);
> 	u64 amount = 0, min_amount, expires;
4711c4440
< 	min_amount = target_runtime - cfs_rq->runtime_remaining;
---
> 	min_amount = sched_cfs_bandwidth_slice() - cfs_rq->runtime_remaining;
4712a4442
> 	raw_spin_lock(&cfs_b->lock);
4723a4454,4455
> 	expires = cfs_b->runtime_expires;
> 	raw_spin_unlock(&cfs_b->lock);
4725a4458,4464
> 	/*
> 	 * we may have advanced our local expiration to account for allowed
> 	 * spread between our sched_clock and the one on which runtime was
> 	 * issued.
> 	 */
> 	if ((s64)(expires - cfs_rq->runtime_expires) > 0)
> 		cfs_rq->runtime_expires = expires;
4730,4731c4469,4473
< /* returns 0 on failure to allocate runtime */
< static int assign_cfs_rq_runtime(struct cfs_rq *cfs_rq)
---
> /*
>  * Note: This depends on the synchronization provided by sched_clock and the
>  * fact that rq->clock snapshots this value.
>  */
> static void expire_cfs_rq_runtime(struct cfs_rq *cfs_rq)
4734d4475
< 	int ret;
4736,4738c4477,4479
< 	raw_spin_lock(&cfs_b->lock);
< 	ret = __assign_cfs_rq_runtime(cfs_b, cfs_rq, sched_cfs_bandwidth_slice());
< 	raw_spin_unlock(&cfs_b->lock);
---
> 	/* if the deadline is ahead of our clock, nothing to do */
> 	if (likely((s64)(rq_clock(rq_of(cfs_rq)) - cfs_rq->runtime_expires) < 0))
> 		return;
4740c4481,4501
< 	return ret;
---
> 	if (cfs_rq->runtime_remaining < 0)
> 		return;
> 
> 	/*
> 	 * If the local deadline has passed we have to consider the
> 	 * possibility that our sched_clock is 'fast' and the global deadline
> 	 * has not truly expired.
> 	 *
> 	 * Fortunately we can check determine whether this the case by checking
> 	 * whether the global deadline has advanced. It is valid to compare
> 	 * cfs_b->runtime_expires without any locks since we only care about
> 	 * exact equality, so a partial write will still work.
> 	 */
> 
> 	if (cfs_rq->runtime_expires != cfs_b->runtime_expires) {
> 		/* extend local deadline, drift is bounded above by 2 ticks */
> 		cfs_rq->runtime_expires += TICK_NSEC;
> 	} else {
> 		/* global deadline is ahead, expiration has passed */
> 		cfs_rq->runtime_remaining = 0;
> 	}
4746a4508
> 	expire_cfs_rq_runtime(cfs_rq);
4751,4752d4512
< 	if (cfs_rq->throttled)
< 		return;
4797a4558
> /* updated child weight may affect parent so we have to do this bottom up */
4804a4566
> 		/* adjust cfs_rq_clock_task() */
4807,4810d4568
< 
< 		/* Add cfs_rq with load or one or more already running entities to the list */
< 		if (!cfs_rq_is_decayed(cfs_rq) || cfs_rq->nr_running)
< 			list_add_leaf_cfs_rq(cfs_rq);
4822c4580
< 	if (!cfs_rq->throttle_count) {
---
> 	if (!cfs_rq->throttle_count)
4824,4825d4581
< 		list_del_leaf_cfs_rq(cfs_rq);
< 	}
4831c4587
< static bool throttle_cfs_rq(struct cfs_rq *cfs_rq)
---
> static void throttle_cfs_rq(struct cfs_rq *cfs_rq)
4836,4857c4592,4593
< 	long task_delta, idle_task_delta, dequeue = 1;
< 
< 	raw_spin_lock(&cfs_b->lock);
< 	/* This will start the period timer if necessary */
< 	if (__assign_cfs_rq_runtime(cfs_b, cfs_rq, 1)) {
< 		/*
< 		 * We have raced with bandwidth becoming available, and if we
< 		 * actually throttled the timer might not unthrottle us for an
< 		 * entire period. We additionally needed to make sure that any
< 		 * subsequent check_cfs_rq_runtime calls agree not to throttle
< 		 * us, as we may commit to do cfs put_prev+pick_next, so we ask
< 		 * for 1ns of runtime rather than just check cfs_b.
< 		 */
< 		dequeue = 0;
< 	} else {
< 		list_add_tail_rcu(&cfs_rq->throttled_list,
< 				  &cfs_b->throttled_cfs_rq);
< 	}
< 	raw_spin_unlock(&cfs_b->lock);
< 
< 	if (!dequeue)
< 		return false;  /* Throttle no longer required. */
---
> 	long task_delta, dequeue = 1;
> 	bool empty;
4867d4602
< 	idle_task_delta = cfs_rq->idle_h_nr_running;
4872,4877c4607
< 			goto done;
< 
< 		dequeue_entity(qcfs_rq, se, DEQUEUE_SLEEP);
< 
< 		if (cfs_rq_is_idle(group_cfs_rq(se)))
< 			idle_task_delta = cfs_rq->h_nr_running;
---
> 			break;
4878a4609,4610
> 		if (dequeue)
> 			dequeue_entity(qcfs_rq, se, DEQUEUE_SLEEP);
4880d4611
< 		qcfs_rq->idle_h_nr_running -= idle_task_delta;
4882,4886c4613,4614
< 		if (qcfs_rq->load.weight) {
< 			/* Avoid re-evaluating load for this entity: */
< 			se = parent_entity(se);
< 			break;
< 		}
---
> 		if (qcfs_rq->load.weight)
> 			dequeue = 0;
4889,4899c4617,4618
< 	for_each_sched_entity(se) {
< 		struct cfs_rq *qcfs_rq = cfs_rq_of(se);
< 		/* throttled entity or throttle-on-deactivate */
< 		if (!se->on_rq)
< 			goto done;
< 
< 		update_load_avg(qcfs_rq, se, 0);
< 		se_update_runnable(se);
< 
< 		if (cfs_rq_is_idle(group_cfs_rq(se)))
< 			idle_task_delta = cfs_rq->h_nr_running;
---
> 	if (!se)
> 		sub_nr_running(rq, task_delta);
4901,4903c4620,4623
< 		qcfs_rq->h_nr_running -= task_delta;
< 		qcfs_rq->idle_h_nr_running -= idle_task_delta;
< 	}
---
> 	cfs_rq->throttled = 1;
> 	cfs_rq->throttled_clock = rq_clock(rq);
> 	raw_spin_lock(&cfs_b->lock);
> 	empty = list_empty(&cfs_b->throttled_cfs_rq);
4905,4906c4625,4629
< 	/* At this point se is NULL and we are at root level*/
< 	sub_nr_running(rq, task_delta);
---
> 	/*
> 	 * Add to the _head_ of the list, so that an already-started
> 	 * distribute_cfs_runtime will not see us
> 	 */
> 	list_add_rcu(&cfs_rq->throttled_list, &cfs_b->throttled_cfs_rq);
4908d4630
< done:
4910,4911c4632,4633
< 	 * Note: distribution will already see us throttled via the
< 	 * throttled-list.  rq->lock protects completion.
---
> 	 * If we're the first throttled task, make sure the bandwidth
> 	 * timer is running.
4913,4915c4635,4638
< 	cfs_rq->throttled = 1;
< 	cfs_rq->throttled_clock = rq_clock(rq);
< 	return true;
---
> 	if (empty)
> 		start_cfs_bandwidth(cfs_b);
> 
> 	raw_spin_unlock(&cfs_b->lock);
4923c4646,4647
< 	long task_delta, idle_task_delta;
---
> 	int enqueue = 1;
> 	long task_delta;
4939,4942c4663
< 	/* Nothing to run but something to decay (on_list)? Complete the branch */
< 	if (!cfs_rq->load.weight) {
< 		if (cfs_rq->on_list)
< 			goto unthrottle_throttle;
---
> 	if (!cfs_rq->load.weight)
4944d4664
< 	}
4947d4666
< 	idle_task_delta = cfs_rq->idle_h_nr_running;
4949,4950d4667
< 		struct cfs_rq *qcfs_rq = cfs_rq_of(se);
< 
4952,4988c4669
< 			break;
< 		enqueue_entity(qcfs_rq, se, ENQUEUE_WAKEUP);
< 
< 		if (cfs_rq_is_idle(group_cfs_rq(se)))
< 			idle_task_delta = cfs_rq->h_nr_running;
< 
< 		qcfs_rq->h_nr_running += task_delta;
< 		qcfs_rq->idle_h_nr_running += idle_task_delta;
< 
< 		/* end evaluation on encountering a throttled cfs_rq */
< 		if (cfs_rq_throttled(qcfs_rq))
< 			goto unthrottle_throttle;
< 	}
< 
< 	for_each_sched_entity(se) {
< 		struct cfs_rq *qcfs_rq = cfs_rq_of(se);
< 
< 		update_load_avg(qcfs_rq, se, UPDATE_TG);
< 		se_update_runnable(se);
< 
< 		if (cfs_rq_is_idle(group_cfs_rq(se)))
< 			idle_task_delta = cfs_rq->h_nr_running;
< 
< 		qcfs_rq->h_nr_running += task_delta;
< 		qcfs_rq->idle_h_nr_running += idle_task_delta;
< 
< 		/* end evaluation on encountering a throttled cfs_rq */
< 		if (cfs_rq_throttled(qcfs_rq))
< 			goto unthrottle_throttle;
< 
< 		/*
< 		 * One parent has been throttled and cfs_rq removed from the
< 		 * list. Add it back to not break the leaf list.
< 		 */
< 		if (throttled_hierarchy(qcfs_rq))
< 			list_add_leaf_cfs_rq(qcfs_rq);
< 	}
---
> 			enqueue = 0;
4990,5000c4671,4674
< 	/* At this point se is NULL and we are at root level*/
< 	add_nr_running(rq, task_delta);
< 
< unthrottle_throttle:
< 	/*
< 	 * The cfs_rq_throttled() breaks in the above iteration can result in
< 	 * incomplete leaf list maintenance, resulting in triggering the
< 	 * assertion below.
< 	 */
< 	for_each_sched_entity(se) {
< 		struct cfs_rq *qcfs_rq = cfs_rq_of(se);
---
> 		cfs_rq = cfs_rq_of(se);
> 		if (enqueue)
> 			enqueue_entity(cfs_rq, se, ENQUEUE_WAKEUP);
> 		cfs_rq->h_nr_running += task_delta;
5002c4676
< 		if (list_add_leaf_cfs_rq(qcfs_rq))
---
> 		if (cfs_rq_throttled(cfs_rq))
5006c4680,4681
< 	assert_list_leaf_cfs_rq(rq);
---
> 	if (!se)
> 		add_nr_running(rq, task_delta);
5008c4683
< 	/* Determine whether we need to wake up potentially idle CPU: */
---
> 	/* determine whether we need to wake up potentially idle cpu */
5013c4688,4689
< static void distribute_cfs_runtime(struct cfs_bandwidth *cfs_b)
---
> static u64 distribute_cfs_runtime(struct cfs_bandwidth *cfs_b,
> 		u64 remaining, u64 expires)
5016c4692,4693
< 	u64 runtime, remaining = 1;
---
> 	u64 runtime;
> 	u64 starting_runtime = remaining;
5024c4701
< 		rq_lock_irqsave(rq, &rf);
---
> 		rq_lock(rq, &rf);
5028,5031d4704
< 		/* By the above check, this should never be true */
< 		SCHED_WARN_ON(cfs_rq->runtime_remaining > 0);
< 
< 		raw_spin_lock(&cfs_b->lock);
5033,5037c4706,4708
< 		if (runtime > cfs_b->runtime)
< 			runtime = cfs_b->runtime;
< 		cfs_b->runtime -= runtime;
< 		remaining = cfs_b->runtime;
< 		raw_spin_unlock(&cfs_b->lock);
---
> 		if (runtime > remaining)
> 			runtime = remaining;
> 		remaining -= runtime;
5039a4711
> 		cfs_rq->runtime_expires = expires;
5046c4718
< 		rq_unlock_irqrestore(rq, &rf);
---
> 		rq_unlock(rq, &rf);
5051a4724,4725
> 
> 	return starting_runtime - remaining;
5060c4734
< static int do_sched_cfs_period_timer(struct cfs_bandwidth *cfs_b, int overrun, unsigned long flags)
---
> static int do_sched_cfs_period_timer(struct cfs_bandwidth *cfs_b, int overrun)
5061a4736
> 	u64 runtime, runtime_expires;
5071,5073d4745
< 	/* Refill extra burst quota even if cfs_b->idle */
< 	__refill_cfs_bandwidth_runtime(cfs_b);
< 
5080a4753,4754
> 	__refill_cfs_bandwidth_runtime(cfs_b);
> 
5089a4764,4765
> 	runtime_expires = cfs_b->runtime_expires;
> 
5091c4767,4771
< 	 * This check is repeated as we release cfs_b->lock while we unthrottle.
---
> 	 * This check is repeated as we are holding onto the new bandwidth while
> 	 * we unthrottle. This can potentially race with an unthrottled group
> 	 * trying to acquire new bandwidth from the global pool. This can result
> 	 * in us over-using our runtime if it is all used during this loop, but
> 	 * only by limited amounts in that extreme case.
5094c4774,4775
< 		raw_spin_unlock_irqrestore(&cfs_b->lock, flags);
---
> 		runtime = cfs_b->runtime;
> 		raw_spin_unlock(&cfs_b->lock);
5096,5097c4777,4779
< 		distribute_cfs_runtime(cfs_b);
< 		raw_spin_lock_irqsave(&cfs_b->lock, flags);
---
> 		runtime = distribute_cfs_runtime(cfs_b, runtime,
> 						 runtime_expires);
> 		raw_spin_lock(&cfs_b->lock);
5099a4782,4783
> 
> 		cfs_b->runtime -= min(runtime, cfs_b->runtime);
5133c4817
< 	s64 remaining;
---
> 	u64 remaining;
5141c4825
< 	if (remaining < (s64)min_expire)
---
> 	if (remaining < min_expire)
5155,5159d4838
< 	/* don't push forwards an existing deferred unthrottle */
< 	if (cfs_b->slack_started)
< 		return;
< 	cfs_b->slack_started = true;
< 
5175c4854,4855
< 	if (cfs_b->quota != RUNTIME_INF) {
---
> 	if (cfs_b->quota != RUNTIME_INF &&
> 	    cfs_rq->runtime_expires == cfs_b->runtime_expires) {
5207c4887
< 	unsigned long flags;
---
> 	u64 expires;
5210,5212c4890
< 	raw_spin_lock_irqsave(&cfs_b->lock, flags);
< 	cfs_b->slack_started = false;
< 
---
> 	raw_spin_lock(&cfs_b->lock);
5214c4892
< 		raw_spin_unlock_irqrestore(&cfs_b->lock, flags);
---
> 		raw_spin_unlock(&cfs_b->lock);
5221c4899,4900
< 	raw_spin_unlock_irqrestore(&cfs_b->lock, flags);
---
> 	expires = cfs_b->runtime_expires;
> 	raw_spin_unlock(&cfs_b->lock);
5226c4905,4910
< 	distribute_cfs_runtime(cfs_b);
---
> 	runtime = distribute_cfs_runtime(cfs_b, runtime, expires);
> 
> 	raw_spin_lock(&cfs_b->lock);
> 	if (expires == cfs_b->runtime_expires)
> 		cfs_b->runtime -= min(runtime, cfs_b->runtime);
> 	raw_spin_unlock(&cfs_b->lock);
5232c4916
<  * runtime as update_curr() throttling can not trigger until it's on-rq.
---
>  * runtime as update_curr() throttling can not not trigger until it's on-rq.
5286c4970,4971
< 	return throttle_cfs_rq(cfs_rq);
---
> 	throttle_cfs_rq(cfs_rq);
> 	return true;
5299,5300d4983
< extern const u64 max_cfs_quota_period;
< 
5305d4987
< 	unsigned long flags;
5308d4989
< 	int count = 0;
5310c4991
< 	raw_spin_lock_irqsave(&cfs_b->lock, flags);
---
> 	raw_spin_lock(&cfs_b->lock);
5316,5347c4997
< 		idle = do_sched_cfs_period_timer(cfs_b, overrun, flags);
< 
< 		if (++count > 3) {
< 			u64 new, old = ktime_to_ns(cfs_b->period);
< 
< 			/*
< 			 * Grow period by a factor of 2 to avoid losing precision.
< 			 * Precision loss in the quota/period ratio can cause __cfs_schedulable
< 			 * to fail.
< 			 */
< 			new = old * 2;
< 			if (new < max_cfs_quota_period) {
< 				cfs_b->period = ns_to_ktime(new);
< 				cfs_b->quota *= 2;
< 				cfs_b->burst *= 2;
< 
< 				pr_warn_ratelimited(
< 	"cfs_period_timer[cpu%d]: period too short, scaling up (new cfs_period_us = %lld, cfs_quota_us = %lld)\n",
< 					smp_processor_id(),
< 					div_u64(new, NSEC_PER_USEC),
< 					div_u64(cfs_b->quota, NSEC_PER_USEC));
< 			} else {
< 				pr_warn_ratelimited(
< 	"cfs_period_timer[cpu%d]: period too short, but cannot scale up without losing precision (cfs_period_us = %lld, cfs_quota_us = %lld)\n",
< 					smp_processor_id(),
< 					div_u64(old, NSEC_PER_USEC),
< 					div_u64(cfs_b->quota, NSEC_PER_USEC));
< 			}
< 
< 			/* reset count so we don't come right back in here */
< 			count = 0;
< 		}
---
> 		idle = do_sched_cfs_period_timer(cfs_b, overrun);
5351c5001
< 	raw_spin_unlock_irqrestore(&cfs_b->lock, flags);
---
> 	raw_spin_unlock(&cfs_b->lock);
5362d5011
< 	cfs_b->burst = 0;
5369d5017
< 	cfs_b->slack_started = false;
5382,5387c5030,5034
< 	if (cfs_b->period_active)
< 		return;
< 
< 	cfs_b->period_active = 1;
< 	hrtimer_forward_now(&cfs_b->period_timer, cfs_b->period);
< 	hrtimer_start_expires(&cfs_b->period_timer, HRTIMER_MODE_ABS_PINNED);
---
> 	if (!cfs_b->period_active) {
> 		cfs_b->period_active = 1;
> 		hrtimer_forward_now(&cfs_b->period_timer, cfs_b->period);
> 		hrtimer_start_expires(&cfs_b->period_timer, HRTIMER_MODE_ABS_PINNED);
> 	}
5401c5048
<  * Both these CPU hotplug callbacks race against unregister_fair_sched_group()
---
>  * Both these cpu hotplug callbacks race against unregister_fair_sched_group()
5407c5054
< /* cpu online callback */
---
> /* cpu online calback */
5412c5059
< 	lockdep_assert_rq_held(rq);
---
> 	lockdep_assert_held(&rq->lock);
5431c5078
< 	lockdep_assert_rq_held(rq);
---
> 	lockdep_assert_held(&rq->lock);
5446c5093
< 		 * Offline rq is schedulable till CPU is completely disabled
---
> 		 * Offline rq is schedulable till cpu is completely disabled
5458,5459c5105
< 
< static inline bool cfs_bandwidth_used(void)
---
> static inline u64 cfs_rq_clock_task(struct cfs_rq *cfs_rq)
5461c5107
< 	return false;
---
> 	return rq_clock_task(rq_of(cfs_rq));
5520c5166
< 			if (task_current(rq, p))
---
> 			if (rq->curr == p)
5537c5183
< 	if (!hrtick_enabled_fair(rq) || curr->sched_class != &fair_sched_class)
---
> 	if (!hrtick_enabled(rq) || curr->sched_class != &fair_sched_class)
5554,5586d5199
< #ifdef CONFIG_SMP
< static inline unsigned long cpu_util(int cpu);
< 
< static inline bool cpu_overutilized(int cpu)
< {
< 	return !fits_capacity(cpu_util(cpu), capacity_of(cpu));
< }
< 
< static inline void update_overutilized_status(struct rq *rq)
< {
< 	if (!READ_ONCE(rq->rd->overutilized) && cpu_overutilized(rq->cpu)) {
< 		WRITE_ONCE(rq->rd->overutilized, SG_OVERUTILIZED);
< 		trace_sched_overutilized_tp(rq->rd, SG_OVERUTILIZED);
< 	}
< }
< #else
< static inline void update_overutilized_status(struct rq *rq) { }
< #endif
< 
< /* Runqueue only has SCHED_IDLE tasks enqueued */
< static int sched_idle_rq(struct rq *rq)
< {
< 	return unlikely(rq->nr_running == rq->cfs.idle_h_nr_running &&
< 			rq->nr_running);
< }
< 
< #ifdef CONFIG_SMP
< static int sched_idle_cpu(int cpu)
< {
< 	return sched_idle_rq(cpu_rq(cpu));
< }
< #endif
< 
5597,5606d5209
< 	int idle_h_nr_running = task_has_idle_policy(p);
< 	int task_new = !(flags & ENQUEUE_WAKEUP);
< 
< 	/*
< 	 * The code below (indirectly) updates schedutil which looks at
< 	 * the cfs_rq utilization to select a frequency.
< 	 * Let's add the task's estimated utilization to the cfs_rq's
< 	 * estimated utilization, before we update schedutil.
< 	 */
< 	util_est_enqueue(&rq->cfs, p);
5622,5628c5225,5230
< 		cfs_rq->h_nr_running++;
< 		cfs_rq->idle_h_nr_running += idle_h_nr_running;
< 
< 		if (cfs_rq_is_idle(cfs_rq))
< 			idle_h_nr_running = 1;
< 
< 		/* end evaluation on encountering a throttled cfs_rq */
---
> 		/*
> 		 * end evaluation on encountering a throttled cfs_rq
> 		 *
> 		 * note: in the case of encountering a throttled cfs_rq we will
> 		 * post the final h_nr_running increment below.
> 		 */
5630c5232,5233
< 			goto enqueue_throttle;
---
> 			break;
> 		cfs_rq->h_nr_running++;
5637,5641d5239
< 
< 		update_load_avg(cfs_rq, se, UPDATE_TG);
< 		se_update_runnable(se);
< 		update_cfs_group(se);
< 
5643d5240
< 		cfs_rq->idle_h_nr_running += idle_h_nr_running;
5645,5648d5241
< 		if (cfs_rq_is_idle(cfs_rq))
< 			idle_h_nr_running = 1;
< 
< 		/* end evaluation on encountering a throttled cfs_rq */
5650,5689c5243
< 			goto enqueue_throttle;
< 
<                /*
<                 * One parent has been throttled and cfs_rq removed from the
<                 * list. Add it back to not break the leaf list.
<                 */
<                if (throttled_hierarchy(cfs_rq))
<                        list_add_leaf_cfs_rq(cfs_rq);
< 	}
< 
< 	/* At this point se is NULL and we are at root level*/
< 	add_nr_running(rq, 1);
< 
< 	/*
< 	 * Since new tasks are assigned an initial util_avg equal to
< 	 * half of the spare capacity of their CPU, tiny tasks have the
< 	 * ability to cross the overutilized threshold, which will
< 	 * result in the load balancer ruining all the task placement
< 	 * done by EAS. As a way to mitigate that effect, do not account
< 	 * for the first enqueue operation of new tasks during the
< 	 * overutilized flag detection.
< 	 *
< 	 * A better way of solving this problem would be to wait for
< 	 * the PELT signals of tasks to converge before taking them
< 	 * into account, but that is not straightforward to implement,
< 	 * and the following generally works well enough in practice.
< 	 */
< 	if (!task_new)
< 		update_overutilized_status(rq);
< 
< enqueue_throttle:
< 	if (cfs_bandwidth_used()) {
< 		/*
< 		 * When bandwidth control is enabled; the cfs_rq_throttled()
< 		 * breaks in the above iteration can result in incomplete
< 		 * leaf list maintenance, resulting in triggering the assertion
< 		 * below.
< 		 */
< 		for_each_sched_entity(se) {
< 			cfs_rq = cfs_rq_of(se);
---
> 			break;
5691,5693c5245,5246
< 			if (list_add_leaf_cfs_rq(cfs_rq))
< 				break;
< 		}
---
> 		update_load_avg(cfs_rq, se, UPDATE_TG);
> 		update_cfs_group(se);
5696c5249,5250
< 	assert_list_leaf_cfs_rq(rq);
---
> 	if (!se)
> 		add_nr_running(rq, 1);
5713,5716d5266
< 	int idle_h_nr_running = task_has_idle_policy(p);
< 	bool was_sched_idle = sched_idle_rq(rq);
< 
< 	util_est_dequeue(&rq->cfs, p);
5722,5728c5272,5277
< 		cfs_rq->h_nr_running--;
< 		cfs_rq->idle_h_nr_running -= idle_h_nr_running;
< 
< 		if (cfs_rq_is_idle(cfs_rq))
< 			idle_h_nr_running = 1;
< 
< 		/* end evaluation on encountering a throttled cfs_rq */
---
> 		/*
> 		 * end evaluation on encountering a throttled cfs_rq
> 		 *
> 		 * note: in the case of encountering a throttled cfs_rq we will
> 		 * post the final h_nr_running decrement below.
> 		*/
5730c5279,5280
< 			goto dequeue_throttle;
---
> 			break;
> 		cfs_rq->h_nr_running--;
5749,5753d5298
< 
< 		update_load_avg(cfs_rq, se, UPDATE_TG);
< 		se_update_runnable(se);
< 		update_cfs_group(se);
< 
5755d5299
< 		cfs_rq->idle_h_nr_running -= idle_h_nr_running;
5757,5760d5300
< 		if (cfs_rq_is_idle(cfs_rq))
< 			idle_h_nr_running = 1;
< 
< 		/* end evaluation on encountering a throttled cfs_rq */
5762c5302
< 			goto dequeue_throttle;
---
> 			break;
5763a5304,5305
> 		update_load_avg(cfs_rq, se, UPDATE_TG);
> 		update_cfs_group(se);
5766,5771c5308,5309
< 	/* At this point se is NULL and we are at root level*/
< 	sub_nr_running(rq, 1);
< 
< 	/* balance early to pull high priority tasks */
< 	if (unlikely(!was_sched_idle && sched_idle_rq(rq)))
< 		rq->next_balance = jiffies;
---
> 	if (!se)
> 		sub_nr_running(rq, 1);
5773,5774d5310
< dequeue_throttle:
< 	util_est_update(&rq->cfs, p, task_sleep);
5784a5321,5323
> /*
>  * per rq 'load' arrray crap; XXX kill this.
>  */
5786,5792c5325,5379
< static struct {
< 	cpumask_var_t idle_cpus_mask;
< 	atomic_t nr_cpus;
< 	int has_blocked;		/* Idle CPUS has blocked load */
< 	unsigned long next_balance;     /* in jiffy units */
< 	unsigned long next_blocked;	/* Next update of blocked load in jiffies */
< } nohz ____cacheline_aligned;
---
> /*
>  * The exact cpuload calculated at every tick would be:
>  *
>  *   load' = (1 - 1/2^i) * load + (1/2^i) * cur_load
>  *
>  * If a cpu misses updates for n ticks (as it was idle) and update gets
>  * called on the n+1-th tick when cpu may be busy, then we have:
>  *
>  *   load_n   = (1 - 1/2^i)^n * load_0
>  *   load_n+1 = (1 - 1/2^i)   * load_n + (1/2^i) * cur_load
>  *
>  * decay_load_missed() below does efficient calculation of
>  *
>  *   load' = (1 - 1/2^i)^n * load
>  *
>  * Because x^(n+m) := x^n * x^m we can decompose any x^n in power-of-2 factors.
>  * This allows us to precompute the above in said factors, thereby allowing the
>  * reduction of an arbitrary n in O(log_2 n) steps. (See also
>  * fixed_power_int())
>  *
>  * The calculation is approximated on a 128 point scale.
>  */
> #define DEGRADE_SHIFT		7
> 
> static const u8 degrade_zero_ticks[CPU_LOAD_IDX_MAX] = {0, 8, 32, 64, 128};
> static const u8 degrade_factor[CPU_LOAD_IDX_MAX][DEGRADE_SHIFT + 1] = {
> 	{   0,   0,  0,  0,  0,  0, 0, 0 },
> 	{  64,  32,  8,  0,  0,  0, 0, 0 },
> 	{  96,  72, 40, 12,  1,  0, 0, 0 },
> 	{ 112,  98, 75, 43, 15,  1, 0, 0 },
> 	{ 120, 112, 98, 76, 45, 16, 2, 0 }
> };
> 
> /*
>  * Update cpu_load for any missed ticks, due to tickless idle. The backlog
>  * would be when CPU is idle and so we just decay the old load without
>  * adding any new load.
>  */
> static unsigned long
> decay_load_missed(unsigned long load, unsigned long missed_updates, int idx)
> {
> 	int j = 0;
> 
> 	if (!missed_updates)
> 		return load;
> 
> 	if (missed_updates >= degrade_zero_ticks[idx])
> 		return 0;
> 
> 	if (idx == 1)
> 		return load >> missed_updates;
> 
> 	while (missed_updates) {
> 		if (missed_updates % 2)
> 			load = (load * degrade_factor[idx][j]) >> DEGRADE_SHIFT;
5793a5381,5385
> 		missed_updates >>= 1;
> 		j++;
> 	}
> 	return load;
> }
5796c5388,5467
< static unsigned long cpu_load(struct rq *rq)
---
> /**
>  * __cpu_load_update - update the rq->cpu_load[] statistics
>  * @this_rq: The rq to update statistics for
>  * @this_load: The current load
>  * @pending_updates: The number of missed updates
>  *
>  * Update rq->cpu_load[] statistics. This function is usually called every
>  * scheduler tick (TICK_NSEC).
>  *
>  * This function computes a decaying average:
>  *
>  *   load[i]' = (1 - 1/2^i) * load[i] + (1/2^i) * load
>  *
>  * Because of NOHZ it might not get called on every tick which gives need for
>  * the @pending_updates argument.
>  *
>  *   load[i]_n = (1 - 1/2^i) * load[i]_n-1 + (1/2^i) * load_n-1
>  *             = A * load[i]_n-1 + B ; A := (1 - 1/2^i), B := (1/2^i) * load
>  *             = A * (A * load[i]_n-2 + B) + B
>  *             = A * (A * (A * load[i]_n-3 + B) + B) + B
>  *             = A^3 * load[i]_n-3 + (A^2 + A + 1) * B
>  *             = A^n * load[i]_0 + (A^(n-1) + A^(n-2) + ... + 1) * B
>  *             = A^n * load[i]_0 + ((1 - A^n) / (1 - A)) * B
>  *             = (1 - 1/2^i)^n * (load[i]_0 - load) + load
>  *
>  * In the above we've assumed load_n := load, which is true for NOHZ_FULL as
>  * any change in load would have resulted in the tick being turned back on.
>  *
>  * For regular NOHZ, this reduces to:
>  *
>  *   load[i]_n = (1 - 1/2^i)^n * load[i]_0
>  *
>  * see decay_load_misses(). For NOHZ_FULL we get to subtract and add the extra
>  * term.
>  */
> static void cpu_load_update(struct rq *this_rq, unsigned long this_load,
> 			    unsigned long pending_updates)
> {
> 	unsigned long __maybe_unused tickless_load = this_rq->cpu_load[0];
> 	int i, scale;
> 
> 	this_rq->nr_load_updates++;
> 
> 	/* Update our load: */
> 	this_rq->cpu_load[0] = this_load; /* Fasttrack for idx 0 */
> 	for (i = 1, scale = 2; i < CPU_LOAD_IDX_MAX; i++, scale += scale) {
> 		unsigned long old_load, new_load;
> 
> 		/* scale is effectively 1 << i now, and >> i divides by scale */
> 
> 		old_load = this_rq->cpu_load[i];
> #ifdef CONFIG_NO_HZ_COMMON
> 		old_load = decay_load_missed(old_load, pending_updates - 1, i);
> 		if (tickless_load) {
> 			old_load -= decay_load_missed(tickless_load, pending_updates - 1, i);
> 			/*
> 			 * old_load can never be a negative value because a
> 			 * decayed tickless_load cannot be greater than the
> 			 * original tickless_load.
> 			 */
> 			old_load += tickless_load;
> 		}
> #endif
> 		new_load = this_load;
> 		/*
> 		 * Round up the averaging division if load is increasing. This
> 		 * prevents us from getting stuck on 9 if the load is 10, for
> 		 * example.
> 		 */
> 		if (new_load > old_load)
> 			new_load += scale - 1;
> 
> 		this_rq->cpu_load[i] = (old_load * (scale - 1) + new_load) >> i;
> 	}
> 
> 	sched_avg_update(this_rq);
> }
> 
> /* Used instead of source_load when we know the type == 0 */
> static unsigned long weighted_cpuload(struct rq *rq)
5798c5469
< 	return cfs_rq_load_avg(&rq->cfs);
---
> 	return cfs_rq_runnable_load_avg(&rq->cfs);
5800a5472
> #ifdef CONFIG_NO_HZ_COMMON
5802,5804c5474,5476
<  * cpu_load_without - compute CPU load without any contributions from *p
<  * @cpu: the CPU which load is requested
<  * @p: the task which load should be discounted
---
>  * There is no sane way to deal with nohz on smp when using jiffies because the
>  * cpu doing the jiffies update might drift wrt the cpu doing the jiffy reading
>  * causing off-by-one errors in observed deltas; {0,2} instead of {1,1}.
5806,5808c5478,5482
<  * The load of a CPU is defined by the load of tasks currently enqueued on that
<  * CPU as well as tasks which are currently sleeping after an execution on that
<  * CPU.
---
>  * Therefore we need to avoid the delta approach from the regular tick when
>  * possible since that would seriously skew the load calculation. This is why we
>  * use cpu_load_update_periodic() for CPUs out of nohz. However we'll rely on
>  * jiffies deltas for updates happening while in nohz mode (idle ticks, idle
>  * loop exit, nohz_idle_balance, nohz full exit...)
5810,5812c5484
<  * This method returns the load of the specified CPU by discounting the load of
<  * the specified task, whenever the task is currently contributing to the CPU
<  * load.
---
>  * This means we might still be one tick off for nohz periods.
5814c5486,5489
< static unsigned long cpu_load_without(struct rq *rq, struct task_struct *p)
---
> 
> static void cpu_load_update_nohz(struct rq *this_rq,
> 				 unsigned long curr_jiffies,
> 				 unsigned long load)
5816,5817c5491
< 	struct cfs_rq *cfs_rq;
< 	unsigned int load;
---
> 	unsigned long pending_updates;
5819,5821c5493,5503
< 	/* Task has no contribution or is new */
< 	if (cpu_of(rq) != task_cpu(p) || !READ_ONCE(p->se.avg.last_update_time))
< 		return cpu_load(rq);
---
> 	pending_updates = curr_jiffies - this_rq->last_load_update_tick;
> 	if (pending_updates) {
> 		this_rq->last_load_update_tick = curr_jiffies;
> 		/*
> 		 * In the regular NOHZ case, we were idle, this means load 0.
> 		 * In the NOHZ_FULL case, we were non-idle, we should consider
> 		 * its weighted load.
> 		 */
> 		cpu_load_update(this_rq, load, pending_updates);
> 	}
> }
5823,5824c5505,5515
< 	cfs_rq = &rq->cfs;
< 	load = READ_ONCE(cfs_rq->avg.load_avg);
---
> /*
>  * Called from nohz_idle_balance() to update the load ratings before doing the
>  * idle balance.
>  */
> static void cpu_load_update_idle(struct rq *this_rq)
> {
> 	/*
> 	 * bail if there's load or we're actually up-to-date.
> 	 */
> 	if (weighted_cpuload(this_rq))
> 		return;
5826,5827c5517,5518
< 	/* Discount task's util from CPU's util */
< 	lsub_positive(&load, task_h_load(p));
---
> 	cpu_load_update_nohz(this_rq, READ_ONCE(jiffies), 0);
> }
5829c5520,5535
< 	return load;
---
> /*
>  * Record CPU load on nohz entry so we know the tickless load to account
>  * on nohz exit. cpu_load[0] happens then to be updated more frequently
>  * than other cpu_load[idx] but it should be fine as cpu_load readers
>  * shouldn't rely into synchronized cpu_load[*] updates.
>  */
> void cpu_load_update_nohz_start(void)
> {
> 	struct rq *this_rq = this_rq();
> 
> 	/*
> 	 * This is all lockless but should be fine. If weighted_cpuload changes
> 	 * concurrently we'll exit nohz. And cpu_load write can race with
> 	 * cpu_load_update_idle() but both updater would be writing the same.
> 	 */
> 	this_rq->cpu_load[0] = weighted_cpuload(this_rq);
5832c5538,5541
< static unsigned long cpu_runnable(struct rq *rq)
---
> /*
>  * Account the tickless load in the end of a nohz frame.
>  */
> void cpu_load_update_nohz_stop(void)
5834c5543,5555
< 	return cfs_rq_runnable_avg(&rq->cfs);
---
> 	unsigned long curr_jiffies = READ_ONCE(jiffies);
> 	struct rq *this_rq = this_rq();
> 	unsigned long load;
> 	struct rq_flags rf;
> 
> 	if (curr_jiffies == this_rq->last_load_update_tick)
> 		return;
> 
> 	load = weighted_cpuload(this_rq);
> 	rq_lock(this_rq, &rf);
> 	update_rq_clock(this_rq);
> 	cpu_load_update_nohz(this_rq, curr_jiffies, load);
> 	rq_unlock(this_rq, &rf);
5835a5557,5561
> #else /* !CONFIG_NO_HZ_COMMON */
> static inline void cpu_load_update_nohz(struct rq *this_rq,
> 					unsigned long curr_jiffies,
> 					unsigned long load) { }
> #endif /* CONFIG_NO_HZ_COMMON */
5837c5563
< static unsigned long cpu_runnable_without(struct rq *rq, struct task_struct *p)
---
> static void cpu_load_update_periodic(struct rq *this_rq, unsigned long load)
5839,5840c5565,5570
< 	struct cfs_rq *cfs_rq;
< 	unsigned int runnable;
---
> #ifdef CONFIG_NO_HZ_COMMON
> 	/* See the mess around cpu_load_update_nohz(). */
> 	this_rq->last_load_update_tick = READ_ONCE(jiffies);
> #endif
> 	cpu_load_update(this_rq, load, 1);
> }
5842,5844c5572,5601
< 	/* Task has no contribution or is new */
< 	if (cpu_of(rq) != task_cpu(p) || !READ_ONCE(p->se.avg.last_update_time))
< 		return cpu_runnable(rq);
---
> /*
>  * Called from scheduler_tick()
>  */
> void cpu_load_update_active(struct rq *this_rq)
> {
> 	unsigned long load = weighted_cpuload(this_rq);
> 
> 	if (tick_nohz_tick_stopped())
> 		cpu_load_update_nohz(this_rq, READ_ONCE(jiffies), load);
> 	else
> 		cpu_load_update_periodic(this_rq, load);
> }
> 
> /*
>  * Return a low guess at the load of a migration-source cpu weighted
>  * according to the scheduling class and "nice" value.
>  *
>  * We want to under-estimate the load of migration sources, to
>  * balance conservatively.
>  */
> static unsigned long source_load(int cpu, int type)
> {
> 	struct rq *rq = cpu_rq(cpu);
> 	unsigned long total = weighted_cpuload(rq);
> 
> 	if (type == 0 || !sched_feat(LB_BIAS))
> 		return total;
> 
> 	return min(rq->cpu_load[type-1], total);
> }
5846,5847c5603,5610
< 	cfs_rq = &rq->cfs;
< 	runnable = READ_ONCE(cfs_rq->avg.runnable_avg);
---
> /*
>  * Return a high guess at the load of a migration-target cpu weighted
>  * according to the scheduling class and "nice" value.
>  */
> static unsigned long target_load(int cpu, int type)
> {
> 	struct rq *rq = cpu_rq(cpu);
> 	unsigned long total = weighted_cpuload(rq);
5849,5850c5612,5613
< 	/* Discount task's runnable from CPU's runnable */
< 	lsub_positive(&runnable, p->se.avg.runnable_avg);
---
> 	if (type == 0 || !sched_feat(LB_BIAS))
> 		return total;
5852c5615
< 	return runnable;
---
> 	return max(rq->cpu_load[type-1], total);
5859a5623,5639
> static unsigned long capacity_orig_of(int cpu)
> {
> 	return cpu_rq(cpu)->cpu_capacity_orig;
> }
> 
> static unsigned long cpu_avg_load_per_task(int cpu)
> {
> 	struct rq *rq = cpu_rq(cpu);
> 	unsigned long nr_running = READ_ONCE(rq->cfs.h_nr_running);
> 	unsigned long load_avg = weighted_cpuload(rq);
> 
> 	if (nr_running)
> 		return load_avg / nr_running;
> 
> 	return 0;
> }
> 
5898c5678
< 	int factor = __this_cpu_read(sd_llc_size);
---
> 	int factor = this_cpu_read(sd_llc_size);
5912,5913c5692,5693
<  * wake_affine_idle() - only considers 'now', it check if the waking CPU is
<  *			cache-affine and is (or	will be) idle.
---
>  * wake_affine_idle() - only considers 'now', it check if the waking CPU is (or
>  *			will be) idle.
5919,5920c5699,5702
< static int
< wake_affine_idle(int this_cpu, int prev_cpu, int sync)
---
> 
> static bool
> wake_affine_idle(struct sched_domain *sd, struct task_struct *p,
> 		 int this_cpu, int prev_cpu, int sync)
5922,5935c5704,5705
< 	/*
< 	 * If this_cpu is idle, it implies the wakeup is from interrupt
< 	 * context. Only allow the move if cache is shared. Otherwise an
< 	 * interrupt intensive workload could force all tasks onto one
< 	 * node depending on the IO topology or IRQ affinity settings.
< 	 *
< 	 * If the prev_cpu is idle and cache affine then avoid a migration.
< 	 * There is no guarantee that the cache hot data from an interrupt
< 	 * is more important than cache hot data on the prev_cpu and from
< 	 * a cpufreq perspective, it's better to have higher utilisation
< 	 * on one CPU.
< 	 */
< 	if (available_idle_cpu(this_cpu) && cpus_share_cache(this_cpu, prev_cpu))
< 		return available_idle_cpu(prev_cpu) ? prev_cpu : this_cpu;
---
> 	if (idle_cpu(this_cpu))
> 		return true;
5938,5941c5708
< 		return this_cpu;
< 
< 	if (available_idle_cpu(prev_cpu))
< 		return prev_cpu;
---
> 		return true;
5943c5710
< 	return nr_cpumask_bits;
---
> 	return false;
5946c5713
< static int
---
> static bool
5953c5720,5721
< 	this_eff_load = cpu_load(cpu_rq(this_cpu));
---
> 	this_eff_load = target_load(this_cpu, sd->wake_idx);
> 	prev_eff_load = source_load(prev_cpu, sd->wake_idx);
5959c5727
< 			return this_cpu;
---
> 			return true;
5971d5738
< 	prev_eff_load = cpu_load(cpu_rq(prev_cpu));
5977,5986c5744
< 	/*
< 	 * If sync, adjust the weight of prev_eff_load such that if
< 	 * prev_eff == this_eff that select_idle_sibling() will consider
< 	 * stacking the wakee on top of the waker if no other CPU is
< 	 * idle.
< 	 */
< 	if (sync)
< 		prev_eff_load += 1;
< 
< 	return this_eff_load < prev_eff_load ? this_cpu : nr_cpumask_bits;
---
> 	return this_eff_load <= prev_eff_load;
5990c5748
< 		       int this_cpu, int prev_cpu, int sync)
---
> 		       int prev_cpu, int sync)
5992c5750,5751
< 	int target = nr_cpumask_bits;
---
> 	int this_cpu = smp_processor_id();
> 	bool affine = false;
5994,5995c5753,5754
< 	if (sched_feat(WA_IDLE))
< 		target = wake_affine_idle(this_cpu, prev_cpu, sync);
---
> 	if (sched_feat(WA_IDLE) && !affine)
> 		affine = wake_affine_idle(sd, p, this_cpu, prev_cpu, sync);
5997,5998c5756,5757
< 	if (sched_feat(WA_WEIGHT) && target == nr_cpumask_bits)
< 		target = wake_affine_weight(sd, p, this_cpu, prev_cpu, sync);
---
> 	if (sched_feat(WA_WEIGHT) && !affine)
> 		affine = wake_affine_weight(sd, p, this_cpu, prev_cpu, sync);
6001,6002c5760,5763
< 	if (target == nr_cpumask_bits)
< 		return prev_cpu;
---
> 	if (affine) {
> 		schedstat_inc(sd->ttwu_move_affine);
> 		schedstat_inc(p->se.statistics.nr_wakeups_affine);
> 	}
6004,6006c5765,5773
< 	schedstat_inc(sd->ttwu_move_affine);
< 	schedstat_inc(p->se.statistics.nr_wakeups_affine);
< 	return target;
---
> 	return affine;
> }
> 
> static inline int task_util(struct task_struct *p);
> static int cpu_util_wake(int cpu, struct task_struct *p);
> 
> static unsigned long capacity_spare_wake(int cpu, struct task_struct *p)
> {
> 	return capacity_orig_of(cpu) - cpu_util_wake(cpu, p);
6008a5776,5781
> /*
>  * find_idlest_group finds and returns the least busy CPU group within the
>  * domain.
>  *
>  * Assumes p is allowed on at least one CPU in sd.
>  */
6010c5783,5908
< find_idlest_group(struct sched_domain *sd, struct task_struct *p, int this_cpu);
---
> find_idlest_group(struct sched_domain *sd, struct task_struct *p,
> 		  int this_cpu, int sd_flag)
> {
> 	struct sched_group *idlest = NULL, *group = sd->groups;
> 	struct sched_group *most_spare_sg = NULL;
> 	unsigned long min_runnable_load = ULONG_MAX;
> 	unsigned long this_runnable_load = ULONG_MAX;
> 	unsigned long min_avg_load = ULONG_MAX, this_avg_load = ULONG_MAX;
> 	unsigned long most_spare = 0, this_spare = 0;
> 	int load_idx = sd->forkexec_idx;
> 	int imbalance_scale = 100 + (sd->imbalance_pct-100)/2;
> 	unsigned long imbalance = scale_load_down(NICE_0_LOAD) *
> 				(sd->imbalance_pct-100) / 100;
> 
> 	if (sd_flag & SD_BALANCE_WAKE)
> 		load_idx = sd->wake_idx;
> 
> 	do {
> 		unsigned long load, avg_load, runnable_load;
> 		unsigned long spare_cap, max_spare_cap;
> 		int local_group;
> 		int i;
> 
> 		/* Skip over this group if it has no CPUs allowed */
> 		if (!cpumask_intersects(sched_group_span(group),
> 					&p->cpus_allowed))
> 			continue;
> 
> 		local_group = cpumask_test_cpu(this_cpu,
> 					       sched_group_span(group));
> 
> 		/*
> 		 * Tally up the load of all CPUs in the group and find
> 		 * the group containing the CPU with most spare capacity.
> 		 */
> 		avg_load = 0;
> 		runnable_load = 0;
> 		max_spare_cap = 0;
> 
> 		for_each_cpu(i, sched_group_span(group)) {
> 			/* Bias balancing toward cpus of our domain */
> 			if (local_group)
> 				load = source_load(i, load_idx);
> 			else
> 				load = target_load(i, load_idx);
> 
> 			runnable_load += load;
> 
> 			avg_load += cfs_rq_load_avg(&cpu_rq(i)->cfs);
> 
> 			spare_cap = capacity_spare_wake(i, p);
> 
> 			if (spare_cap > max_spare_cap)
> 				max_spare_cap = spare_cap;
> 		}
> 
> 		/* Adjust by relative CPU capacity of the group */
> 		avg_load = (avg_load * SCHED_CAPACITY_SCALE) /
> 					group->sgc->capacity;
> 		runnable_load = (runnable_load * SCHED_CAPACITY_SCALE) /
> 					group->sgc->capacity;
> 
> 		if (local_group) {
> 			this_runnable_load = runnable_load;
> 			this_avg_load = avg_load;
> 			this_spare = max_spare_cap;
> 		} else {
> 			if (min_runnable_load > (runnable_load + imbalance)) {
> 				/*
> 				 * The runnable load is significantly smaller
> 				 * so we can pick this new cpu
> 				 */
> 				min_runnable_load = runnable_load;
> 				min_avg_load = avg_load;
> 				idlest = group;
> 			} else if ((runnable_load < (min_runnable_load + imbalance)) &&
> 				   (100*min_avg_load > imbalance_scale*avg_load)) {
> 				/*
> 				 * The runnable loads are close so take the
> 				 * blocked load into account through avg_load.
> 				 */
> 				min_avg_load = avg_load;
> 				idlest = group;
> 			}
> 
> 			if (most_spare < max_spare_cap) {
> 				most_spare = max_spare_cap;
> 				most_spare_sg = group;
> 			}
> 		}
> 	} while (group = group->next, group != sd->groups);
> 
> 	/*
> 	 * The cross-over point between using spare capacity or least load
> 	 * is too conservative for high utilization tasks on partially
> 	 * utilized systems if we require spare_capacity > task_util(p),
> 	 * so we allow for some task stuffing by using
> 	 * spare_capacity > task_util(p)/2.
> 	 *
> 	 * Spare capacity can't be used for fork because the utilization has
> 	 * not been set yet, we must first select a rq to compute the initial
> 	 * utilization.
> 	 */
> 	if (sd_flag & SD_BALANCE_FORK)
> 		goto skip_spare;
> 
> 	if (this_spare > task_util(p) / 2 &&
> 	    imbalance_scale*this_spare > 100*most_spare)
> 		return NULL;
> 
> 	if (most_spare > task_util(p) / 2)
> 		return most_spare_sg;
> 
> skip_spare:
> 	if (!idlest)
> 		return NULL;
> 
> 	if (min_runnable_load > (this_runnable_load + imbalance))
> 		return NULL;
> 
> 	if ((this_runnable_load < (min_runnable_load + imbalance)) &&
> 	     (100*this_avg_load < imbalance_scale*min_avg_load))
> 		return NULL;
> 
> 	return idlest;
> }
6013c5911
<  * find_idlest_group_cpu - find the idlest CPU among the CPUs in the group.
---
>  * find_idlest_group_cpu - find the idlest cpu among the cpus in group.
6030,6039c5928,5930
< 	for_each_cpu_and(i, sched_group_span(group), p->cpus_ptr) {
< 		struct rq *rq = cpu_rq(i);
< 
< 		if (!sched_core_cookie_match(rq, p))
< 			continue;
< 
< 		if (sched_idle_cpu(i))
< 			return i;
< 
< 		if (available_idle_cpu(i)) {
---
> 	for_each_cpu_and(i, sched_group_span(group), &p->cpus_allowed) {
> 		if (idle_cpu(i)) {
> 			struct rq *rq = cpu_rq(i);
6061,6062c5952,5953
< 			load = cpu_load(cpu_rq(i));
< 			if (load < min_load) {
---
> 			load = weighted_cpuload(cpu_rq(i));
> 			if (load < min_load || (load == min_load && i == this_cpu)) {
6077c5968
< 	if (!cpumask_intersects(sched_domain_span(sd), p->cpus_ptr))
---
> 	if (!cpumask_intersects(sched_domain_span(sd), &p->cpus_allowed))
6080,6086d5970
< 	/*
< 	 * We need task's util for cpu_util_without, sync it up to
< 	 * prev_cpu's last_update_time.
< 	 */
< 	if (!(sd_flag & SD_BALANCE_FORK))
< 		sync_entity_load_avg(&p->se);
< 
6097c5981
< 		group = find_idlest_group(sd, p, cpu);
---
> 		group = find_idlest_group(sd, p, cpu, sd_flag);
6105c5989
< 			/* Now try balancing at a lower domain level of 'cpu': */
---
> 			/* Now try balancing at a lower domain level of cpu */
6110c5994
< 		/* Now try balancing at a lower domain level of 'new_cpu': */
---
> 		/* Now try balancing at a lower domain level of new_cpu */
6119a6004
> 		/* while loop will break here if sd == NULL */
6125,6133d6009
< static inline int __select_idle_cpu(int cpu, struct task_struct *p)
< {
< 	if ((available_idle_cpu(cpu) || sched_idle_cpu(cpu)) &&
< 	    sched_cpu_cookie_match(cpu_rq(cpu), p))
< 		return cpu;
< 
< 	return -1;
< }
< 
6135,6136d6010
< DEFINE_STATIC_KEY_FALSE(sched_smt_present);
< EXPORT_SYMBOL_GPL(sched_smt_present);
6178c6052
< 		if (!available_idle_cpu(cpu))
---
> 		if (!idle_cpu(cpu))
6192c6066
< static int select_idle_core(struct task_struct *p, int core, struct cpumask *cpus, int *idle_cpu)
---
> static int select_idle_core(struct task_struct *p, struct sched_domain *sd, int target)
6194,6195c6068,6069
< 	bool idle = true;
< 	int cpu;
---
> 	struct cpumask *cpus = this_cpu_cpumask_var_ptr(select_idle_mask);
> 	int core, cpu;
6198c6072
< 		return __select_idle_cpu(core, p);
---
> 		return -1;
6200,6210c6074,6085
< 	for_each_cpu(cpu, cpu_smt_mask(core)) {
< 		if (!available_idle_cpu(cpu)) {
< 			idle = false;
< 			if (*idle_cpu == -1) {
< 				if (sched_idle_cpu(cpu) && cpumask_test_cpu(cpu, p->cpus_ptr)) {
< 					*idle_cpu = cpu;
< 					break;
< 				}
< 				continue;
< 			}
< 			break;
---
> 	if (!test_idle_cores(target, false))
> 		return -1;
> 
> 	cpumask_and(cpus, sched_domain_span(sd), &p->cpus_allowed);
> 
> 	for_each_cpu_wrap(core, cpus, target) {
> 		bool idle = true;
> 
> 		for_each_cpu(cpu, cpu_smt_mask(core)) {
> 			cpumask_clear_cpu(cpu, cpus);
> 			if (!idle_cpu(cpu))
> 				idle = false;
6212,6213c6087,6089
< 		if (*idle_cpu == -1 && cpumask_test_cpu(cpu, p->cpus_ptr))
< 			*idle_cpu = cpu;
---
> 
> 		if (idle)
> 			return core;
6216,6217c6092,6095
< 	if (idle)
< 		return core;
---
> 	/*
> 	 * Failed to find an idle core; stop looking for one.
> 	 */
> 	set_idle_cores(target, 0);
6219d6096
< 	cpumask_andnot(cpus, cpus, cpu_smt_mask(core));
6229a6107,6109
> 	if (!static_branch_likely(&sched_smt_present))
> 		return -1;
> 
6231,6232c6111
< 		if (!cpumask_test_cpu(cpu, p->cpus_ptr) ||
< 		    !cpumask_test_cpu(cpu, sched_domain_span(sd)))
---
> 		if (!cpumask_test_cpu(cpu, &p->cpus_allowed))
6234c6113
< 		if (available_idle_cpu(cpu) || sched_idle_cpu(cpu))
---
> 		if (idle_cpu(cpu))
6243,6247c6122
< static inline void set_idle_cores(int cpu, int val)
< {
< }
< 
< static inline bool test_idle_cores(int cpu, bool def)
---
> static inline int select_idle_core(struct task_struct *p, struct sched_domain *sd, int target)
6249,6254c6124
< 	return def;
< }
< 
< static inline int select_idle_core(struct task_struct *p, int core, struct cpumask *cpus, int *idle_cpu)
< {
< 	return __select_idle_cpu(core, p);
---
> 	return -1;
6269c6139
< static int select_idle_cpu(struct task_struct *p, struct sched_domain *sd, bool has_idle_core, int target)
---
> static int select_idle_cpu(struct task_struct *p, struct sched_domain *sd, int target)
6271,6274d6140
< 	struct cpumask *cpus = this_cpu_cpumask_var_ptr(select_idle_mask);
< 	int i, cpu, idle_cpu = -1, nr = INT_MAX;
< 	struct rq *this_rq = this_rq();
< 	int this = smp_processor_id();
6276c6142,6145
< 	u64 time = 0;
---
> 	u64 avg_cost, avg_idle;
> 	u64 time, cost;
> 	s64 delta;
> 	int cpu, nr = INT_MAX;
6282,6298c6151,6156
< 	cpumask_and(cpus, sched_domain_span(sd), p->cpus_ptr);
< 
< 	if (sched_feat(SIS_PROP) && !has_idle_core) {
< 		u64 avg_cost, avg_idle, span_avg;
< 		unsigned long now = jiffies;
< 
< 		/*
< 		 * If we're busy, the assumption that the last idle period
< 		 * predicts the future is flawed; age away the remaining
< 		 * predicted idle time.
< 		 */
< 		if (unlikely(this_rq->wake_stamp < now)) {
< 			while (this_rq->wake_stamp < now && this_rq->wake_avg_idle) {
< 				this_rq->wake_stamp++;
< 				this_rq->wake_avg_idle >>= 1;
< 			}
< 		}
---
> 	/*
> 	 * Due to large variance we need a large fuzz factor; hackbench in
> 	 * particularly is sensitive here.
> 	 */
> 	avg_idle = this_rq()->avg_idle / 512;
> 	avg_cost = this_sd->avg_scan_cost + 1;
6300,6301c6158,6159
< 		avg_idle = this_rq->wake_avg_idle;
< 		avg_cost = this_sd->avg_scan_cost + 1;
---
> 	if (sched_feat(SIS_AVG_CPU) && avg_idle < avg_cost)
> 		return -1;
6303c6161,6162
< 		span_avg = sd->span_weight * avg_idle;
---
> 	if (sched_feat(SIS_PROP)) {
> 		u64 span_avg = sd->span_weight * avg_idle;
6308,6309d6166
< 
< 		time = cpu_clock(this);
6312,6337c6169
< 	for_each_cpu_wrap(cpu, cpus, target + 1) {
< 		if (has_idle_core) {
< 			i = select_idle_core(p, cpu, cpus, &idle_cpu);
< 			if ((unsigned int)i < nr_cpumask_bits)
< 				return i;
< 
< 		} else {
< 			if (!--nr)
< 				return -1;
< 			idle_cpu = __select_idle_cpu(cpu, p);
< 			if ((unsigned int)idle_cpu < nr_cpumask_bits)
< 				break;
< 		}
< 	}
< 
< 	if (has_idle_core)
< 		set_idle_cores(target, false);
< 
< 	if (sched_feat(SIS_PROP) && !has_idle_core) {
< 		time = cpu_clock(this) - time;
< 
< 		/*
< 		 * Account for the scan cost of wakeups against the average
< 		 * idle time.
< 		 */
< 		this_rq->wake_avg_idle -= min(this_rq->wake_avg_idle, time);
---
> 	time = local_clock();
6339,6365c6171,6174
< 		update_avg(&this_sd->avg_scan_cost, time);
< 	}
< 
< 	return idle_cpu;
< }
< 
< /*
<  * Scan the asym_capacity domain for idle CPUs; pick the first idle one on which
<  * the task fits. If no CPU is big enough, but there are idle ones, try to
<  * maximize capacity.
<  */
< static int
< select_idle_capacity(struct task_struct *p, struct sched_domain *sd, int target)
< {
< 	unsigned long task_util, best_cap = 0;
< 	int cpu, best_cpu = -1;
< 	struct cpumask *cpus;
< 
< 	cpus = this_cpu_cpumask_var_ptr(select_idle_mask);
< 	cpumask_and(cpus, sched_domain_span(sd), p->cpus_ptr);
< 
< 	task_util = uclamp_task_util(p);
< 
< 	for_each_cpu_wrap(cpu, cpus, target) {
< 		unsigned long cpu_cap = capacity_of(cpu);
< 
< 		if (!available_idle_cpu(cpu) && !sched_idle_cpu(cpu))
---
> 	for_each_cpu_wrap(cpu, sched_domain_span(sd), target) {
> 		if (!--nr)
> 			return -1;
> 		if (!cpumask_test_cpu(cpu, &p->cpus_allowed))
6367,6373c6176,6177
< 		if (fits_capacity(task_util, cpu_cap))
< 			return cpu;
< 
< 		if (cpu_cap > best_cap) {
< 			best_cap = cpu_cap;
< 			best_cpu = cpu;
< 		}
---
> 		if (idle_cpu(cpu))
> 			break;
6376,6382c6180,6183
< 	return best_cpu;
< }
< 
< static inline bool asym_fits_capacity(int task_util, int cpu)
< {
< 	if (static_branch_unlikely(&sched_asym_cpucapacity))
< 		return fits_capacity(task_util, capacity_of(cpu));
---
> 	time = local_clock() - time;
> 	cost = this_sd->avg_scan_cost;
> 	delta = (s64)(time - cost) / 8;
> 	this_sd->avg_scan_cost += delta;
6384c6185
< 	return true;
---
> 	return cpu;
6392d6192
< 	bool has_idle_core = false;
6394,6409c6194
< 	unsigned long task_util;
< 	int i, recent_used_cpu;
< 
< 	/*
< 	 * On asymmetric system, update task utilization because we will check
< 	 * that the task fits with cpu's capacity.
< 	 */
< 	if (static_branch_unlikely(&sched_asym_cpucapacity)) {
< 		sync_entity_load_avg(&p->se);
< 		task_util = uclamp_task_util(p);
< 	}
< 
< 	/*
< 	 * per-cpu select_idle_mask usage
< 	 */
< 	lockdep_assert_irqs_disabled();
---
> 	int i;
6411,6412c6196
< 	if ((available_idle_cpu(target) || sched_idle_cpu(target)) &&
< 	    asym_fits_capacity(task_util, target))
---
> 	if (idle_cpu(target))
6416c6200
< 	 * If the previous CPU is cache affine and idle, don't be stupid:
---
> 	 * If the previous cpu is cache affine and idle, don't be stupid.
6418,6433c6202
< 	if (prev != target && cpus_share_cache(prev, target) &&
< 	    (available_idle_cpu(prev) || sched_idle_cpu(prev)) &&
< 	    asym_fits_capacity(task_util, prev))
< 		return prev;
< 
< 	/*
< 	 * Allow a per-cpu kthread to stack with the wakee if the
< 	 * kworker thread and the tasks previous CPUs are the same.
< 	 * The assumption is that the wakee queued work for the
< 	 * per-cpu kthread that is now complete and the wakeup is
< 	 * essentially a sync wakeup. An obvious example of this
< 	 * pattern is IO completions.
< 	 */
< 	if (is_per_cpu_kthread(current) &&
< 	    prev == smp_processor_id() &&
< 	    this_rq()->nr_running <= 1) {
---
> 	if (prev != target && cpus_share_cache(prev, target) && idle_cpu(prev))
6435,6472d6203
< 	}
< 
< 	/* Check a recently used CPU as a potential idle candidate: */
< 	recent_used_cpu = p->recent_used_cpu;
< 	p->recent_used_cpu = prev;
< 	if (recent_used_cpu != prev &&
< 	    recent_used_cpu != target &&
< 	    cpus_share_cache(recent_used_cpu, target) &&
< 	    (available_idle_cpu(recent_used_cpu) || sched_idle_cpu(recent_used_cpu)) &&
< 	    cpumask_test_cpu(p->recent_used_cpu, p->cpus_ptr) &&
< 	    asym_fits_capacity(task_util, recent_used_cpu)) {
< 		/*
< 		 * Replace recent_used_cpu with prev as it is a potential
< 		 * candidate for the next wake:
< 		 */
< 		p->recent_used_cpu = prev;
< 		return recent_used_cpu;
< 	}
< 
< 	/*
< 	 * For asymmetric CPU capacity systems, our domain of interest is
< 	 * sd_asym_cpucapacity rather than sd_llc.
< 	 */
< 	if (static_branch_unlikely(&sched_asym_cpucapacity)) {
< 		sd = rcu_dereference(per_cpu(sd_asym_cpucapacity, target));
< 		/*
< 		 * On an asymmetric CPU capacity system where an exclusive
< 		 * cpuset defines a symmetric island (i.e. one unique
< 		 * capacity_orig value through the cpuset), the key will be set
< 		 * but the CPUs within that cpuset will not have a domain with
< 		 * SD_ASYM_CPUCAPACITY. These should follow the usual symmetric
< 		 * capacity path.
< 		 */
< 		if (sd) {
< 			i = select_idle_capacity(p, sd, target);
< 			return ((unsigned)i < nr_cpumask_bits) ? i : target;
< 		}
< 	}
6478,6479c6209,6211
< 	if (sched_smt_active()) {
< 		has_idle_core = test_idle_cores(target, false);
---
> 	i = select_idle_core(p, sd, target);
> 	if ((unsigned)i < nr_cpumask_bits)
> 		return i;
6481,6486c6213,6215
< 		if (!has_idle_core && cpus_share_cache(prev, target)) {
< 			i = select_idle_smt(p, sd, prev);
< 			if ((unsigned int)i < nr_cpumask_bits)
< 				return i;
< 		}
< 	}
---
> 	i = select_idle_cpu(p, sd, target);
> 	if ((unsigned)i < nr_cpumask_bits)
> 		return i;
6488c6217
< 	i = select_idle_cpu(p, sd, has_idle_core, target);
---
> 	i = select_idle_smt(p, sd, target);
6495,6501c6224,6228
< /**
<  * cpu_util - Estimates the amount of capacity of a CPU used by CFS tasks.
<  * @cpu: the CPU to get the utilization of
<  *
<  * The unit of the return value must be the one of capacity so we can compare
<  * the utilization with the capacity of the CPU that is available for CFS task
<  * (ie cpu_capacity).
---
> /*
>  * cpu_util returns the amount of capacity of a CPU that is used by CFS
>  * tasks. The unit of the return value must be the one of capacity so we can
>  * compare the utilization with the capacity of the CPU that is available for
>  * CFS task (ie cpu_capacity).
6512,6519d6238
<  * The estimated utilization of a CPU is defined to be the maximum between its
<  * cfs_rq.avg.util_avg and the sum of the estimated utilization of the tasks
<  * currently RUNNABLE on that CPU.
<  * This allows to properly represent the expected utilization of a CPU which
<  * has just got a big task running since a long sleep period. At the same time
<  * however it preserves the benefits of the "blocked utilization" in
<  * describing the potential for other tasks waking up on the same CPU.
<  *
6530,6531d6248
<  *
<  * Return: the (estimated) utilization for the specified CPU
6533c6250
< static inline unsigned long cpu_util(int cpu)
---
> static int cpu_util(int cpu)
6535,6542c6252,6253
< 	struct cfs_rq *cfs_rq;
< 	unsigned int util;
< 
< 	cfs_rq = &cpu_rq(cpu)->cfs;
< 	util = READ_ONCE(cfs_rq->avg.util_avg);
< 
< 	if (sched_feat(UTIL_EST))
< 		util = max(util, READ_ONCE(cfs_rq->avg.util_est.enqueued));
---
> 	unsigned long util = cpu_rq(cpu)->cfs.avg.util_avg;
> 	unsigned long capacity = capacity_orig_of(cpu);
6544c6255
< 	return min_t(unsigned long, util, capacity_orig_of(cpu));
---
> 	return (util >= capacity) ? capacity : util;
6547,6560c6258
< /*
<  * cpu_util_without: compute cpu utilization without any contributions from *p
<  * @cpu: the CPU which utilization is requested
<  * @p: the task which utilization should be discounted
<  *
<  * The utilization of a CPU is defined by the utilization of tasks currently
<  * enqueued on that CPU as well as tasks which are currently sleeping after an
<  * execution on that CPU.
<  *
<  * This method returns the utilization of the specified CPU by discounting the
<  * utilization of the specified task, whenever the task is currently
<  * contributing to the CPU utilization.
<  */
< static unsigned long cpu_util_without(int cpu, struct task_struct *p)
---
> static inline int task_util(struct task_struct *p)
6562,6633c6260
< 	struct cfs_rq *cfs_rq;
< 	unsigned int util;
< 
< 	/* Task has no contribution or is new */
< 	if (cpu != task_cpu(p) || !READ_ONCE(p->se.avg.last_update_time))
< 		return cpu_util(cpu);
< 
< 	cfs_rq = &cpu_rq(cpu)->cfs;
< 	util = READ_ONCE(cfs_rq->avg.util_avg);
< 
< 	/* Discount task's util from CPU's util */
< 	lsub_positive(&util, task_util(p));
< 
< 	/*
< 	 * Covered cases:
< 	 *
< 	 * a) if *p is the only task sleeping on this CPU, then:
< 	 *      cpu_util (== task_util) > util_est (== 0)
< 	 *    and thus we return:
< 	 *      cpu_util_without = (cpu_util - task_util) = 0
< 	 *
< 	 * b) if other tasks are SLEEPING on this CPU, which is now exiting
< 	 *    IDLE, then:
< 	 *      cpu_util >= task_util
< 	 *      cpu_util > util_est (== 0)
< 	 *    and thus we discount *p's blocked utilization to return:
< 	 *      cpu_util_without = (cpu_util - task_util) >= 0
< 	 *
< 	 * c) if other tasks are RUNNABLE on that CPU and
< 	 *      util_est > cpu_util
< 	 *    then we use util_est since it returns a more restrictive
< 	 *    estimation of the spare capacity on that CPU, by just
< 	 *    considering the expected utilization of tasks already
< 	 *    runnable on that CPU.
< 	 *
< 	 * Cases a) and b) are covered by the above code, while case c) is
< 	 * covered by the following code when estimated utilization is
< 	 * enabled.
< 	 */
< 	if (sched_feat(UTIL_EST)) {
< 		unsigned int estimated =
< 			READ_ONCE(cfs_rq->avg.util_est.enqueued);
< 
< 		/*
< 		 * Despite the following checks we still have a small window
< 		 * for a possible race, when an execl's select_task_rq_fair()
< 		 * races with LB's detach_task():
< 		 *
< 		 *   detach_task()
< 		 *     p->on_rq = TASK_ON_RQ_MIGRATING;
< 		 *     ---------------------------------- A
< 		 *     deactivate_task()                   \
< 		 *       dequeue_task()                     + RaceTime
< 		 *         util_est_dequeue()              /
< 		 *     ---------------------------------- B
< 		 *
< 		 * The additional check on "current == p" it's required to
< 		 * properly fix the execl regression and it helps in further
< 		 * reducing the chances for the above race.
< 		 */
< 		if (unlikely(task_on_rq_queued(p) || current == p))
< 			lsub_positive(&estimated, _task_util_est(p));
< 
< 		util = max(util, estimated);
< 	}
< 
< 	/*
< 	 * Utilization (estimated) can exceed the CPU capacity, thus let's
< 	 * clamp to the maximum CPU capacity to ensure consistency with
< 	 * the cpu_util call.
< 	 */
< 	return min_t(unsigned long, util, capacity_orig_of(cpu));
---
> 	return p->se.avg.util_avg;
6637,6638c6264,6265
<  * Predicts what cpu_util(@cpu) would return if @p was migrated (and enqueued)
<  * to @dst_cpu.
---
>  * cpu_util_wake: Compute cpu utilization with any contributions from
>  * the waking task p removed.
6640c6267
< static unsigned long cpu_util_next(int cpu, struct task_struct *p, int dst_cpu)
---
> static int cpu_util_wake(int cpu, struct task_struct *p)
6642,6643c6269
< 	struct cfs_rq *cfs_rq = &cpu_rq(cpu)->cfs;
< 	unsigned long util_est, util = READ_ONCE(cfs_rq->avg.util_avg);
---
> 	unsigned long util, capacity;
6645,6666c6271,6273
< 	/*
< 	 * If @p migrates from @cpu to another, remove its contribution. Or,
< 	 * if @p migrates from another CPU to @cpu, add its contribution. In
< 	 * the other cases, @cpu is not impacted by the migration, so the
< 	 * util_avg should already be correct.
< 	 */
< 	if (task_cpu(p) == cpu && dst_cpu != cpu)
< 		lsub_positive(&util, task_util(p));
< 	else if (task_cpu(p) != cpu && dst_cpu == cpu)
< 		util += task_util(p);
< 
< 	if (sched_feat(UTIL_EST)) {
< 		util_est = READ_ONCE(cfs_rq->avg.util_est.enqueued);
< 
< 		/*
< 		 * During wake-up, the task isn't enqueued yet and doesn't
< 		 * appear in the cfs_rq->avg.util_est.enqueued of any rq,
< 		 * so just add it (if needed) to "simulate" what will be
< 		 * cpu_util() after the task has been enqueued.
< 		 */
< 		if (dst_cpu == cpu)
< 			util_est += _task_util_est(p);
---
> 	/* Task has no contribution or is new */
> 	if (cpu != task_cpu(p) || !p->se.avg.last_update_time)
> 		return cpu_util(cpu);
6668,6669c6275,6276
< 		util = max(util, util_est);
< 	}
---
> 	capacity = capacity_orig_of(cpu);
> 	util = max_t(long, cpu_rq(cpu)->cfs.avg.util_avg - task_util(p), 0);
6671c6278
< 	return min(util, capacity_orig_of(cpu));
---
> 	return (util >= capacity) ? capacity : util;
6675,6679c6282,6286
<  * compute_energy(): Estimates the energy that @pd would consume if @p was
<  * migrated to @dst_cpu. compute_energy() predicts what will be the utilization
<  * landscape of @pd's CPUs after the task migration, and uses the Energy Model
<  * to compute what would be the energy if we decided to actually migrate that
<  * task.
---
>  * Disable WAKE_AFFINE in the case where task @p doesn't fit in the
>  * capacity of either the waking CPU @cpu or the previous CPU @prev_cpu.
>  *
>  * In that case WAKE_AFFINE doesn't make sense and we'll let
>  * BALANCE_WAKE sort things out.
6681,6682c6288
< static long
< compute_energy(struct task_struct *p, int dst_cpu, struct perf_domain *pd)
---
> static int wake_cap(struct task_struct *p, int cpu, int prev_cpu)
6684,6688c6290
< 	struct cpumask *pd_mask = perf_domain_span(pd);
< 	unsigned long cpu_cap = arch_scale_cpu_capacity(cpumask_first(pd_mask));
< 	unsigned long max_util = 0, sum_util = 0;
< 	unsigned long _cpu_cap = cpu_cap;
< 	int cpu;
---
> 	long min_cap, max_cap;
6690c6292,6293
< 	_cpu_cap -= arch_scale_thermal_pressure(cpumask_first(pd_mask));
---
> 	min_cap = min(capacity_orig_of(prev_cpu), capacity_orig_of(cpu));
> 	max_cap = cpu_rq(cpu)->rd->max_cpu_capacity;
6692,6810c6295,6297
< 	/*
< 	 * The capacity state of CPUs of the current rd can be driven by CPUs
< 	 * of another rd if they belong to the same pd. So, account for the
< 	 * utilization of these CPUs too by masking pd with cpu_online_mask
< 	 * instead of the rd span.
< 	 *
< 	 * If an entire pd is outside of the current rd, it will not appear in
< 	 * its pd list and will not be accounted by compute_energy().
< 	 */
< 	for_each_cpu_and(cpu, pd_mask, cpu_online_mask) {
< 		unsigned long util_freq = cpu_util_next(cpu, p, dst_cpu);
< 		unsigned long cpu_util, util_running = util_freq;
< 		struct task_struct *tsk = NULL;
< 
< 		/*
< 		 * When @p is placed on @cpu:
< 		 *
< 		 * util_running = max(cpu_util, cpu_util_est) +
< 		 *		  max(task_util, _task_util_est)
< 		 *
< 		 * while cpu_util_next is: max(cpu_util + task_util,
< 		 *			       cpu_util_est + _task_util_est)
< 		 */
< 		if (cpu == dst_cpu) {
< 			tsk = p;
< 			util_running =
< 				cpu_util_next(cpu, p, -1) + task_util_est(p);
< 		}
< 
< 		/*
< 		 * Busy time computation: utilization clamping is not
< 		 * required since the ratio (sum_util / cpu_capacity)
< 		 * is already enough to scale the EM reported power
< 		 * consumption at the (eventually clamped) cpu_capacity.
< 		 */
< 		cpu_util = effective_cpu_util(cpu, util_running, cpu_cap,
< 					      ENERGY_UTIL, NULL);
< 
< 		sum_util += min(cpu_util, _cpu_cap);
< 
< 		/*
< 		 * Performance domain frequency: utilization clamping
< 		 * must be considered since it affects the selection
< 		 * of the performance domain frequency.
< 		 * NOTE: in case RT tasks are running, by default the
< 		 * FREQUENCY_UTIL's utilization can be max OPP.
< 		 */
< 		cpu_util = effective_cpu_util(cpu, util_freq, cpu_cap,
< 					      FREQUENCY_UTIL, tsk);
< 		max_util = max(max_util, min(cpu_util, _cpu_cap));
< 	}
< 
< 	return em_cpu_energy(pd->em_pd, max_util, sum_util, _cpu_cap);
< }
< 
< /*
<  * find_energy_efficient_cpu(): Find most energy-efficient target CPU for the
<  * waking task. find_energy_efficient_cpu() looks for the CPU with maximum
<  * spare capacity in each performance domain and uses it as a potential
<  * candidate to execute the task. Then, it uses the Energy Model to figure
<  * out which of the CPU candidates is the most energy-efficient.
<  *
<  * The rationale for this heuristic is as follows. In a performance domain,
<  * all the most energy efficient CPU candidates (according to the Energy
<  * Model) are those for which we'll request a low frequency. When there are
<  * several CPUs for which the frequency request will be the same, we don't
<  * have enough data to break the tie between them, because the Energy Model
<  * only includes active power costs. With this model, if we assume that
<  * frequency requests follow utilization (e.g. using schedutil), the CPU with
<  * the maximum spare capacity in a performance domain is guaranteed to be among
<  * the best candidates of the performance domain.
<  *
<  * In practice, it could be preferable from an energy standpoint to pack
<  * small tasks on a CPU in order to let other CPUs go in deeper idle states,
<  * but that could also hurt our chances to go cluster idle, and we have no
<  * ways to tell with the current Energy Model if this is actually a good
<  * idea or not. So, find_energy_efficient_cpu() basically favors
<  * cluster-packing, and spreading inside a cluster. That should at least be
<  * a good thing for latency, and this is consistent with the idea that most
<  * of the energy savings of EAS come from the asymmetry of the system, and
<  * not so much from breaking the tie between identical CPUs. That's also the
<  * reason why EAS is enabled in the topology code only for systems where
<  * SD_ASYM_CPUCAPACITY is set.
<  *
<  * NOTE: Forkees are not accepted in the energy-aware wake-up path because
<  * they don't have any useful utilization data yet and it's not possible to
<  * forecast their impact on energy consumption. Consequently, they will be
<  * placed by find_idlest_cpu() on the least loaded CPU, which might turn out
<  * to be energy-inefficient in some use-cases. The alternative would be to
<  * bias new tasks towards specific types of CPUs first, or to try to infer
<  * their util_avg from the parent task, but those heuristics could hurt
<  * other use-cases too. So, until someone finds a better way to solve this,
<  * let's keep things simple by re-using the existing slow path.
<  */
< static int find_energy_efficient_cpu(struct task_struct *p, int prev_cpu)
< {
< 	unsigned long prev_delta = ULONG_MAX, best_delta = ULONG_MAX;
< 	struct root_domain *rd = cpu_rq(smp_processor_id())->rd;
< 	int cpu, best_energy_cpu = prev_cpu, target = -1;
< 	unsigned long cpu_cap, util, base_energy = 0;
< 	struct sched_domain *sd;
< 	struct perf_domain *pd;
< 
< 	rcu_read_lock();
< 	pd = rcu_dereference(rd->pd);
< 	if (!pd || READ_ONCE(rd->overutilized))
< 		goto unlock;
< 
< 	/*
< 	 * Energy-aware wake-up happens on the lowest sched_domain starting
< 	 * from sd_asym_cpucapacity spanning over this_cpu and prev_cpu.
< 	 */
< 	sd = rcu_dereference(*this_cpu_ptr(&sd_asym_cpucapacity));
< 	while (sd && !cpumask_test_cpu(prev_cpu, sched_domain_span(sd)))
< 		sd = sd->parent;
< 	if (!sd)
< 		goto unlock;
< 
< 	target = prev_cpu;
---
> 	/* Minimum capacity is close to max, no need to abort wake_affine */
> 	if (max_cap - min_cap < max_cap >> 3)
> 		return 0;
6811a6299
> 	/* Bring task utilization in sync with prev_cpu */
6813,6883d6300
< 	if (!task_util_est(p))
< 		goto unlock;
< 
< 	for (; pd; pd = pd->next) {
< 		unsigned long cur_delta, spare_cap, max_spare_cap = 0;
< 		bool compute_prev_delta = false;
< 		unsigned long base_energy_pd;
< 		int max_spare_cap_cpu = -1;
< 
< 		for_each_cpu_and(cpu, perf_domain_span(pd), sched_domain_span(sd)) {
< 			if (!cpumask_test_cpu(cpu, p->cpus_ptr))
< 				continue;
< 
< 			util = cpu_util_next(cpu, p, cpu);
< 			cpu_cap = capacity_of(cpu);
< 			spare_cap = cpu_cap;
< 			lsub_positive(&spare_cap, util);
< 
< 			/*
< 			 * Skip CPUs that cannot satisfy the capacity request.
< 			 * IOW, placing the task there would make the CPU
< 			 * overutilized. Take uclamp into account to see how
< 			 * much capacity we can get out of the CPU; this is
< 			 * aligned with sched_cpu_util().
< 			 */
< 			util = uclamp_rq_util_with(cpu_rq(cpu), util, p);
< 			if (!fits_capacity(util, cpu_cap))
< 				continue;
< 
< 			if (cpu == prev_cpu) {
< 				/* Always use prev_cpu as a candidate. */
< 				compute_prev_delta = true;
< 			} else if (spare_cap > max_spare_cap) {
< 				/*
< 				 * Find the CPU with the maximum spare capacity
< 				 * in the performance domain.
< 				 */
< 				max_spare_cap = spare_cap;
< 				max_spare_cap_cpu = cpu;
< 			}
< 		}
< 
< 		if (max_spare_cap_cpu < 0 && !compute_prev_delta)
< 			continue;
< 
< 		/* Compute the 'base' energy of the pd, without @p */
< 		base_energy_pd = compute_energy(p, -1, pd);
< 		base_energy += base_energy_pd;
< 
< 		/* Evaluate the energy impact of using prev_cpu. */
< 		if (compute_prev_delta) {
< 			prev_delta = compute_energy(p, prev_cpu, pd);
< 			if (prev_delta < base_energy_pd)
< 				goto unlock;
< 			prev_delta -= base_energy_pd;
< 			best_delta = min(best_delta, prev_delta);
< 		}
< 
< 		/* Evaluate the energy impact of using max_spare_cap_cpu. */
< 		if (max_spare_cap_cpu >= 0) {
< 			cur_delta = compute_energy(p, max_spare_cap_cpu, pd);
< 			if (cur_delta < base_energy_pd)
< 				goto unlock;
< 			cur_delta -= base_energy_pd;
< 			if (cur_delta < best_delta) {
< 				best_delta = cur_delta;
< 				best_energy_cpu = max_spare_cap_cpu;
< 			}
< 		}
< 	}
< 	rcu_read_unlock();
6885,6898c6302
< 	/*
< 	 * Pick the best CPU if prev_cpu cannot be used, or if it saves at
< 	 * least 6% of the energy used by prev_cpu.
< 	 */
< 	if ((prev_delta == ULONG_MAX) ||
< 	    (prev_delta - best_delta) > ((prev_delta + base_energy) >> 4))
< 		target = best_energy_cpu;
< 
< 	return target;
< 
< unlock:
< 	rcu_read_unlock();
< 
< 	return target;
---
> 	return min_cap * 1024 < task_util(p) * capacity_margin;
6903c6307
<  * that have the relevant SD flag set. In practice, this is SD_BALANCE_WAKE,
---
>  * that have the 'sd_flag' flag set. In practice, this is SD_BALANCE_WAKE,
6906,6907c6310,6311
<  * Balances load by selecting the idlest CPU in the idlest group, or under
<  * certain conditions an idle sibling CPU if the domain has SD_WAKE_AFFINE set.
---
>  * Balances load by selecting the idlest cpu in the idlest group, or under
>  * certain conditions an idle sibling cpu if the domain has SD_WAKE_AFFINE set.
6909c6313,6315
<  * Returns the target CPU number.
---
>  * Returns the target cpu number.
>  *
>  * preempt must be disabled.
6912c6318
< select_task_rq_fair(struct task_struct *p, int prev_cpu, int wake_flags)
---
> select_task_rq_fair(struct task_struct *p, int prev_cpu, int sd_flag, int wake_flags)
6914,6915c6320
< 	int sync = (wake_flags & WF_SYNC) && !(current->flags & PF_EXITING);
< 	struct sched_domain *tmp, *sd = NULL;
---
> 	struct sched_domain *tmp, *affine_sd = NULL, *sd = NULL;
6919,6920c6324
< 	/* SD_flags and WF_flags share the first nibble */
< 	int sd_flag = wake_flags & 0xF;
---
> 	int sync = wake_flags & WF_SYNC;
6922,6926c6326
< 	/*
< 	 * required for stable ->cpus_allowed
< 	 */
< 	lockdep_assert_held(&p->pi_lock);
< 	if (wake_flags & WF_TTWU) {
---
> 	if (sd_flag & SD_BALANCE_WAKE) {
6928,6936c6328,6329
< 
< 		if (sched_energy_enabled()) {
< 			new_cpu = find_energy_efficient_cpu(p, prev_cpu);
< 			if (new_cpu >= 0)
< 				return new_cpu;
< 			new_cpu = prev_cpu;
< 		}
< 
< 		want_affine = !wake_wide(p) && cpumask_test_cpu(cpu, p->cpus_ptr);
---
> 		want_affine = !wake_wide(p) && !wake_cap(p, cpu, prev_cpu)
> 			      && cpumask_test_cpu(cpu, &p->cpus_allowed);
6940a6334,6336
> 		if (!(tmp->flags & SD_LOAD_BALANCE))
> 			break;
> 
6942c6338
< 		 * If both 'cpu' and 'prev_cpu' are part of this domain,
---
> 		 * If both cpu and prev_cpu are part of this domain,
6947,6950c6343
< 			if (cpu != prev_cpu)
< 				new_cpu = wake_affine(tmp, p, cpu, prev_cpu, sync);
< 
< 			sd = NULL; /* Prefer wake_affine over balance flags */
---
> 			affine_sd = tmp;
6960,6961c6353,6376
< 	if (unlikely(sd)) {
< 		/* Slow path */
---
> 	if (affine_sd) {
> 		sd = NULL; /* Prefer wake_affine over balance flags */
> 		if (cpu == prev_cpu)
> 			goto pick_cpu;
> 
> 		if (wake_affine(affine_sd, p, prev_cpu, sync))
> 			new_cpu = cpu;
> 	}
> 
> 	if (sd && !(sd_flag & SD_BALANCE_FORK)) {
> 		/*
> 		 * We're going to need the task's util for capacity_spare_wake
> 		 * in find_idlest_group. Sync it up to prev_cpu's
> 		 * last_update_time.
> 		 */
> 		sync_entity_load_avg(&p->se);
> 	}
> 
> 	if (!sd) {
> pick_cpu:
> 		if (sd_flag & SD_BALANCE_WAKE) /* XXX always ? */
> 			new_cpu = select_idle_sibling(p, prev_cpu, new_cpu);
> 
> 	} else {
6963,6965d6377
< 	} else if (wake_flags & WF_TTWU) { /* XXX always ? */
< 		/* Fast path */
< 		new_cpu = select_idle_sibling(p, prev_cpu, new_cpu);
6975c6387
<  * Called immediately before a task is migrated to a new CPU; task_cpu(p) and
---
>  * Called immediately before a task is migrated to a new cpu; task_cpu(p) and
6977c6389
<  * previous CPU. The caller guarantees p->pi_lock or task_rq(p)->lock is held.
---
>  * previous cpu. The caller guarantees p->pi_lock or task_rq(p)->lock is held.
6979c6391
< static void migrate_task_rq_fair(struct task_struct *p, int new_cpu)
---
> static void migrate_task_rq_fair(struct task_struct *p)
6987c6399
< 	if (READ_ONCE(p->__state) == TASK_WAKING) {
---
> 	if (p->state == TASK_WAKING) {
7012c6424
< 		lockdep_assert_rq_held(task_rq(p));
---
> 		lockdep_assert_held(&task_rq(p)->lock);
7032,7033d6443
< 
< 	update_scan_period(p, new_cpu);
7040,7048d6449
< 
< static int
< balance_fair(struct rq *rq, struct task_struct *prev, struct rq_flags *rf)
< {
< 	if (rq->nr_running)
< 		return 1;
< 
< 	return newidle_balance(rq, rf) != 0;
< }
7051c6452,6453
< static unsigned long wakeup_gran(struct sched_entity *se)
---
> static unsigned long
> wakeup_gran(struct sched_entity *curr, struct sched_entity *se)
7093c6495
< 	gran = wakeup_gran(se);
---
> 	gran = wakeup_gran(curr, se);
7101a6504,6506
> 	if (entity_is_task(se) && unlikely(task_of(se)->policy == SCHED_IDLE))
> 		return;
> 
7105,7106d6509
< 		if (se_is_idle(se))
< 			return;
7112a6516,6518
> 	if (entity_is_task(se) && unlikely(task_of(se)->policy == SCHED_IDLE))
> 		return;
> 
7116,7117d6521
< 		if (se_is_idle(se))
< 			return;
7138d6541
< 	int cse_is_idle, pse_is_idle;
7145c6548
< 	 * unconditionally check_preempt_curr() after an enqueue (which may have
---
> 	 * unconditionally check_prempt_curr() after an enqueue (which may have
7171,7172c6574,6575
< 	if (unlikely(task_has_idle_policy(curr)) &&
< 	    likely(!task_has_idle_policy(p)))
---
> 	if (unlikely(curr->policy == SCHED_IDLE) &&
> 	    likely(p->policy != SCHED_IDLE))
7183,7196d6585
< 	BUG_ON(!pse);
< 
< 	cse_is_idle = se_is_idle(se);
< 	pse_is_idle = se_is_idle(pse);
< 
< 	/*
< 	 * Preempt an idle group in favor of a non-idle group (and don't preempt
< 	 * in the inverse case).
< 	 */
< 	if (cse_is_idle && !pse_is_idle)
< 		goto preempt;
< 	if (cse_is_idle != pse_is_idle)
< 		return;
< 
7197a6587
> 	BUG_ON(!pse);
7228,7261c6618
< #ifdef CONFIG_SMP
< static struct task_struct *pick_task_fair(struct rq *rq)
< {
< 	struct sched_entity *se;
< 	struct cfs_rq *cfs_rq;
< 
< again:
< 	cfs_rq = &rq->cfs;
< 	if (!cfs_rq->nr_running)
< 		return NULL;
< 
< 	do {
< 		struct sched_entity *curr = cfs_rq->curr;
< 
< 		/* When we pick for a remote RQ, we'll not have done put_prev_entity() */
< 		if (curr) {
< 			if (curr->on_rq)
< 				update_curr(cfs_rq);
< 			else
< 				curr = NULL;
< 
< 			if (unlikely(check_cfs_rq_runtime(cfs_rq)))
< 				goto again;
< 		}
< 
< 		se = pick_next_entity(cfs_rq, curr);
< 		cfs_rq = group_cfs_rq(se);
< 	} while (cfs_rq);
< 
< 	return task_of(se);
< }
< #endif
< 
< struct task_struct *
---
> static struct task_struct *
7270c6627
< 	if (!sched_fair_runnable(rq))
---
> 	if (!cfs_rq->nr_running)
7274c6631
< 	if (!prev || prev->sched_class != &fair_sched_class)
---
> 	if (prev->sched_class != &fair_sched_class)
7351,7352c6708,6709
< 	if (prev)
< 		put_prev_task(rq, prev);
---
> 
> 	put_prev_task(rq, prev);
7362c6719
< done: __maybe_unused;
---
> done: __maybe_unused
7372c6729
< 	if (hrtick_enabled_fair(rq))
---
> 	if (hrtick_enabled(rq))
7375,7376d6731
< 	update_misfit_status(p, rq);
< 
7380,7383c6735
< 	if (!rf)
< 		return NULL;
< 
< 	new_tasks = newidle_balance(rq, rf);
---
> 	new_tasks = idle_balance(rq, rf);
7386c6738
< 	 * Because newidle_balance() releases (and re-acquires) rq->lock, it is
---
> 	 * Because idle_balance() releases (and re-acquires) rq->lock, it is
7396,7401d6747
< 	/*
< 	 * rq is about to be idle, check if we need to update the
< 	 * lost_idle_time of clock_pelt
< 	 */
< 	update_idle_rq_clock_pelt(rq);
< 
7405,7409d6750
< static struct task_struct *__pick_next_task_fair(struct rq *rq)
< {
< 	return pick_next_task_fair(rq, NULL, NULL);
< }
< 
7454c6795
< 		rq_clock_skip_update(rq);
---
> 		rq_clock_skip_update(rq, true);
7460c6801
< static bool yield_to_task_fair(struct rq *rq, struct task_struct *p)
---
> static bool yield_to_task_fair(struct rq *rq, struct task_struct *p, bool preempt)
7483c6824
<  * per-CPU scheduler provides, namely provide a proportional amount of compute
---
>  * per-cpu scheduler provides, namely provide a proportional amount of compute
7488c6829
<  * Where W_i,n is the n-th weight average for CPU i. The instantaneous weight
---
>  * Where W_i,n is the n-th weight average for cpu i. The instantaneous weight
7493c6834
<  * Where w_i,j is the weight of the j-th runnable task on CPU i. This weight
---
>  * Where w_i,j is the weight of the j-th runnable task on cpu i. This weight
7501c6842
<  * C_i is the compute capacity of CPU i, typically it is the
---
>  * C_i is the compute capacity of cpu i, typically it is the
7522c6863
<  * for all i,j solution, we create a tree of CPUs that follows the hardware
---
>  * for all i,j solution, we create a tree of cpus that follows the hardware
7524c6865
<  * in O(log n) layers. Furthermore we reduce the number of CPUs going up the
---
>  * in O(log n) layers. Furthermore we reduce the number of cpus going up the
7526c6867
<  * of load-balance at each level inv. proportional to the number of CPUs in
---
>  * of load-balance at each level inv. proportional to the number of cpus in
7535c6876
<  *         |         |     `- number of CPUs doing load-balance
---
>  *         |         |     `- number of cpus doing load-balance
7543c6884
<  * to every other CPU in at most O(log n) steps:
---
>  * to every other cpu in at most O(log n) steps:
7555c6896
<  * Showing there's indeed a path between every CPU in at most O(log n) steps.
---
>  * Showing there's indeed a path between every cpu in at most O(log n) steps.
7565c6906
<  * balancing is more aggressive and has the newly idle CPU iterate up the domain
---
>  * balancing is more aggressive and has the newly idle cpu iterate up the domain
7586c6927
<  * w_i,j,k is the weight of the j-th runnable task in the k-th cgroup on CPU i.
---
>  * w_i,j,k is the weight of the j-th runnable task in the k-th cgroup on cpu i.
7599,7643d6939
< /*
<  * 'group_type' describes the group of CPUs at the moment of load balancing.
<  *
<  * The enum is ordered by pulling priority, with the group with lowest priority
<  * first so the group_type can simply be compared when selecting the busiest
<  * group. See update_sd_pick_busiest().
<  */
< enum group_type {
< 	/* The group has spare capacity that can be used to run more tasks.  */
< 	group_has_spare = 0,
< 	/*
< 	 * The group is fully used and the tasks don't compete for more CPU
< 	 * cycles. Nevertheless, some tasks might wait before running.
< 	 */
< 	group_fully_busy,
< 	/*
< 	 * SD_ASYM_CPUCAPACITY only: One task doesn't fit with CPU's capacity
< 	 * and must be migrated to a more powerful CPU.
< 	 */
< 	group_misfit_task,
< 	/*
< 	 * SD_ASYM_PACKING only: One local CPU with higher capacity is available,
< 	 * and the task should be migrated to it instead of running on the
< 	 * current CPU.
< 	 */
< 	group_asym_packing,
< 	/*
< 	 * The tasks' affinity constraints previously prevented the scheduler
< 	 * from balancing the load across the system.
< 	 */
< 	group_imbalanced,
< 	/*
< 	 * The CPU is overloaded and can't provide expected CPU cycles to all
< 	 * tasks.
< 	 */
< 	group_overloaded
< };
< 
< enum migration_type {
< 	migrate_load = 0,
< 	migrate_util,
< 	migrate_task,
< 	migrate_misfit
< };
< 
7648d6943
< #define LBF_ACTIVE_LB	0x10
7673d6967
< 	enum migration_type	migration_type;
7684c6978
< 	lockdep_assert_rq_held(env->src_rq);
---
> 	lockdep_assert_held(&env->src_rq->lock);
7689,7693c6983
< 	if (unlikely(task_has_idle_policy(p)))
< 		return 0;
< 
< 	/* SMT siblings share cache */
< 	if (env->sd->flags & SD_SHARE_CPUCAPACITY)
---
> 	if (unlikely(p->policy == SCHED_IDLE))
7706,7713d6995
< 
< 	/*
< 	 * Don't migrate task if the task's cookie does not match
< 	 * with the destination CPU's core cookie.
< 	 */
< 	if (!sched_core_cookie_match(cpu_rq(env->dst_cpu), p))
< 		return 1;
< 
7731,7732c7013,7014
< 	unsigned long src_weight, dst_weight;
< 	int src_nid, dst_nid, dist;
---
> 	unsigned long src_faults, dst_faults;
> 	int src_nid, dst_nid;
7759c7041
< 	if (env->idle == CPU_IDLE)
---
> 	if (env->idle != CPU_NOT_IDLE)
7762d7043
< 	dist = node_distance(src_nid, dst_nid);
7764,7765c7045,7046
< 		src_weight = group_weight(p, src_nid, dist);
< 		dst_weight = group_weight(p, dst_nid, dist);
---
> 		src_faults = group_faults(p, src_nid);
> 		dst_faults = group_faults(p, dst_nid);
7767,7768c7048,7049
< 		src_weight = task_weight(p, src_nid, dist);
< 		dst_weight = task_weight(p, dst_nid, dist);
---
> 		src_faults = task_faults(p, src_nid);
> 		dst_faults = task_faults(p, dst_nid);
7771c7052
< 	return dst_weight < src_weight;
---
> 	return dst_faults < src_faults;
7790c7071
< 	lockdep_assert_rq_held(env->src_rq);
---
> 	lockdep_assert_held(&env->src_rq->lock);
7795c7076
< 	 * 2) cannot be migrated to this CPU due to cpus_ptr, or
---
> 	 * 2) cannot be migrated to this CPU due to cpus_allowed, or
7802,7806c7083
< 	/* Disregard pcpu kthreads; they are where they need to be. */
< 	if (kthread_is_per_cpu(p))
< 		return 0;
< 
< 	if (!cpumask_test_cpu(env->dst_cpu, p->cpus_ptr)) {
---
> 	if (!cpumask_test_cpu(env->dst_cpu, &p->cpus_allowed)) {
7814c7091
< 		 * Remember if this task can be migrated to any other CPU in
---
> 		 * Remember if this task can be migrated to any other cpu in
7818,7821c7095,7096
< 		 * Avoid computing new_dst_cpu
< 		 * - for NEWLY_IDLE
< 		 * - if we have already computed one in current iteration
< 		 * - if it's an active balance
---
> 		 * Avoid computing new_dst_cpu for NEWLY_IDLE or if we have
> 		 * already computed one in current iteration.
7823,7824c7098
< 		if (env->idle == CPU_NEWLY_IDLE ||
< 		    env->flags & (LBF_DST_PINNED | LBF_ACTIVE_LB))
---
> 		if (env->idle == CPU_NEWLY_IDLE || (env->flags & LBF_DST_PINNED))
7827c7101
< 		/* Prevent to re-select dst_cpu via env's CPUs: */
---
> 		/* Prevent to re-select dst_cpu via env's cpus */
7829c7103
< 			if (cpumask_test_cpu(cpu, p->cpus_ptr)) {
---
> 			if (cpumask_test_cpu(cpu, &p->cpus_allowed)) {
7839c7113
< 	/* Record that we found at least one task that could run on dst_cpu */
---
> 	/* Record that we found atleast one task that could run on dst_cpu */
7849,7852c7123,7125
< 	 * 1) active balance
< 	 * 2) destination numa is preferred
< 	 * 3) task is cache cold, or
< 	 * 4) too many balance attempts have failed.
---
> 	 * 1) destination numa is preferred
> 	 * 2) task is cache cold, or
> 	 * 3) too many balance attempts have failed.
7854,7856d7126
< 	if (env->flags & LBF_ACTIVE_LB)
< 		return 1;
< 
7879c7149
< 	lockdep_assert_rq_held(env->src_rq);
---
> 	lockdep_assert_held(&env->src_rq->lock);
7880a7151
> 	p->on_rq = TASK_ON_RQ_MIGRATING;
7895c7166
< 	lockdep_assert_rq_held(env->src_rq);
---
> 	lockdep_assert_held(&env->src_rq->lock);
7919c7190
<  * detach_tasks() -- tries to detach up to imbalance load/util/tasks from
---
>  * detach_tasks() -- tries to detach up to imbalance weighted load from
7927d7197
< 	unsigned long util, load;
7928a7199
> 	unsigned long load;
7931,7940c7202
< 	lockdep_assert_rq_held(env->src_rq);
< 
< 	/*
< 	 * Source run queue has been emptied by another CPU, clear
< 	 * LBF_ALL_PINNED flag as we will not test any task.
< 	 */
< 	if (env->src_rq->nr_running <= 1) {
< 		env->flags &= ~LBF_ALL_PINNED;
< 		return 0;
< 	}
---
> 	lockdep_assert_held(&env->src_rq->lock);
7970,7979c7232
< 		switch (env->migration_type) {
< 		case migrate_load:
< 			/*
< 			 * Depending of the number of CPUs and tasks and the
< 			 * cgroup hierarchy, task_h_load() can return a null
< 			 * value. Make sure that env->imbalance decreases
< 			 * otherwise detach_tasks() will stop only after
< 			 * detaching up to loop_max tasks.
< 			 */
< 			load = max_t(unsigned long, task_h_load(p), 1);
---
> 		load = task_h_load(p);
7981,8013c7234,7235
< 			if (sched_feat(LB_MIN) &&
< 			    load < 16 && !env->sd->nr_balance_failed)
< 				goto next;
< 
< 			/*
< 			 * Make sure that we don't migrate too much load.
< 			 * Nevertheless, let relax the constraint if
< 			 * scheduler fails to find a good waiting task to
< 			 * migrate.
< 			 */
< 			if (shr_bound(load, env->sd->nr_balance_failed) > env->imbalance)
< 				goto next;
< 
< 			env->imbalance -= load;
< 			break;
< 
< 		case migrate_util:
< 			util = task_util_est(p);
< 
< 			if (util > env->imbalance)
< 				goto next;
< 
< 			env->imbalance -= util;
< 			break;
< 
< 		case migrate_task:
< 			env->imbalance--;
< 			break;
< 
< 		case migrate_misfit:
< 			/* This is not a misfit task */
< 			if (task_fits_capacity(p, capacity_of(env->src_cpu)))
< 				goto next;
---
> 		if (sched_feat(LB_MIN) && load < 16 && !env->sd->nr_balance_failed)
> 			goto next;
8015,8017c7237,7238
< 			env->imbalance = 0;
< 			break;
< 		}
---
> 		if ((load / 2) > env->imbalance)
> 			goto next;
8022a7244
> 		env->imbalance -= load;
8024c7246
< #ifdef CONFIG_PREEMPTION
---
> #ifdef CONFIG_PREEMPT
8036c7258
< 		 * load/util/tasks.
---
> 		 * weighted load.
8061c7283
< 	lockdep_assert_rq_held(rq);
---
> 	lockdep_assert_held(&rq->lock);
8064a7287
> 	p->on_rq = TASK_ON_RQ_QUEUED;
8105,8151c7328
< #ifdef CONFIG_NO_HZ_COMMON
< static inline bool cfs_rq_has_blocked(struct cfs_rq *cfs_rq)
< {
< 	if (cfs_rq->avg.load_avg)
< 		return true;
< 
< 	if (cfs_rq->avg.util_avg)
< 		return true;
< 
< 	return false;
< }
< 
< static inline bool others_have_blocked(struct rq *rq)
< {
< 	if (READ_ONCE(rq->avg_rt.util_avg))
< 		return true;
< 
< 	if (READ_ONCE(rq->avg_dl.util_avg))
< 		return true;
< 
< 	if (thermal_load_avg(rq))
< 		return true;
< 
< #ifdef CONFIG_HAVE_SCHED_AVG_IRQ
< 	if (READ_ONCE(rq->avg_irq.util_avg))
< 		return true;
< #endif
< 
< 	return false;
< }
< 
< static inline void update_blocked_load_tick(struct rq *rq)
< {
< 	WRITE_ONCE(rq->last_blocked_load_update_tick, jiffies);
< }
< 
< static inline void update_blocked_load_status(struct rq *rq, bool has_blocked)
< {
< 	if (!has_blocked)
< 		rq->has_blocked_load = 0;
< }
< #else
< static inline bool cfs_rq_has_blocked(struct cfs_rq *cfs_rq) { return false; }
< static inline bool others_have_blocked(struct rq *rq) { return false; }
< static inline void update_blocked_load_tick(struct rq *rq) {}
< static inline void update_blocked_load_status(struct rq *rq, bool has_blocked) {}
< #endif
---
> #ifdef CONFIG_FAIR_GROUP_SCHED
8153c7330
< static bool __update_blocked_others(struct rq *rq, bool *done)
---
> static inline bool cfs_rq_is_decayed(struct cfs_rq *cfs_rq)
8155,8164c7332,7333
< 	const struct sched_class *curr_class;
< 	u64 now = rq_clock_pelt(rq);
< 	unsigned long thermal_pressure;
< 	bool decayed;
< 
< 	/*
< 	 * update_load_avg() can call cpufreq_update_util(). Make sure that RT,
< 	 * DL and IRQ signals have been updated before updating CFS.
< 	 */
< 	curr_class = rq->curr->sched_class;
---
> 	if (cfs_rq->load.weight)
> 		return false;
8166c7335,7336
< 	thermal_pressure = arch_scale_thermal_pressure(cpu_of(rq));
---
> 	if (cfs_rq->avg.load_sum)
> 		return false;
8168,8171c7338,7339
< 	decayed = update_rt_rq_load_avg(now, rq, curr_class == &rt_sched_class) |
< 		  update_dl_rq_load_avg(now, rq, curr_class == &dl_sched_class) |
< 		  update_thermal_load_avg(rq_clock_thermal(rq), rq, thermal_pressure) |
< 		  update_irq_load_avg(rq, 0);
---
> 	if (cfs_rq->avg.util_sum)
> 		return false;
8173,8174c7341,7342
< 	if (others_have_blocked(rq))
< 		*done = false;
---
> 	if (cfs_rq->avg.runnable_load_sum)
> 		return false;
8176c7344
< 	return decayed;
---
> 	return true;
8179,8181c7347
< #ifdef CONFIG_FAIR_GROUP_SCHED
< 
< static bool __update_blocked_fair(struct rq *rq, bool *done)
---
> static void update_blocked_averages(int cpu)
8182a7349
> 	struct rq *rq = cpu_rq(cpu);
8184,8185c7351,7354
< 	bool decayed = false;
< 	int cpu = cpu_of(rq);
---
> 	struct rq_flags rf;
> 
> 	rq_lock_irqsave(rq, &rf);
> 	update_rq_clock(rq);
8194,8195c7363,7365
< 		if (update_cfs_rq_load_avg(cfs_rq_clock_pelt(cfs_rq), cfs_rq)) {
< 			update_tg_load_avg(cfs_rq);
---
> 		/* throttled entities do not contribute to load */
> 		if (throttled_hierarchy(cfs_rq))
> 			continue;
8197,8199c7367,7368
< 			if (cfs_rq == &rq->cfs)
< 				decayed = true;
< 		}
---
> 		if (update_cfs_rq_load_avg(cfs_rq_clock_task(cfs_rq), cfs_rq))
> 			update_tg_load_avg(cfs_rq, 0);
8204c7373
< 			update_load_avg(cfs_rq_of(se), se, UPDATE_TG);
---
> 			update_load_avg(cfs_rq_of(se), se, 0);
8212,8215d7380
< 
< 		/* Don't need periodic decay once load/util_avg are null */
< 		if (cfs_rq_has_blocked(cfs_rq))
< 			*done = false;
8217,8218c7382
< 
< 	return decayed;
---
> 	rq_unlock_irqrestore(rq, &rf);
8236c7400
< 	WRITE_ONCE(cfs_rq->h_load_next, NULL);
---
> 	cfs_rq->h_load_next = NULL;
8239c7403
< 		WRITE_ONCE(cfs_rq->h_load_next, se);
---
> 		cfs_rq->h_load_next = se;
8249c7413
< 	while ((se = READ_ONCE(cfs_rq->h_load_next)) != NULL) {
---
> 	while ((se = cfs_rq->h_load_next) != NULL) {
8268c7432
< static bool __update_blocked_fair(struct rq *rq, bool *done)
---
> static inline void update_blocked_averages(int cpu)
8269a7434
> 	struct rq *rq = cpu_rq(cpu);
8271,8275c7436
< 	bool decayed;
< 
< 	decayed = update_cfs_rq_load_avg(cfs_rq_clock_pelt(cfs_rq), cfs_rq);
< 	if (cfs_rq_has_blocked(cfs_rq))
< 		*done = false;
---
> 	struct rq_flags rf;
8277c7438,7441
< 	return decayed;
---
> 	rq_lock_irqsave(rq, &rf);
> 	update_rq_clock(rq);
> 	update_cfs_rq_load_avg(cfs_rq_clock_task(cfs_rq), cfs_rq);
> 	rq_unlock_irqrestore(rq, &rf);
8286,8304d7449
< static void update_blocked_averages(int cpu)
< {
< 	bool decayed = false, done = true;
< 	struct rq *rq = cpu_rq(cpu);
< 	struct rq_flags rf;
< 
< 	rq_lock_irqsave(rq, &rf);
< 	update_blocked_load_tick(rq);
< 	update_rq_clock(rq);
< 
< 	decayed |= __update_blocked_others(rq, &done);
< 	decayed |= __update_blocked_fair(rq, &done);
< 
< 	update_blocked_load_status(rq, !done);
< 	if (decayed)
< 		cpufreq_update_util(rq, 0);
< 	rq_unlock_irqrestore(rq, &rf);
< }
< 
8306a7452,7457
> enum group_type {
> 	group_other = 0,
> 	group_imbalanced,
> 	group_overloaded,
> };
> 
8312a7464,7465
> 	unsigned long sum_weighted_load; /* Weighted load of group's tasks */
> 	unsigned long load_per_task;
8314,8317c7467,7468
< 	unsigned long group_util; /* Total utilization over the CPUs of the group */
< 	unsigned long group_runnable; /* Total runnable time over the CPUs of the group */
< 	unsigned int sum_nr_running; /* Nr of tasks running in the group */
< 	unsigned int sum_h_nr_running; /* Nr of CFS tasks running in the group */
---
> 	unsigned long group_util; /* Total utilization of the group */
> 	unsigned int sum_nr_running; /* Nr tasks running in the group */
8321,8322c7472
< 	unsigned int group_asym_packing; /* Tasks should be moved to preferred CPU */
< 	unsigned long group_misfit_task_load; /* A CPU has a task too big for its capacity */
---
> 	int group_no_capacity;
8335a7486
> 	unsigned long total_running;
8339d7489
< 	unsigned int prefer_sibling; /* tasks should go to sibling first */
8350,8352c7500,7501
< 	 * We must however set busiest_stat::group_type and
< 	 * busiest_stat::idle_cpus to the worst busiest group because
< 	 * update_sd_pick_busiest() reads these before assignment.
---
> 	 * We must however clear busiest_stat::avg_load because
> 	 * update_sd_pick_busiest() reads this before assignment.
8356a7506
> 		.total_running = 0UL,
8360,8361c7510,7512
< 			.idle_cpus = UINT_MAX,
< 			.group_type = group_has_spare,
---
> 			.avg_load = 0UL,
> 			.sum_nr_running = 0,
> 			.group_type = group_other,
8365a7517,7544
> /**
>  * get_sd_load_idx - Obtain the load index for a given sched domain.
>  * @sd: The sched_domain whose load_idx is to be obtained.
>  * @idle: The idle status of the CPU for whose sd load_idx is obtained.
>  *
>  * Return: The load index.
>  */
> static inline int get_sd_load_idx(struct sched_domain *sd,
> 					enum cpu_idle_type idle)
> {
> 	int load_idx;
> 
> 	switch (idle) {
> 	case CPU_NOT_IDLE:
> 		load_idx = sd->busy_idx;
> 		break;
> 
> 	case CPU_NEWLY_IDLE:
> 		load_idx = sd->newidle_idx;
> 		break;
> 	default:
> 		load_idx = sd->idle_idx;
> 		break;
> 	}
> 
> 	return load_idx;
> }
> 
8369,8371c7548,7549
< 	unsigned long max = arch_scale_cpu_capacity(cpu);
< 	unsigned long used, free;
< 	unsigned long irq;
---
> 	u64 total, used, age_stamp, avg;
> 	s64 delta;
8373c7551,7557
< 	irq = cpu_util_irq(rq);
---
> 	/*
> 	 * Since we're reading these variables without serialization make sure
> 	 * we read them once before doing sanity checks on them.
> 	 */
> 	age_stamp = READ_ONCE(rq->age_stamp);
> 	avg = READ_ONCE(rq->rt_avg);
> 	delta = __rq_clock_broken(rq) - age_stamp;
8375,8376c7559,7560
< 	if (unlikely(irq >= max))
< 		return 1;
---
> 	if (unlikely(delta < 0))
> 		delta = 0;
8378,8386c7562
< 	/*
< 	 * avg_rt.util_avg and avg_dl.util_avg track binary signals
< 	 * (running and not running) with weights 0 and 1024 respectively.
< 	 * avg_thermal.load_avg tracks thermal pressure and the weighted
< 	 * average uses the actual delta max capacity(load).
< 	 */
< 	used = READ_ONCE(rq->avg_rt.util_avg);
< 	used += READ_ONCE(rq->avg_dl.util_avg);
< 	used += thermal_load_avg(rq);
---
> 	total = sched_avg_period() + delta;
8388,8389c7564
< 	if (unlikely(used >= max))
< 		return 1;
---
> 	used = div_u64(avg, total);
8391c7566,7567
< 	free = max - used;
---
> 	if (likely(used < SCHED_CAPACITY_SCALE))
> 		return SCHED_CAPACITY_SCALE - used;
8393c7569
< 	return scale_irq_capacity(free, irq, max);
---
> 	return 1;
8398c7574
< 	unsigned long capacity = scale_rt_capacity(cpu);
---
> 	unsigned long capacity = arch_scale_cpu_capacity(sd, cpu);
8401c7577,7580
< 	cpu_rq(cpu)->cpu_capacity_orig = arch_scale_cpu_capacity(cpu);
---
> 	cpu_rq(cpu)->cpu_capacity_orig = capacity;
> 
> 	capacity *= scale_rt_capacity(cpu);
> 	capacity >>= SCHED_CAPACITY_SHIFT;
8407,8408d7585
< 	trace_sched_cpu_capacity_tp(cpu_rq(cpu));
< 
8411d7587
< 	sdg->sgc->max_capacity = capacity;
8418c7594
< 	unsigned long capacity, min_capacity, max_capacity;
---
> 	unsigned long capacity, min_capacity;
8432d7607
< 	max_capacity = 0;
8441c7616,7635
< 			unsigned long cpu_cap = capacity_of(cpu);
---
> 			struct sched_group_capacity *sgc;
> 			struct rq *rq = cpu_rq(cpu);
> 
> 			/*
> 			 * build_sched_domains() -> init_sched_groups_capacity()
> 			 * gets here before we've attached the domains to the
> 			 * runqueues.
> 			 *
> 			 * Use capacity_of(), which is set irrespective of domains
> 			 * in update_cpu_capacity().
> 			 *
> 			 * This avoids capacity from being 0 and
> 			 * causing divide-by-zero issues on boot.
> 			 */
> 			if (unlikely(!rq->sd)) {
> 				capacity += capacity_of(cpu);
> 			} else {
> 				sgc = rq->sd->groups->sgc;
> 				capacity += sgc->capacity;
> 			}
8443,8445c7637
< 			capacity += cpu_cap;
< 			min_capacity = min(cpu_cap, min_capacity);
< 			max_capacity = max(cpu_cap, max_capacity);
---
> 			min_capacity = min(capacity, min_capacity);
8459d7650
< 			max_capacity = max(sgc->max_capacity, max_capacity);
8466d7656
< 	sdg->sgc->max_capacity = max_capacity;
8482,8493d7671
<  * Check whether a rq has a misfit task and if it looks like we can actually
<  * help that task: we can migrate the task to a CPU of higher capacity, or
<  * the task's current CPU is heavily pressured.
<  */
< static inline int check_misfit_status(struct rq *rq, struct sched_domain *sd)
< {
< 	return rq->misfit_task_load &&
< 		(rq->cpu_capacity_orig < rq->rd->max_cpu_capacity ||
< 		 check_cpu_capacity(rq, sd));
< }
< 
< /*
8495c7673
<  * groups is inadequate due to ->cpus_ptr constraints.
---
>  * groups is inadequate due to ->cpus_allowed constraints.
8497,8498c7675,7676
<  * Imagine a situation of two groups of 4 CPUs each and 4 tasks each with a
<  * cpumask covering 1 CPU of the first group and 3 CPUs of the second group.
---
>  * Imagine a situation of two groups of 4 cpus each and 4 tasks each with a
>  * cpumask covering 1 cpu of the first group and 3 cpus of the second group.
8506c7684
<  * cpu 3 and leave one of the CPUs in the second group unused.
---
>  * cpu 3 and leave one of the cpus in the second group unused.
8540c7718
< group_has_capacity(unsigned int imbalance_pct, struct sg_lb_stats *sgs)
---
> group_has_capacity(struct lb_env *env, struct sg_lb_stats *sgs)
8545,8548d7722
< 	if ((sgs->group_capacity * imbalance_pct) <
< 			(sgs->group_runnable * 100))
< 		return false;
< 
8550c7724
< 			(sgs->group_util * imbalance_pct))
---
> 			(sgs->group_util * env->sd->imbalance_pct))
8565c7739
< group_is_overloaded(unsigned int imbalance_pct, struct sg_lb_stats *sgs)
---
> group_is_overloaded(struct lb_env *env, struct sg_lb_stats *sgs)
8571,8575c7745
< 			(sgs->group_util * imbalance_pct))
< 		return true;
< 
< 	if ((sgs->group_capacity * imbalance_pct) <
< 			(sgs->group_runnable * 100))
---
> 			(sgs->group_util * env->sd->imbalance_pct))
8580a7751,7761
> /*
>  * group_smaller_cpu_capacity: Returns true if sched_group sg has smaller
>  * per-CPU capacity than sched_group ref.
>  */
> static inline bool
> group_smaller_cpu_capacity(struct sched_group *sg, struct sched_group *ref)
> {
> 	return sg->sgc->min_capacity * capacity_margin <
> 						ref->sgc->min_capacity * 1024;
> }
> 
8582,8583c7763
< group_type group_classify(unsigned int imbalance_pct,
< 			  struct sched_group *group,
---
> group_type group_classify(struct sched_group *group,
8586c7766
< 	if (group_is_overloaded(imbalance_pct, sgs))
---
> 	if (sgs->group_no_capacity)
8592,8601c7772
< 	if (sgs->group_asym_packing)
< 		return group_asym_packing;
< 
< 	if (sgs->group_misfit_task_load)
< 		return group_misfit_task;
< 
< 	if (!group_has_capacity(imbalance_pct, sgs))
< 		return group_fully_busy;
< 
< 	return group_has_spare;
---
> 	return group_other;
8607a7779,7780
>  * @load_idx: Load index of sched_domain of this_cpu for load calc.
>  * @local_group: Does group contain this_cpu.
8609c7782
<  * @sg_status: Holds flag indicating the status of the sched_group
---
>  * @overload: Indicate more than one runnable task for any CPU.
8612,8614c7785,7787
< 				      struct sched_group *group,
< 				      struct sg_lb_stats *sgs,
< 				      int *sg_status)
---
> 			struct sched_group *group, int load_idx,
> 			int local_group, struct sg_lb_stats *sgs,
> 			bool *overload)
8616c7789,7790
< 	int i, nr_running, local_group;
---
> 	unsigned long load;
> 	int i, nr_running;
8620,8621d7793
< 	local_group = cpumask_test_cpu(env->dst_cpu, sched_group_span(group));
< 
8625c7797,7803
< 		sgs->group_load += cpu_load(rq);
---
> 		/* Bias balancing toward cpus of our domain */
> 		if (local_group)
> 			load = target_load(i, load_idx);
> 		else
> 			load = source_load(i, load_idx);
> 
> 		sgs->group_load += load;
8627,8628c7805
< 		sgs->group_runnable += cpu_runnable(rq);
< 		sgs->sum_h_nr_running += rq->cfs.h_nr_running;
---
> 		sgs->sum_nr_running += rq->cfs.h_nr_running;
8631,8632d7807
< 		sgs->sum_nr_running += nr_running;
< 
8634,8637c7809
< 			*sg_status |= SG_OVERLOAD;
< 
< 		if (cpu_overutilized(i))
< 			*sg_status |= SG_OVERUTILIZED;
---
> 			*overload = true;
8642a7815
> 		sgs->sum_weighted_load += weighted_cpuload(rq);
8646c7819
< 		if (!nr_running && idle_cpu(i)) {
---
> 		if (!nr_running && idle_cpu(i))
8648,8668d7820
< 			/* Idle cpu can't have misfit task */
< 			continue;
< 		}
< 
< 		if (local_group)
< 			continue;
< 
< 		/* Check for a misfit task on the cpu */
< 		if (env->sd->flags & SD_ASYM_CPUCAPACITY &&
< 		    sgs->group_misfit_task_load < rq->misfit_task_load) {
< 			sgs->group_misfit_task_load = rq->misfit_task_load;
< 			*sg_status |= SG_OVERLOAD;
< 		}
< 	}
< 
< 	/* Check if dst CPU is idle and preferred to this group */
< 	if (env->sd->flags & SD_ASYM_PACKING &&
< 	    env->idle != CPU_NOT_IDLE &&
< 	    sgs->sum_h_nr_running &&
< 	    sched_asym_prefer(env->dst_cpu, group->asym_prefer_cpu)) {
< 		sgs->group_asym_packing = 1;
8670a7823
> 	/* Adjust by relative CPU capacity of the group */
8671a7825
> 	sgs->avg_load = (sgs->group_load*SCHED_CAPACITY_SCALE) / sgs->group_capacity;
8673c7827,7828
< 	sgs->group_weight = group->group_weight;
---
> 	if (sgs->sum_nr_running)
> 		sgs->load_per_task = sgs->sum_weighted_load / sgs->sum_nr_running;
8675c7830
< 	sgs->group_type = group_classify(env->sd->imbalance_pct, group, sgs);
---
> 	sgs->group_weight = group->group_weight;
8677,8680c7832,7833
< 	/* Computing avg_load makes sense only when group is overloaded */
< 	if (sgs->group_type == group_overloaded)
< 		sgs->avg_load = (sgs->group_load * SCHED_CAPACITY_SCALE) /
< 				sgs->group_capacity;
---
> 	sgs->group_no_capacity = group_is_overloaded(env, sgs);
> 	sgs->group_type = group_classify(group, sgs);
8703,8717d7855
< 	/* Make sure that there is at least one task to pull */
< 	if (!sgs->sum_h_nr_running)
< 		return false;
< 
< 	/*
< 	 * Don't try to pull misfit tasks we can't help.
< 	 * We can use max_capacity here as reduction in capacity on some
< 	 * CPUs in the group should either be possible to resolve
< 	 * internally or be covered by avg_load imbalance (eventually).
< 	 */
< 	if (sgs->group_type == group_misfit_task &&
< 	    (!capacity_greater(capacity_of(env->dst_cpu), sg->sgc->max_capacity) ||
< 	     sds->local_stat.group_type != group_has_spare))
< 		return false;
< 
8724,8740c7862
< 	/*
< 	 * The candidate and the current busiest group are the same type of
< 	 * group. Let check which one is the busiest according to the type.
< 	 */
< 
< 	switch (sgs->group_type) {
< 	case group_overloaded:
< 		/* Select the overloaded group with highest avg_load. */
< 		if (sgs->avg_load <= busiest->avg_load)
< 			return false;
< 		break;
< 
< 	case group_imbalanced:
< 		/*
< 		 * Select the 1st imbalanced group as we don't have any way to
< 		 * choose one more than another.
< 		 */
---
> 	if (sgs->avg_load <= busiest->avg_load)
8743,8756c7865,7866
< 	case group_asym_packing:
< 		/* Prefer to move from lowest priority CPU's work */
< 		if (sched_asym_prefer(sg->asym_prefer_cpu, sds->busiest->asym_prefer_cpu))
< 			return false;
< 		break;
< 
< 	case group_misfit_task:
< 		/*
< 		 * If we have more than one misfit sg go with the biggest
< 		 * misfit.
< 		 */
< 		if (sgs->group_misfit_task_load < busiest->group_misfit_task_load)
< 			return false;
< 		break;
---
> 	if (!(env->sd->flags & SD_ASYM_CPUCAPACITY))
> 		goto asym_packing;
8758,8785c7868,7876
< 	case group_fully_busy:
< 		/*
< 		 * Select the fully busy group with highest avg_load. In
< 		 * theory, there is no need to pull task from such kind of
< 		 * group because tasks have all compute capacity that they need
< 		 * but we can still improve the overall throughput by reducing
< 		 * contention when accessing shared HW resources.
< 		 *
< 		 * XXX for now avg_load is not computed and always 0 so we
< 		 * select the 1st one.
< 		 */
< 		if (sgs->avg_load <= busiest->avg_load)
< 			return false;
< 		break;
< 
< 	case group_has_spare:
< 		/*
< 		 * Select not overloaded group with lowest number of idle cpus
< 		 * and highest number of running tasks. We could also compare
< 		 * the spare capacity which is more stable but it can end up
< 		 * that the group has less spare capacity but finally more idle
< 		 * CPUs which means less opportunity to pull tasks.
< 		 */
< 		if (sgs->idle_cpus > busiest->idle_cpus)
< 			return false;
< 		else if ((sgs->idle_cpus == busiest->idle_cpus) &&
< 			 (sgs->sum_nr_running <= busiest->sum_nr_running))
< 			return false;
---
> 	/*
> 	 * Candidate sg has no more than one task per CPU and
> 	 * has higher per-CPU capacity. Migrating tasks to less
> 	 * capable CPUs may harm throughput. Maximize throughput,
> 	 * power/energy consequences are not considered.
> 	 */
> 	if (sgs->sum_nr_running <= sgs->group_weight &&
> 	    group_smaller_cpu_capacity(sds->local, sg))
> 		return false;
8787,8788c7878,7881
< 		break;
< 	}
---
> asym_packing:
> 	/* This is the busiest node in its class. */
> 	if (!(env->sd->flags & SD_ASYM_PACKING))
> 		return true;
8789a7883,7885
> 	/* No ASYM_PACKING if target cpu is already busy */
> 	if (env->idle == CPU_NOT_IDLE)
> 		return true;
8791,8799c7887,7900
< 	 * Candidate sg has no more than one task per CPU and has higher
< 	 * per-CPU capacity. Migrating tasks to less capable CPUs may harm
< 	 * throughput. Maximize throughput, power/energy consequences are not
< 	 * considered.
< 	 */
< 	if ((env->sd->flags & SD_ASYM_CPUCAPACITY) &&
< 	    (sgs->group_type <= group_fully_busy) &&
< 	    (capacity_greater(sg->sgc->min_capacity, capacity_of(env->dst_cpu))))
< 		return false;
---
> 	 * ASYM_PACKING needs to move all the work to the highest
> 	 * prority CPUs in the group, therefore mark all groups
> 	 * of lower priority than ourself as busy.
> 	 */
> 	if (sgs->sum_nr_running &&
> 	    sched_asym_prefer(env->dst_cpu, sg->asym_prefer_cpu)) {
> 		if (!sds->busiest)
> 			return true;
> 
> 		/* Prefer to move from lowest priority cpu's work */
> 		if (sched_asym_prefer(sds->busiest->asym_prefer_cpu,
> 				      sg->asym_prefer_cpu))
> 			return true;
> 	}
8801c7902
< 	return true;
---
> 	return false;
8807c7908
< 	if (sgs->sum_h_nr_running > sgs->nr_numa_running)
---
> 	if (sgs->sum_nr_running > sgs->nr_numa_running)
8809c7910
< 	if (sgs->sum_h_nr_running > sgs->nr_preferred_running)
---
> 	if (sgs->sum_nr_running > sgs->nr_preferred_running)
8834,9154d7934
< 
< struct sg_lb_stats;
< 
< /*
<  * task_running_on_cpu - return 1 if @p is running on @cpu.
<  */
< 
< static unsigned int task_running_on_cpu(int cpu, struct task_struct *p)
< {
< 	/* Task has no contribution or is new */
< 	if (cpu != task_cpu(p) || !READ_ONCE(p->se.avg.last_update_time))
< 		return 0;
< 
< 	if (task_on_rq_queued(p))
< 		return 1;
< 
< 	return 0;
< }
< 
< /**
<  * idle_cpu_without - would a given CPU be idle without p ?
<  * @cpu: the processor on which idleness is tested.
<  * @p: task which should be ignored.
<  *
<  * Return: 1 if the CPU would be idle. 0 otherwise.
<  */
< static int idle_cpu_without(int cpu, struct task_struct *p)
< {
< 	struct rq *rq = cpu_rq(cpu);
< 
< 	if (rq->curr != rq->idle && rq->curr != p)
< 		return 0;
< 
< 	/*
< 	 * rq->nr_running can't be used but an updated version without the
< 	 * impact of p on cpu must be used instead. The updated nr_running
< 	 * be computed and tested before calling idle_cpu_without().
< 	 */
< 
< #ifdef CONFIG_SMP
< 	if (rq->ttwu_pending)
< 		return 0;
< #endif
< 
< 	return 1;
< }
< 
< /*
<  * update_sg_wakeup_stats - Update sched_group's statistics for wakeup.
<  * @sd: The sched_domain level to look for idlest group.
<  * @group: sched_group whose statistics are to be updated.
<  * @sgs: variable to hold the statistics for this group.
<  * @p: The task for which we look for the idlest group/CPU.
<  */
< static inline void update_sg_wakeup_stats(struct sched_domain *sd,
< 					  struct sched_group *group,
< 					  struct sg_lb_stats *sgs,
< 					  struct task_struct *p)
< {
< 	int i, nr_running;
< 
< 	memset(sgs, 0, sizeof(*sgs));
< 
< 	for_each_cpu(i, sched_group_span(group)) {
< 		struct rq *rq = cpu_rq(i);
< 		unsigned int local;
< 
< 		sgs->group_load += cpu_load_without(rq, p);
< 		sgs->group_util += cpu_util_without(i, p);
< 		sgs->group_runnable += cpu_runnable_without(rq, p);
< 		local = task_running_on_cpu(i, p);
< 		sgs->sum_h_nr_running += rq->cfs.h_nr_running - local;
< 
< 		nr_running = rq->nr_running - local;
< 		sgs->sum_nr_running += nr_running;
< 
< 		/*
< 		 * No need to call idle_cpu_without() if nr_running is not 0
< 		 */
< 		if (!nr_running && idle_cpu_without(i, p))
< 			sgs->idle_cpus++;
< 
< 	}
< 
< 	/* Check if task fits in the group */
< 	if (sd->flags & SD_ASYM_CPUCAPACITY &&
< 	    !task_fits_capacity(p, group->sgc->max_capacity)) {
< 		sgs->group_misfit_task_load = 1;
< 	}
< 
< 	sgs->group_capacity = group->sgc->capacity;
< 
< 	sgs->group_weight = group->group_weight;
< 
< 	sgs->group_type = group_classify(sd->imbalance_pct, group, sgs);
< 
< 	/*
< 	 * Computing avg_load makes sense only when group is fully busy or
< 	 * overloaded
< 	 */
< 	if (sgs->group_type == group_fully_busy ||
< 		sgs->group_type == group_overloaded)
< 		sgs->avg_load = (sgs->group_load * SCHED_CAPACITY_SCALE) /
< 				sgs->group_capacity;
< }
< 
< static bool update_pick_idlest(struct sched_group *idlest,
< 			       struct sg_lb_stats *idlest_sgs,
< 			       struct sched_group *group,
< 			       struct sg_lb_stats *sgs)
< {
< 	if (sgs->group_type < idlest_sgs->group_type)
< 		return true;
< 
< 	if (sgs->group_type > idlest_sgs->group_type)
< 		return false;
< 
< 	/*
< 	 * The candidate and the current idlest group are the same type of
< 	 * group. Let check which one is the idlest according to the type.
< 	 */
< 
< 	switch (sgs->group_type) {
< 	case group_overloaded:
< 	case group_fully_busy:
< 		/* Select the group with lowest avg_load. */
< 		if (idlest_sgs->avg_load <= sgs->avg_load)
< 			return false;
< 		break;
< 
< 	case group_imbalanced:
< 	case group_asym_packing:
< 		/* Those types are not used in the slow wakeup path */
< 		return false;
< 
< 	case group_misfit_task:
< 		/* Select group with the highest max capacity */
< 		if (idlest->sgc->max_capacity >= group->sgc->max_capacity)
< 			return false;
< 		break;
< 
< 	case group_has_spare:
< 		/* Select group with most idle CPUs */
< 		if (idlest_sgs->idle_cpus > sgs->idle_cpus)
< 			return false;
< 
< 		/* Select group with lowest group_util */
< 		if (idlest_sgs->idle_cpus == sgs->idle_cpus &&
< 			idlest_sgs->group_util <= sgs->group_util)
< 			return false;
< 
< 		break;
< 	}
< 
< 	return true;
< }
< 
< /*
<  * Allow a NUMA imbalance if busy CPUs is less than 25% of the domain.
<  * This is an approximation as the number of running tasks may not be
<  * related to the number of busy CPUs due to sched_setaffinity.
<  */
< static inline bool allow_numa_imbalance(int dst_running, int dst_weight)
< {
< 	return (dst_running < (dst_weight >> 2));
< }
< 
< /*
<  * find_idlest_group() finds and returns the least busy CPU group within the
<  * domain.
<  *
<  * Assumes p is allowed on at least one CPU in sd.
<  */
< static struct sched_group *
< find_idlest_group(struct sched_domain *sd, struct task_struct *p, int this_cpu)
< {
< 	struct sched_group *idlest = NULL, *local = NULL, *group = sd->groups;
< 	struct sg_lb_stats local_sgs, tmp_sgs;
< 	struct sg_lb_stats *sgs;
< 	unsigned long imbalance;
< 	struct sg_lb_stats idlest_sgs = {
< 			.avg_load = UINT_MAX,
< 			.group_type = group_overloaded,
< 	};
< 
< 	do {
< 		int local_group;
< 
< 		/* Skip over this group if it has no CPUs allowed */
< 		if (!cpumask_intersects(sched_group_span(group),
< 					p->cpus_ptr))
< 			continue;
< 
< 		/* Skip over this group if no cookie matched */
< 		if (!sched_group_cookie_match(cpu_rq(this_cpu), p, group))
< 			continue;
< 
< 		local_group = cpumask_test_cpu(this_cpu,
< 					       sched_group_span(group));
< 
< 		if (local_group) {
< 			sgs = &local_sgs;
< 			local = group;
< 		} else {
< 			sgs = &tmp_sgs;
< 		}
< 
< 		update_sg_wakeup_stats(sd, group, sgs, p);
< 
< 		if (!local_group && update_pick_idlest(idlest, &idlest_sgs, group, sgs)) {
< 			idlest = group;
< 			idlest_sgs = *sgs;
< 		}
< 
< 	} while (group = group->next, group != sd->groups);
< 
< 
< 	/* There is no idlest group to push tasks to */
< 	if (!idlest)
< 		return NULL;
< 
< 	/* The local group has been skipped because of CPU affinity */
< 	if (!local)
< 		return idlest;
< 
< 	/*
< 	 * If the local group is idler than the selected idlest group
< 	 * don't try and push the task.
< 	 */
< 	if (local_sgs.group_type < idlest_sgs.group_type)
< 		return NULL;
< 
< 	/*
< 	 * If the local group is busier than the selected idlest group
< 	 * try and push the task.
< 	 */
< 	if (local_sgs.group_type > idlest_sgs.group_type)
< 		return idlest;
< 
< 	switch (local_sgs.group_type) {
< 	case group_overloaded:
< 	case group_fully_busy:
< 
< 		/* Calculate allowed imbalance based on load */
< 		imbalance = scale_load_down(NICE_0_LOAD) *
< 				(sd->imbalance_pct-100) / 100;
< 
< 		/*
< 		 * When comparing groups across NUMA domains, it's possible for
< 		 * the local domain to be very lightly loaded relative to the
< 		 * remote domains but "imbalance" skews the comparison making
< 		 * remote CPUs look much more favourable. When considering
< 		 * cross-domain, add imbalance to the load on the remote node
< 		 * and consider staying local.
< 		 */
< 
< 		if ((sd->flags & SD_NUMA) &&
< 		    ((idlest_sgs.avg_load + imbalance) >= local_sgs.avg_load))
< 			return NULL;
< 
< 		/*
< 		 * If the local group is less loaded than the selected
< 		 * idlest group don't try and push any tasks.
< 		 */
< 		if (idlest_sgs.avg_load >= (local_sgs.avg_load + imbalance))
< 			return NULL;
< 
< 		if (100 * local_sgs.avg_load <= sd->imbalance_pct * idlest_sgs.avg_load)
< 			return NULL;
< 		break;
< 
< 	case group_imbalanced:
< 	case group_asym_packing:
< 		/* Those type are not used in the slow wakeup path */
< 		return NULL;
< 
< 	case group_misfit_task:
< 		/* Select group with the highest max capacity */
< 		if (local->sgc->max_capacity >= idlest->sgc->max_capacity)
< 			return NULL;
< 		break;
< 
< 	case group_has_spare:
< 		if (sd->flags & SD_NUMA) {
< #ifdef CONFIG_NUMA_BALANCING
< 			int idlest_cpu;
< 			/*
< 			 * If there is spare capacity at NUMA, try to select
< 			 * the preferred node
< 			 */
< 			if (cpu_to_node(this_cpu) == p->numa_preferred_nid)
< 				return NULL;
< 
< 			idlest_cpu = cpumask_first(sched_group_span(idlest));
< 			if (cpu_to_node(idlest_cpu) == p->numa_preferred_nid)
< 				return idlest;
< #endif
< 			/*
< 			 * Otherwise, keep the task on this node to stay close
< 			 * its wakeup source and improve locality. If there is
< 			 * a real need of migration, periodic load balance will
< 			 * take care of it.
< 			 */
< 			if (allow_numa_imbalance(local_sgs.sum_nr_running, sd->span_weight))
< 				return NULL;
< 		}
< 
< 		/*
< 		 * Select group with highest number of idle CPUs. We could also
< 		 * compare the utilization which is more stable but it can end
< 		 * up that the group has less spare capacity but finally more
< 		 * idle CPUs which means more opportunity to run task.
< 		 */
< 		if (local_sgs.idle_cpus >= idlest_sgs.idle_cpus)
< 			return NULL;
< 		break;
< 	}
< 
< 	return idlest;
< }
< 
9160d7939
< 
9167c7946,7952
< 	int sg_status = 0;
---
> 	int load_idx, prefer_sibling = 0;
> 	bool overload = false;
> 
> 	if (child && child->flags & SD_PREFER_SIBLING)
> 		prefer_sibling = 1;
> 
> 	load_idx = get_sd_load_idx(env->sd, env->idle);
9183c7968,7969
< 		update_sg_lb_stats(env, sg, sgs, &sg_status);
---
> 		update_sg_lb_stats(env, sg, load_idx, local_group, sgs,
> 						&overload);
9187a7974,7989
> 		/*
> 		 * In case the child domain prefers tasks go to siblings
> 		 * first, lower the sg capacity so that we'll try
> 		 * and move all the excess tasks away. We lower the capacity
> 		 * of a group only if the local group has the capacity to fit
> 		 * these excess tasks. The extra check prevents the case where
> 		 * you always pull from the heaviest group when it is already
> 		 * under-utilized (possible with a large weight task outweighs
> 		 * the tasks on the system).
> 		 */
> 		if (prefer_sibling && sds->local &&
> 		    group_has_capacity(env, local) &&
> 		    (sgs->sum_nr_running > local->sum_nr_running + 1)) {
> 			sgs->group_no_capacity = 1;
> 			sgs->group_type = group_classify(sg, sgs);
> 		}
9195a7998
> 		sds->total_running += sgs->sum_nr_running;
9202,9205d8004
< 	/* Tag domain that child domain prefers tasks go to siblings first */
< 	sds->prefer_sibling = child && child->flags & SD_PREFER_SIBLING;
< 
< 
9210,9211d8008
< 		struct root_domain *rd = env->dst_rq->rd;
< 
9213,9222c8010,8011
< 		WRITE_ONCE(rd->overload, sg_status & SG_OVERLOAD);
< 
< 		/* Update over-utilization (tipping point, U >= 0) indicator */
< 		WRITE_ONCE(rd->overutilized, sg_status & SG_OVERUTILIZED);
< 		trace_sched_overutilized_tp(rd, sg_status & SG_OVERUTILIZED);
< 	} else if (sg_status & SG_OVERUTILIZED) {
< 		struct root_domain *rd = env->dst_rq->rd;
< 
< 		WRITE_ONCE(rd->overutilized, SG_OVERUTILIZED);
< 		trace_sched_overutilized_tp(rd, SG_OVERUTILIZED);
---
> 		if (env->dst_rq->rd->overload != overload)
> 			env->dst_rq->rd->overload = overload;
9226c8015,8060
< #define NUMA_IMBALANCE_MIN 2
---
> /**
>  * check_asym_packing - Check to see if the group is packed into the
>  *			sched domain.
>  *
>  * This is primarily intended to used at the sibling level.  Some
>  * cores like POWER7 prefer to use lower numbered SMT threads.  In the
>  * case of POWER7, it can move to lower SMT modes only when higher
>  * threads are idle.  When in lower SMT modes, the threads will
>  * perform better since they share less core resources.  Hence when we
>  * have idle threads, we want them to be the higher ones.
>  *
>  * This packing function is run on idle threads.  It checks to see if
>  * the busiest CPU in this domain (core in the P7 case) has a higher
>  * CPU number than the packing function is being run on.  Here we are
>  * assuming lower CPU number will be equivalent to lower a SMT thread
>  * number.
>  *
>  * Return: 1 when packing is required and a task should be moved to
>  * this CPU.  The amount of the imbalance is returned in env->imbalance.
>  *
>  * @env: The load balancing environment.
>  * @sds: Statistics of the sched_domain which is to be packed
>  */
> static int check_asym_packing(struct lb_env *env, struct sd_lb_stats *sds)
> {
> 	int busiest_cpu;
> 
> 	if (!(env->sd->flags & SD_ASYM_PACKING))
> 		return 0;
> 
> 	if (env->idle == CPU_NOT_IDLE)
> 		return 0;
> 
> 	if (!sds->busiest)
> 		return 0;
> 
> 	busiest_cpu = sds->busiest->asym_prefer_cpu;
> 	if (sched_asym_prefer(busiest_cpu, env->dst_cpu))
> 		return 0;
> 
> 	env->imbalance = DIV_ROUND_CLOSEST(
> 		sds->busiest_stat.avg_load * sds->busiest_stat.group_capacity,
> 		SCHED_CAPACITY_SCALE);
> 
> 	return 1;
> }
9228,9229c8062,8070
< static inline long adjust_numa_imbalance(int imbalance,
< 				int dst_running, int dst_weight)
---
> /**
>  * fix_small_imbalance - Calculate the minor imbalance that exists
>  *			amongst the groups of a sched_domain, during
>  *			load balancing.
>  * @env: The load balancing environment.
>  * @sds: Statistics of the sched_domain whose imbalance is to be calculated.
>  */
> static inline
> void fix_small_imbalance(struct lb_env *env, struct sd_lb_stats *sds)
9231,9232c8072,8093
< 	if (!allow_numa_imbalance(dst_running, dst_weight))
< 		return imbalance;
---
> 	unsigned long tmp, capa_now = 0, capa_move = 0;
> 	unsigned int imbn = 2;
> 	unsigned long scaled_busy_load_per_task;
> 	struct sg_lb_stats *local, *busiest;
> 
> 	local = &sds->local_stat;
> 	busiest = &sds->busiest_stat;
> 
> 	if (!local->sum_nr_running)
> 		local->load_per_task = cpu_avg_load_per_task(env->dst_cpu);
> 	else if (busiest->load_per_task > local->load_per_task)
> 		imbn = 1;
> 
> 	scaled_busy_load_per_task =
> 		(busiest->load_per_task * SCHED_CAPACITY_SCALE) /
> 		busiest->group_capacity;
> 
> 	if (busiest->avg_load + scaled_busy_load_per_task >=
> 	    local->avg_load + (scaled_busy_load_per_task * imbn)) {
> 		env->imbalance = busiest->load_per_task;
> 		return;
> 	}
9235,9236c8096,8098
< 	 * Allow a small imbalance based on a simple pair of communicating
< 	 * tasks that remain local when the destination is lightly loaded.
---
> 	 * OK, we don't have enough imbalance to justify moving tasks,
> 	 * however we may be able to increase total CPU capacity used by
> 	 * moving them.
9238,9239d8099
< 	if (imbalance <= NUMA_IMBALANCE_MIN)
< 		return 0;
9241c8101,8129
< 	return imbalance;
---
> 	capa_now += busiest->group_capacity *
> 			min(busiest->load_per_task, busiest->avg_load);
> 	capa_now += local->group_capacity *
> 			min(local->load_per_task, local->avg_load);
> 	capa_now /= SCHED_CAPACITY_SCALE;
> 
> 	/* Amount of load we'd subtract */
> 	if (busiest->avg_load > scaled_busy_load_per_task) {
> 		capa_move += busiest->group_capacity *
> 			    min(busiest->load_per_task,
> 				busiest->avg_load - scaled_busy_load_per_task);
> 	}
> 
> 	/* Amount of load we'd add */
> 	if (busiest->avg_load * busiest->group_capacity <
> 	    busiest->load_per_task * SCHED_CAPACITY_SCALE) {
> 		tmp = (busiest->avg_load * busiest->group_capacity) /
> 		      local->group_capacity;
> 	} else {
> 		tmp = (busiest->load_per_task * SCHED_CAPACITY_SCALE) /
> 		      local->group_capacity;
> 	}
> 	capa_move += local->group_capacity *
> 		    min(local->load_per_task, local->avg_load + tmp);
> 	capa_move /= SCHED_CAPACITY_SCALE;
> 
> 	/* Move if we gain throughput */
> 	if (capa_move > capa_now)
> 		env->imbalance = busiest->load_per_task;
9251a8140
> 	unsigned long max_pull, load_above_capacity = ~0UL;
9257,9273d8145
< 	if (busiest->group_type == group_misfit_task) {
< 		/* Set imbalance to allow misfit tasks to be balanced. */
< 		env->migration_type = migrate_misfit;
< 		env->imbalance = 1;
< 		return;
< 	}
< 
< 	if (busiest->group_type == group_asym_packing) {
< 		/*
< 		 * In case of asym capacity, we will try to migrate all load to
< 		 * the preferred CPU.
< 		 */
< 		env->migration_type = migrate_task;
< 		env->imbalance = busiest->sum_h_nr_running;
< 		return;
< 	}
< 
9277,9279c8149
< 		 * to ensure CPU-load equilibrium, try to move any task to fix
< 		 * the imbalance. The next load balance will take care of
< 		 * balancing back the system.
---
> 		 * to ensure cpu-load equilibrium, look at wider averages. XXX
9281,9283c8151,8152
< 		env->migration_type = migrate_task;
< 		env->imbalance = 1;
< 		return;
---
> 		busiest->load_per_task =
> 			min(busiest->load_per_task, sds->avg_load);
9287,9288c8156,8159
< 	 * Try to use spare capacity of local group without overloading it or
< 	 * emptying busiest.
---
> 	 * Avg load of busiest sg can be less and avg load of local sg can
> 	 * be greater than avg load across all sgs of sd because avg load
> 	 * factors in sg capacity and sgs with smaller group_type are
> 	 * skipped when updating the busiest sg:
9290,9346c8161,8164
< 	if (local->group_type == group_has_spare) {
< 		if ((busiest->group_type > group_fully_busy) &&
< 		    !(env->sd->flags & SD_SHARE_PKG_RESOURCES)) {
< 			/*
< 			 * If busiest is overloaded, try to fill spare
< 			 * capacity. This might end up creating spare capacity
< 			 * in busiest or busiest still being overloaded but
< 			 * there is no simple way to directly compute the
< 			 * amount of load to migrate in order to balance the
< 			 * system.
< 			 */
< 			env->migration_type = migrate_util;
< 			env->imbalance = max(local->group_capacity, local->group_util) -
< 					 local->group_util;
< 
< 			/*
< 			 * In some cases, the group's utilization is max or even
< 			 * higher than capacity because of migrations but the
< 			 * local CPU is (newly) idle. There is at least one
< 			 * waiting task in this overloaded busiest group. Let's
< 			 * try to pull it.
< 			 */
< 			if (env->idle != CPU_NOT_IDLE && env->imbalance == 0) {
< 				env->migration_type = migrate_task;
< 				env->imbalance = 1;
< 			}
< 
< 			return;
< 		}
< 
< 		if (busiest->group_weight == 1 || sds->prefer_sibling) {
< 			unsigned int nr_diff = busiest->sum_nr_running;
< 			/*
< 			 * When prefer sibling, evenly spread running tasks on
< 			 * groups.
< 			 */
< 			env->migration_type = migrate_task;
< 			lsub_positive(&nr_diff, local->sum_nr_running);
< 			env->imbalance = nr_diff >> 1;
< 		} else {
< 
< 			/*
< 			 * If there is no overload, we just want to even the number of
< 			 * idle cpus.
< 			 */
< 			env->migration_type = migrate_task;
< 			env->imbalance = max_t(long, 0, (local->idle_cpus -
< 						 busiest->idle_cpus) >> 1);
< 		}
< 
< 		/* Consider allowing a small imbalance between NUMA groups */
< 		if (env->sd->flags & SD_NUMA) {
< 			env->imbalance = adjust_numa_imbalance(env->imbalance,
< 				busiest->sum_nr_running, busiest->group_weight);
< 		}
< 
< 		return;
---
> 	if (busiest->avg_load <= sds->avg_load ||
> 	    local->avg_load >= sds->avg_load) {
> 		env->imbalance = 0;
> 		return fix_small_imbalance(env, sds);
9350,9351c8168
< 	 * Local is fully busy but has to take more load to relieve the
< 	 * busiest group
---
> 	 * If there aren't any idle cpus, avoid creating some.
9353,9371c8170,8178
< 	if (local->group_type < group_overloaded) {
< 		/*
< 		 * Local will become overloaded so the avg_load metrics are
< 		 * finally needed.
< 		 */
< 
< 		local->avg_load = (local->group_load * SCHED_CAPACITY_SCALE) /
< 				  local->group_capacity;
< 
< 		sds->avg_load = (sds->total_load * SCHED_CAPACITY_SCALE) /
< 				sds->total_capacity;
< 		/*
< 		 * If the local group is more loaded than the selected
< 		 * busiest group don't try to pull any tasks.
< 		 */
< 		if (local->avg_load >= busiest->avg_load) {
< 			env->imbalance = 0;
< 			return;
< 		}
---
> 	if (busiest->group_type == group_overloaded &&
> 	    local->group_type   == group_overloaded) {
> 		load_above_capacity = busiest->sum_nr_running * SCHED_CAPACITY_SCALE;
> 		if (load_above_capacity > busiest->group_capacity) {
> 			load_above_capacity -= busiest->group_capacity;
> 			load_above_capacity *= scale_load_down(NICE_0_LOAD);
> 			load_above_capacity /= busiest->group_capacity;
> 		} else
> 			load_above_capacity = ~0UL;
9375,9380c8182,8186
< 	 * Both group are or will become overloaded and we're trying to get all
< 	 * the CPUs to the average_load, so we don't want to push ourselves
< 	 * above the average load, nor do we wish to reduce the max loaded CPU
< 	 * below the average load. At the same time, we also don't want to
< 	 * reduce the group load below the group capacity. Thus we look for
< 	 * the minimum possible imbalance.
---
> 	 * We're trying to get all the cpus to the average_load, so we don't
> 	 * want to push ourselves above the average load, nor do we wish to
> 	 * reduce the max loaded cpu below the average load. At the same time,
> 	 * we also don't want to reduce the group load below the group
> 	 * capacity. Thus we look for the minimum possible imbalance.
9382c8188,8190
< 	env->migration_type = migrate_load;
---
> 	max_pull = min(busiest->avg_load - sds->avg_load, load_above_capacity);
> 
> 	/* How much load to actually move to equalise the imbalance */
9384c8192
< 		(busiest->avg_load - sds->avg_load) * busiest->group_capacity,
---
> 		max_pull * busiest->group_capacity,
9386a8195,8203
> 
> 	/*
> 	 * if *imbalance is less than the average load per runnable task
> 	 * there is no guarantee that any tasks will be moved so we'll have
> 	 * a think about bumping its value to force at least one task to be
> 	 * moved
> 	 */
> 	if (env->imbalance < busiest->load_per_task)
> 		return fix_small_imbalance(env, sds);
9391,9410d8207
< /*
<  * Decision matrix according to the local and busiest group type:
<  *
<  * busiest \ local has_spare fully_busy misfit asym imbalanced overloaded
<  * has_spare        nr_idle   balanced   N/A    N/A  balanced   balanced
<  * fully_busy       nr_idle   nr_idle    N/A    N/A  balanced   balanced
<  * misfit_task      force     N/A        N/A    N/A  force      force
<  * asym_packing     force     force      N/A    N/A  force      force
<  * imbalanced       force     force      N/A    N/A  force      force
<  * overloaded       force     force      N/A    N/A  force      avg_load
<  *
<  * N/A :      Not Applicable because already filtered while updating
<  *            statistics.
<  * balanced : The system is balanced for these 2 groups.
<  * force :    Calculate the imbalance as load migration is probably needed.
<  * avg_load : Only if imbalance is significant enough.
<  * nr_idle :  dst_cpu is not busy and the number of idle CPUs is quite
<  *            different in groups.
<  */
< 
9415c8212
<  * Also calculates the amount of runnable load which should be moved
---
>  * Also calculates the amount of weighted load which should be moved
9430c8227
< 	 * Compute the various statistics relevant for load balancing at
---
> 	 * Compute the various statistics relavent for load balancing at
9434,9441d8230
< 
< 	if (sched_energy_enabled()) {
< 		struct root_domain *rd = env->dst_rq->rd;
< 
< 		if (rcu_dereference(rd->pd) && !READ_ONCE(rd->overutilized))
< 			goto out_balanced;
< 	}
< 
9444a8234,8237
> 	/* ASYM feature bypasses nice load balance check */
> 	if (check_asym_packing(env, &sds))
> 		return sds.busiest;
> 
9446c8239
< 	if (!sds.busiest)
---
> 	if (!sds.busiest || busiest->sum_nr_running == 0)
9449,9455c8242,8244
< 	/* Misfit tasks should be dealt with regardless of the avg load */
< 	if (busiest->group_type == group_misfit_task)
< 		goto force_balance;
< 
< 	/* ASYM feature bypasses nice load balance check */
< 	if (busiest->group_type == group_asym_packing)
< 		goto force_balance;
---
> 	/* XXX broken for overlapping NUMA groups */
> 	sds.avg_load = (SCHED_CAPACITY_SCALE * sds.total_load)
> 						/ sds.total_capacity;
9460c8249
< 	 * isn't true due to cpus_ptr constraints and the like.
---
> 	 * isn't true due to cpus_allowed constraints and the like.
9465a8255,8262
> 	 * When dst_cpu is idle, prevent SMP nice and/or asymmetric group
> 	 * capacities from resulting in underutilization due to avg_load.
> 	 */
> 	if (env->idle != CPU_NOT_IDLE && group_has_capacity(env, local) &&
> 	    busiest->group_no_capacity)
> 		goto force_balance;
> 
> 	/*
9469c8266
< 	if (local->group_type > busiest->group_type)
---
> 	if (local->avg_load >= busiest->avg_load)
9473,9474c8270,8271
< 	 * When groups are overloaded, use the avg_load to ensure fairness
< 	 * between tasks.
---
> 	 * Don't pull any tasks if this group is already above the domain
> 	 * average load.
9476,9486c8273,8274
< 	if (local->group_type == group_overloaded) {
< 		/*
< 		 * If the local group is more loaded than the selected
< 		 * busiest group don't try to pull any tasks.
< 		 */
< 		if (local->avg_load >= busiest->avg_load)
< 			goto out_balanced;
< 
< 		/* XXX broken for overlapping NUMA groups */
< 		sds.avg_load = (sds.total_load * SCHED_CAPACITY_SCALE) /
< 				sds.total_capacity;
---
> 	if (local->avg_load >= sds.avg_load)
> 		goto out_balanced;
9487a8276
> 	if (env->idle == CPU_IDLE) {
9489,9490c8278,8282
< 		 * Don't pull any tasks if this group is already above the
< 		 * domain average load.
---
> 		 * This cpu is idle. If the busiest group is not overloaded
> 		 * and there is no imbalance between this and busiest group
> 		 * wrt idle cpus, it is balanced. The imbalance becomes
> 		 * significant if the diff is greater than 1 otherwise we
> 		 * might end up to just move the imbalance on another group
9492c8284,8285
< 		if (local->avg_load >= sds.avg_load)
---
> 		if ((busiest->group_type != group_overloaded) &&
> 				(local->idle_cpus <= (busiest->idle_cpus + 1)))
9494c8287
< 
---
> 	} else {
9496,9497c8289,8290
< 		 * If the busiest group is more loaded, use imbalance_pct to be
< 		 * conservative.
---
> 		 * In the CPU_NEWLY_IDLE, CPU_NOT_IDLE cases, use
> 		 * imbalance_pct to be conservative.
9504,9537d8296
< 	/* Try to move all excess tasks to child's sibling domain */
< 	if (sds.prefer_sibling && local->group_type == group_has_spare &&
< 	    busiest->sum_nr_running > local->sum_nr_running + 1)
< 		goto force_balance;
< 
< 	if (busiest->group_type != group_overloaded) {
< 		if (env->idle == CPU_NOT_IDLE)
< 			/*
< 			 * If the busiest group is not overloaded (and as a
< 			 * result the local one too) but this CPU is already
< 			 * busy, let another idle CPU try to pull task.
< 			 */
< 			goto out_balanced;
< 
< 		if (busiest->group_weight > 1 &&
< 		    local->idle_cpus <= (busiest->idle_cpus + 1))
< 			/*
< 			 * If the busiest group is not overloaded
< 			 * and there is no imbalance between this and busiest
< 			 * group wrt idle CPUs, it is balanced. The imbalance
< 			 * becomes significant if the diff is greater than 1
< 			 * otherwise we might end up to just move the imbalance
< 			 * on another group. Of course this applies only if
< 			 * there is more than 1 CPU per group.
< 			 */
< 			goto out_balanced;
< 
< 		if (busiest->sum_h_nr_running == 1)
< 			/*
< 			 * busiest doesn't have any tasks waiting to run
< 			 */
< 			goto out_balanced;
< 	}
< 
9541c8300
< 	return env->imbalance ? sds.busiest : NULL;
---
> 	return sds.busiest;
9549c8308
<  * find_busiest_queue - find the busiest runqueue among the CPUs in the group.
---
>  * find_busiest_queue - find the busiest runqueue among the cpus in group.
9555,9556c8314
< 	unsigned long busiest_util = 0, busiest_load = 0, busiest_capacity = 1;
< 	unsigned int busiest_nr = 0;
---
> 	unsigned long busiest_load = 0, busiest_capacity = 1;
9560,9561c8318
< 		unsigned long capacity, load, util;
< 		unsigned int nr_running;
---
> 		unsigned long capacity, wl;
9589,9592d8345
< 		nr_running = rq->cfs.h_nr_running;
< 		if (!nr_running)
< 			continue;
< 
9595,9660c8348
< 		/*
< 		 * For ASYM_CPUCAPACITY domains, don't pick a CPU that could
< 		 * eventually lead to active_balancing high->low capacity.
< 		 * Higher per-CPU capacity is considered better than balancing
< 		 * average load.
< 		 */
< 		if (env->sd->flags & SD_ASYM_CPUCAPACITY &&
< 		    !capacity_greater(capacity_of(env->dst_cpu), capacity) &&
< 		    nr_running == 1)
< 			continue;
< 
< 		switch (env->migration_type) {
< 		case migrate_load:
< 			/*
< 			 * When comparing with load imbalance, use cpu_load()
< 			 * which is not scaled with the CPU capacity.
< 			 */
< 			load = cpu_load(rq);
< 
< 			if (nr_running == 1 && load > env->imbalance &&
< 			    !check_cpu_capacity(rq, env->sd))
< 				break;
< 
< 			/*
< 			 * For the load comparisons with the other CPUs,
< 			 * consider the cpu_load() scaled with the CPU
< 			 * capacity, so that the load can be moved away
< 			 * from the CPU that is potentially running at a
< 			 * lower capacity.
< 			 *
< 			 * Thus we're looking for max(load_i / capacity_i),
< 			 * crosswise multiplication to rid ourselves of the
< 			 * division works out to:
< 			 * load_i * capacity_j > load_j * capacity_i;
< 			 * where j is our previous maximum.
< 			 */
< 			if (load * busiest_capacity > busiest_load * capacity) {
< 				busiest_load = load;
< 				busiest_capacity = capacity;
< 				busiest = rq;
< 			}
< 			break;
< 
< 		case migrate_util:
< 			util = cpu_util(cpu_of(rq));
< 
< 			/*
< 			 * Don't try to pull utilization from a CPU with one
< 			 * running task. Whatever its utilization, we will fail
< 			 * detach the task.
< 			 */
< 			if (nr_running <= 1)
< 				continue;
< 
< 			if (busiest_util < util) {
< 				busiest_util = util;
< 				busiest = rq;
< 			}
< 			break;
< 
< 		case migrate_task:
< 			if (busiest_nr < nr_running) {
< 				busiest_nr = nr_running;
< 				busiest = rq;
< 			}
< 			break;
---
> 		wl = weighted_cpuload(rq);
9662,9670c8350,8353
< 		case migrate_misfit:
< 			/*
< 			 * For ASYM_CPUCAPACITY domains with misfit tasks we
< 			 * simply seek the "biggest" misfit task.
< 			 */
< 			if (rq->misfit_task_load > busiest_load) {
< 				busiest_load = rq->misfit_task_load;
< 				busiest = rq;
< 			}
---
> 		/*
> 		 * When comparing with imbalance, use weighted_cpuload()
> 		 * which is not scaled with the cpu capacity.
> 		 */
9672c8355,8357
< 			break;
---
> 		if (rq->nr_running == 1 && wl > env->imbalance &&
> 		    !check_cpu_capacity(rq, env->sd))
> 			continue;
9673a8359,8373
> 		/*
> 		 * For the load comparisons with the other cpu's, consider
> 		 * the weighted_cpuload() scaled with the cpu capacity, so
> 		 * that the load can be moved away from the cpu that is
> 		 * potentially running at a lower capacity.
> 		 *
> 		 * Thus we're looking for max(wl_i / capacity_i), crosswise
> 		 * multiplication to rid ourselves of the division works out
> 		 * to: wl_i * capacity_j > wl_j * capacity_i;  where j is
> 		 * our previous maximum.
> 		 */
> 		if (wl * busiest_capacity > busiest_load * capacity) {
> 			busiest_load = wl;
> 			busiest_capacity = capacity;
> 			busiest = rq;
9686,9714d8385
< static inline bool
< asym_active_balance(struct lb_env *env)
< {
< 	/*
< 	 * ASYM_PACKING needs to force migrate tasks from busy but
< 	 * lower priority CPUs in order to pack all tasks in the
< 	 * highest priority CPUs.
< 	 */
< 	return env->idle != CPU_NOT_IDLE && (env->sd->flags & SD_ASYM_PACKING) &&
< 	       sched_asym_prefer(env->dst_cpu, env->src_cpu);
< }
< 
< static inline bool
< imbalanced_active_balance(struct lb_env *env)
< {
< 	struct sched_domain *sd = env->sd;
< 
< 	/*
< 	 * The imbalanced case includes the case of pinned tasks preventing a fair
< 	 * distribution of the load on the system but also the even distribution of the
< 	 * threads on a system with spare capacity
< 	 */
< 	if ((env->migration_type == migrate_task) &&
< 	    (sd->nr_balance_failed > sd->cache_nice_tries+2))
< 		return 1;
< 
< 	return 0;
< }
< 
9719,9720c8390
< 	if (asym_active_balance(env))
< 		return 1;
---
> 	if (env->idle == CPU_NEWLY_IDLE) {
9722,9723c8392,8400
< 	if (imbalanced_active_balance(env))
< 		return 1;
---
> 		/*
> 		 * ASYM_PACKING needs to force migrate tasks from busy but
> 		 * lower priority CPUs in order to pack all tasks in the
> 		 * highest priority CPUs.
> 		 */
> 		if ((sd->flags & SD_ASYM_PACKING) &&
> 		    sched_asym_prefer(env->dst_cpu, env->src_cpu))
> 			return 1;
> 	}
9738,9741c8415
< 	if (env->migration_type == migrate_misfit)
< 		return 1;
< 
< 	return 0;
---
> 	return unlikely(sd->nr_balance_failed > sd->cache_nice_tries+2);
9749c8423
< 	int cpu;
---
> 	int cpu, balance_cpu = -1;
9759c8433
< 	 * In the newly idle case, we will allow all the CPUs
---
> 	 * In the newly idle case, we will allow all the cpu's
9765c8439
< 	/* Try to find first idle CPU */
---
> 	/* Try to find first idle cpu */
9770,9771c8444,8445
< 		/* Are we the first idle CPU? */
< 		return cpu == env->dst_cpu;
---
> 		balance_cpu = cpu;
> 		break;
9774,9775c8448,8455
< 	/* Are we the first CPU of this group ? */
< 	return group_balance_cpu(sg) == env->dst_cpu;
---
> 	if (balance_cpu == -1)
> 		balance_cpu = group_balance_cpu(sg);
> 
> 	/*
> 	 * First idle cpu or the first cpu(busiest) in this sched group
> 	 * is eligible for doing load balancing at this and above domains.
> 	 */
> 	return balance_cpu == env->dst_cpu;
9835,9836d8514
< 	/* Clear this flag as soon as we find a pullable task */
< 	env.flags |= LBF_ALL_PINNED;
9843a8522
> 		env.flags |= LBF_ALL_PINNED;
9882c8561
< 		 * iterate on same src_cpu is dependent on number of CPUs in our
---
> 		 * iterate on same src_cpu is dependent on number of cpus in our
9892c8571
< 		 * given_cpu) causing excess load to be moved to given_cpu.
---
> 		 * given_cpu) causing exceess load to be moved to given_cpu.
9899,9900c8578,8579
< 			/* Prevent to re-select dst_cpu via env's CPUs */
< 			__cpumask_clear_cpu(env.dst_cpu, env.cpus);
---
> 			/* Prevent to re-select dst_cpu via env's cpus */
> 			cpumask_clear_cpu(env.dst_cpu, env.cpus);
9927c8606
< 			__cpumask_clear_cpu(cpu_of(busiest), cpus);
---
> 			cpumask_clear_cpu(cpu_of(busiest), cpus);
9959c8638
< 			raw_spin_rq_lock_irqsave(busiest, flags);
---
> 			raw_spin_lock_irqsave(&busiest->lock, flags);
9961,9964c8640,8642
< 			/*
< 			 * Don't kick the active_load_balance_cpu_stop,
< 			 * if the curr task on busiest CPU can't be
< 			 * moved to this_cpu:
---
> 			/* don't kick the active_load_balance_cpu_stop,
> 			 * if the curr task on busiest cpu can't be
> 			 * moved to this_cpu
9966,9967c8644,8647
< 			if (!cpumask_test_cpu(this_cpu, busiest->curr->cpus_ptr)) {
< 				raw_spin_rq_unlock_irqrestore(busiest, flags);
---
> 			if (!cpumask_test_cpu(this_cpu, &busiest->curr->cpus_allowed)) {
> 				raw_spin_unlock_irqrestore(&busiest->lock,
> 							    flags);
> 				env.flags |= LBF_ALL_PINNED;
9971,9973d8650
< 			/* Record that we found at least one task that could run on this_cpu */
< 			env.flags &= ~LBF_ALL_PINNED;
< 
9984c8661
< 			raw_spin_rq_unlock_irqrestore(busiest, flags);
---
> 			raw_spin_unlock_irqrestore(&busiest->lock, flags);
9990a8668,8670
> 
> 			/* We've kicked active balancing, force task migration. */
> 			sd->nr_balance_failed = sd->cache_nice_tries+1;
9992c8672
< 	} else {
---
> 	} else
9994d8673
< 	}
9996c8675
< 	if (likely(!active_balance) || need_active_balance(&env)) {
---
> 	if (likely(!active_balance)) {
9998a8678,8686
> 	} else {
> 		/*
> 		 * If we've begun active balancing, start to back off. This
> 		 * case may not be covered by the all_pinned logic if there
> 		 * is only 1 task on the busy runqueue (because we don't call
> 		 * detach_tasks).
> 		 */
> 		if (sd->balance_interval < sd->max_interval)
> 			sd->balance_interval *= 2;
10006,10007c8694
< 	 * constraints. Clear the imbalance flag only if other tasks got
< 	 * a chance to move and fix the imbalance.
---
> 	 * constraints. Clear the imbalance flag if it was set.
10009c8696
< 	if (sd_parent && !(env.flags & LBF_ALL_PINNED)) {
---
> 	if (sd_parent) {
10027,10037d8713
< 	ld_moved = 0;
< 
< 	/*
< 	 * newidle_balance() disregards balance intervals, so we could
< 	 * repeatedly reach this code, which would lead to balance_interval
< 	 * skyrocketing in a short amount of time. Skip the balance_interval
< 	 * increase logic to avoid that.
< 	 */
< 	if (env.idle == CPU_NEWLY_IDLE)
< 		goto out;
< 
10039,10041c8715,8717
< 	if ((env.flags & LBF_ALL_PINNED &&
< 	     sd->balance_interval < MAX_PINNED_INTERVAL) ||
< 	    sd->balance_interval < sd->max_interval)
---
> 	if (((env.flags & LBF_ALL_PINNED) &&
> 			sd->balance_interval < MAX_PINNED_INTERVAL) ||
> 			(sd->balance_interval < sd->max_interval))
10042a8719,8720
> 
> 	ld_moved = 0;
10057,10065d8734
< 
< 	/*
< 	 * Reduce likelihood of busy balancing at higher domains racing with
< 	 * balancing at lower domains by preventing their balancing periods
< 	 * from being multiples of each other.
< 	 */
< 	if (cpu_busy)
< 		interval -= 1;
< 
10085c8754,8868
<  * active_load_balance_cpu_stop is run by the CPU stopper. It pushes
---
>  * idle_balance is called by schedule() if this_cpu is about to become
>  * idle. Attempts to pull tasks from other CPUs.
>  */
> static int idle_balance(struct rq *this_rq, struct rq_flags *rf)
> {
> 	unsigned long next_balance = jiffies + HZ;
> 	int this_cpu = this_rq->cpu;
> 	struct sched_domain *sd;
> 	int pulled_task = 0;
> 	u64 curr_cost = 0;
> 
> 	/*
> 	 * We must set idle_stamp _before_ calling idle_balance(), such that we
> 	 * measure the duration of idle_balance() as idle time.
> 	 */
> 	this_rq->idle_stamp = rq_clock(this_rq);
> 
> 	/*
> 	 * Do not pull tasks towards !active CPUs...
> 	 */
> 	if (!cpu_active(this_cpu))
> 		return 0;
> 
> 	/*
> 	 * This is OK, because current is on_cpu, which avoids it being picked
> 	 * for load-balance and preemption/IRQs are still disabled avoiding
> 	 * further scheduler activity on it and we're being very careful to
> 	 * re-start the picking loop.
> 	 */
> 	rq_unpin_lock(this_rq, rf);
> 
> 	if (this_rq->avg_idle < sysctl_sched_migration_cost ||
> 	    !this_rq->rd->overload) {
> 		rcu_read_lock();
> 		sd = rcu_dereference_check_sched_domain(this_rq->sd);
> 		if (sd)
> 			update_next_balance(sd, &next_balance);
> 		rcu_read_unlock();
> 
> 		goto out;
> 	}
> 
> 	raw_spin_unlock(&this_rq->lock);
> 
> 	update_blocked_averages(this_cpu);
> 	rcu_read_lock();
> 	for_each_domain(this_cpu, sd) {
> 		int continue_balancing = 1;
> 		u64 t0, domain_cost;
> 
> 		if (!(sd->flags & SD_LOAD_BALANCE))
> 			continue;
> 
> 		if (this_rq->avg_idle < curr_cost + sd->max_newidle_lb_cost) {
> 			update_next_balance(sd, &next_balance);
> 			break;
> 		}
> 
> 		if (sd->flags & SD_BALANCE_NEWIDLE) {
> 			t0 = sched_clock_cpu(this_cpu);
> 
> 			pulled_task = load_balance(this_cpu, this_rq,
> 						   sd, CPU_NEWLY_IDLE,
> 						   &continue_balancing);
> 
> 			domain_cost = sched_clock_cpu(this_cpu) - t0;
> 			if (domain_cost > sd->max_newidle_lb_cost)
> 				sd->max_newidle_lb_cost = domain_cost;
> 
> 			curr_cost += domain_cost;
> 		}
> 
> 		update_next_balance(sd, &next_balance);
> 
> 		/*
> 		 * Stop searching for tasks to pull if there are
> 		 * now runnable tasks on this rq.
> 		 */
> 		if (pulled_task || this_rq->nr_running > 0)
> 			break;
> 	}
> 	rcu_read_unlock();
> 
> 	raw_spin_lock(&this_rq->lock);
> 
> 	if (curr_cost > this_rq->max_idle_balance_cost)
> 		this_rq->max_idle_balance_cost = curr_cost;
> 
> 	/*
> 	 * While browsing the domains, we released the rq lock, a task could
> 	 * have been enqueued in the meantime. Since we're not going idle,
> 	 * pretend we pulled a task.
> 	 */
> 	if (this_rq->cfs.h_nr_running && !pulled_task)
> 		pulled_task = 1;
> 
> out:
> 	/* Move the next balance forward */
> 	if (time_after(this_rq->next_balance, next_balance))
> 		this_rq->next_balance = next_balance;
> 
> 	/* Is there a task of a high priority class? */
> 	if (this_rq->nr_running != this_rq->cfs.h_nr_running)
> 		pulled_task = -1;
> 
> 	if (pulled_task)
> 		this_rq->idle_stamp = 0;
> 
> 	rq_repin_lock(this_rq, rf);
> 
> 	return pulled_task;
> }
> 
> /*
>  * active_load_balance_cpu_stop is run by cpu stopper. It pushes
10109c8892
< 	/* Make sure the requested CPU hasn't gone down in the meantime: */
---
> 	/* make sure the requested cpu hasn't gone down in the meantime */
10121c8904
< 	 * Bjorn Helgaas on a 128-CPU setup.
---
> 	 * Bjorn Helgaas on a 128-cpu setup.
10128,10129c8911,8913
< 		if (cpumask_test_cpu(busiest_cpu, sched_domain_span(sd)))
< 			break;
---
> 		if ((sd->flags & SD_LOAD_BALANCE) &&
> 		    cpumask_test_cpu(busiest_cpu, sched_domain_span(sd)))
> 				break;
10140c8924,8930
< 			.flags		= LBF_ACTIVE_LB,
---
> 			/*
> 			 * can_migrate_task() doesn't need to compute new_dst_cpu
> 			 * for active balancing. Since we have CPU_IDLE, but no
> 			 * @dst_grpmask we need to make that test go away with lying
> 			 * about DST_PINNED.
> 			 */
> 			.flags		= LBF_DST_PINNED,
10168,10271d8957
< static DEFINE_SPINLOCK(balancing);
< 
< /*
<  * Scale the max load_balance interval with the number of CPUs in the system.
<  * This trades load-balance latency on larger machines for less cross talk.
<  */
< void update_max_interval(void)
< {
< 	max_load_balance_interval = HZ*num_online_cpus()/10;
< }
< 
< /*
<  * It checks each scheduling domain to see if it is due to be balanced,
<  * and initiates a balancing operation if so.
<  *
<  * Balancing parameters are set up in init_sched_domains.
<  */
< static void rebalance_domains(struct rq *rq, enum cpu_idle_type idle)
< {
< 	int continue_balancing = 1;
< 	int cpu = rq->cpu;
< 	int busy = idle != CPU_IDLE && !sched_idle_cpu(cpu);
< 	unsigned long interval;
< 	struct sched_domain *sd;
< 	/* Earliest time when we have to do rebalance again */
< 	unsigned long next_balance = jiffies + 60*HZ;
< 	int update_next_balance = 0;
< 	int need_serialize, need_decay = 0;
< 	u64 max_cost = 0;
< 
< 	rcu_read_lock();
< 	for_each_domain(cpu, sd) {
< 		/*
< 		 * Decay the newidle max times here because this is a regular
< 		 * visit to all the domains. Decay ~1% per second.
< 		 */
< 		if (time_after(jiffies, sd->next_decay_max_lb_cost)) {
< 			sd->max_newidle_lb_cost =
< 				(sd->max_newidle_lb_cost * 253) / 256;
< 			sd->next_decay_max_lb_cost = jiffies + HZ;
< 			need_decay = 1;
< 		}
< 		max_cost += sd->max_newidle_lb_cost;
< 
< 		/*
< 		 * Stop the load balance at this level. There is another
< 		 * CPU in our sched group which is doing load balancing more
< 		 * actively.
< 		 */
< 		if (!continue_balancing) {
< 			if (need_decay)
< 				continue;
< 			break;
< 		}
< 
< 		interval = get_sd_balance_interval(sd, busy);
< 
< 		need_serialize = sd->flags & SD_SERIALIZE;
< 		if (need_serialize) {
< 			if (!spin_trylock(&balancing))
< 				goto out;
< 		}
< 
< 		if (time_after_eq(jiffies, sd->last_balance + interval)) {
< 			if (load_balance(cpu, rq, sd, idle, &continue_balancing)) {
< 				/*
< 				 * The LBF_DST_PINNED logic could have changed
< 				 * env->dst_cpu, so we can't know our idle
< 				 * state even if we migrated tasks. Update it.
< 				 */
< 				idle = idle_cpu(cpu) ? CPU_IDLE : CPU_NOT_IDLE;
< 				busy = idle != CPU_IDLE && !sched_idle_cpu(cpu);
< 			}
< 			sd->last_balance = jiffies;
< 			interval = get_sd_balance_interval(sd, busy);
< 		}
< 		if (need_serialize)
< 			spin_unlock(&balancing);
< out:
< 		if (time_after(next_balance, sd->last_balance + interval)) {
< 			next_balance = sd->last_balance + interval;
< 			update_next_balance = 1;
< 		}
< 	}
< 	if (need_decay) {
< 		/*
< 		 * Ensure the rq-wide value also decays but keep it at a
< 		 * reasonable floor to avoid funnies with rq->avg_idle.
< 		 */
< 		rq->max_idle_balance_cost =
< 			max((u64)sysctl_sched_migration_cost, max_cost);
< 	}
< 	rcu_read_unlock();
< 
< 	/*
< 	 * next_balance will be updated only when there is a need.
< 	 * When the cpu is attached to null domain for ex, it will not be
< 	 * updated.
< 	 */
< 	if (likely(update_next_balance))
< 		rq->next_balance = next_balance;
< 
< }
< 
10283,10284d8968
<  * - HK_FLAG_MISC CPUs are used for this task, because HK_FLAG_SCHED not set
<  *   anywhere yet.
10285a8970,8974
> static struct {
> 	cpumask_var_t idle_cpus_mask;
> 	atomic_t nr_cpus;
> 	unsigned long next_balance;     /* in jiffy units */
> } nohz ____cacheline_aligned;
10289,10290c8978
< 	int ilb;
< 	const struct cpumask *hk_mask;
---
> 	int ilb = cpumask_first(nohz.idle_cpus_mask);
10292,10301c8980,8981
< 	hk_mask = housekeeping_cpumask(HK_FLAG_MISC);
< 
< 	for_each_cpu_and(ilb, nohz.idle_cpus_mask, hk_mask) {
< 
< 		if (ilb == smp_processor_id())
< 			continue;
< 
< 		if (idle_cpu(ilb))
< 			return ilb;
< 	}
---
> 	if (ilb < nr_cpu_ids && idle_cpu(ilb))
> 		return ilb;
10307,10308c8987,8989
<  * Kick a CPU to do the nohz balancing, if it is time for it. We pick any
<  * idle CPU in the HK_FLAG_MISC housekeeping set (if there is one).
---
>  * Kick a CPU to do the nohz balancing, if it is time for it. We pick the
>  * nohz_load_balancer CPU (if there is one) otherwise fallback to any idle
>  * CPU (if there is one).
10310c8991
< static void kick_ilb(unsigned int flags)
---
> static void nohz_balancer_kick(void)
10314,10319c8995
< 	/*
< 	 * Increase nohz.next_balance only when if full ilb is triggered but
< 	 * not if we only update stats.
< 	 */
< 	if (flags & NOHZ_BALANCE_KICK)
< 		nohz.next_balance = jiffies+1;
---
> 	nohz.next_balance++;
10326,10331c9002
< 	/*
< 	 * Access to rq::nohz_csd is serialized by NOHZ_KICK_MASK; he who sets
< 	 * the first flag owns it; cleared by nohz_csd_func().
< 	 */
< 	flags = atomic_fetch_or(flags, nohz_flags(ilb_cpu));
< 	if (flags & NOHZ_KICK_MASK)
---
> 	if (test_and_set_bit(NOHZ_BALANCE_KICK, nohz_flags(ilb_cpu)))
10333d9003
< 
10335c9005,9006
< 	 * This way we generate an IPI on the target CPU which
---
> 	 * Use smp_send_reschedule() instead of resched_cpu().
> 	 * This way we generate a sched IPI on the target cpu which
10339c9010,9011
< 	smp_call_function_single_async(ilb_cpu, &cpu_rq(ilb_cpu)->nohz_csd);
---
> 	smp_send_reschedule(ilb_cpu);
> 	return;
10342,10346c9014
< /*
<  * Current decision point for kicking the idle load balancer in the presence
<  * of idle CPUs in the system.
<  */
< static void nohz_balancer_kick(struct rq *rq)
---
> void nohz_balance_exit_idle(unsigned int cpu)
10348,10413c9016
< 	unsigned long now = jiffies;
< 	struct sched_domain_shared *sds;
< 	struct sched_domain *sd;
< 	int nr_busy, i, cpu = rq->cpu;
< 	unsigned int flags = 0;
< 
< 	if (unlikely(rq->idle_balance))
< 		return;
< 
< 	/*
< 	 * We may be recently in ticked or tickless idle mode. At the first
< 	 * busy tick after returning from idle, we will update the busy stats.
< 	 */
< 	nohz_balance_exit_idle(rq);
< 
< 	/*
< 	 * None are in tickless mode and hence no need for NOHZ idle load
< 	 * balancing.
< 	 */
< 	if (likely(!atomic_read(&nohz.nr_cpus)))
< 		return;
< 
< 	if (READ_ONCE(nohz.has_blocked) &&
< 	    time_after(now, READ_ONCE(nohz.next_blocked)))
< 		flags = NOHZ_STATS_KICK;
< 
< 	if (time_before(now, nohz.next_balance))
< 		goto out;
< 
< 	if (rq->nr_running >= 2) {
< 		flags = NOHZ_KICK_MASK;
< 		goto out;
< 	}
< 
< 	rcu_read_lock();
< 
< 	sd = rcu_dereference(rq->sd);
< 	if (sd) {
< 		/*
< 		 * If there's a CFS task and the current CPU has reduced
< 		 * capacity; kick the ILB to see if there's a better CPU to run
< 		 * on.
< 		 */
< 		if (rq->cfs.h_nr_running >= 1 && check_cpu_capacity(rq, sd)) {
< 			flags = NOHZ_KICK_MASK;
< 			goto unlock;
< 		}
< 	}
< 
< 	sd = rcu_dereference(per_cpu(sd_asym_packing, cpu));
< 	if (sd) {
< 		/*
< 		 * When ASYM_PACKING; see if there's a more preferred CPU
< 		 * currently idle; in which case, kick the ILB to move tasks
< 		 * around.
< 		 */
< 		for_each_cpu_and(i, sched_domain_span(sd), nohz.idle_cpus_mask) {
< 			if (sched_asym_prefer(i, cpu)) {
< 				flags = NOHZ_KICK_MASK;
< 				goto unlock;
< 			}
< 		}
< 	}
< 
< 	sd = rcu_dereference(per_cpu(sd_asym_cpucapacity, cpu));
< 	if (sd) {
---
> 	if (unlikely(test_bit(NOHZ_TICK_STOPPED, nohz_flags(cpu)))) {
10415,10416c9018
< 		 * When ASYM_CPUCAPACITY; see if there's a higher capacity CPU
< 		 * to run the misfit task on.
---
> 		 * Completely isolated CPUs don't ever set, so we must test.
10418,10420c9020,9022
< 		if (check_misfit_status(rq, sd)) {
< 			flags = NOHZ_KICK_MASK;
< 			goto unlock;
---
> 		if (likely(cpumask_test_cpu(cpu, nohz.idle_cpus_mask))) {
> 			cpumask_clear_cpu(cpu, nohz.idle_cpus_mask);
> 			atomic_dec(&nohz.nr_cpus);
10422,10430c9024
< 
< 		/*
< 		 * For asymmetric systems, we do not want to nicely balance
< 		 * cache use, instead we want to embrace asymmetry and only
< 		 * ensure tasks have enough CPU capacity.
< 		 *
< 		 * Skip the LLC logic because it's not relevant in that case.
< 		 */
< 		goto unlock;
---
> 		clear_bit(NOHZ_TICK_STOPPED, nohz_flags(cpu));
10432,10454d9025
< 
< 	sds = rcu_dereference(per_cpu(sd_llc_shared, cpu));
< 	if (sds) {
< 		/*
< 		 * If there is an imbalance between LLC domains (IOW we could
< 		 * increase the overall cache use), we need some less-loaded LLC
< 		 * domain to pull some load. Likewise, we may need to spread
< 		 * load within the current LLC domain (e.g. packed SMT cores but
< 		 * other CPUs are idle). We can't really know from here how busy
< 		 * the others are - so just get a nohz balance going if it looks
< 		 * like this LLC domain has tasks we could move.
< 		 */
< 		nr_busy = atomic_read(&sds->nr_busy_cpus);
< 		if (nr_busy > 1) {
< 			flags = NOHZ_KICK_MASK;
< 			goto unlock;
< 		}
< 	}
< unlock:
< 	rcu_read_unlock();
< out:
< 	if (flags)
< 		kick_ilb(flags);
10457c9028
< static void set_cpu_sd_state_busy(int cpu)
---
> static inline void set_cpu_sd_state_busy(void)
10459a9031
> 	int cpu = smp_processor_id();
10473,10487c9045
< void nohz_balance_exit_idle(struct rq *rq)
< {
< 	SCHED_WARN_ON(rq != this_rq());
< 
< 	if (likely(!rq->nohz_tick_stopped))
< 		return;
< 
< 	rq->nohz_tick_stopped = 0;
< 	cpumask_clear_cpu(rq->cpu, nohz.idle_cpus_mask);
< 	atomic_dec(&nohz.nr_cpus);
< 
< 	set_cpu_sd_state_busy(rq->cpu);
< }
< 
< static void set_cpu_sd_state_idle(int cpu)
---
> void set_cpu_sd_state_idle(void)
10489a9048
> 	int cpu = smp_processor_id();
10504c9063
<  * This routine will record that the CPU is going idle with tick stopped.
---
>  * This routine will record that the cpu is going idle with tick stopped.
10509,10513c9068,9070
< 	struct rq *rq = cpu_rq(cpu);
< 
< 	SCHED_WARN_ON(cpu != smp_processor_id());
< 
< 	/* If this CPU is going down, then nothing needs to be done: */
---
> 	/*
> 	 * If this cpu is going down, then nothing needs to be done.
> 	 */
10521,10526c9078,9079
< 	/*
< 	 * Can be set safely without rq->lock held
< 	 * If a clear happens, it will have evaluated last additions because
< 	 * rq->lock is held during the check and the clear
< 	 */
< 	rq->has_blocked_load = 1;
---
> 	if (test_bit(NOHZ_TICK_STOPPED, nohz_flags(cpu)))
> 		return;
10529,10532c9082
< 	 * The tick is still stopped but load could have been added in the
< 	 * meantime. We set the nohz.has_blocked flag to trig a check of the
< 	 * *_avg. The CPU is already part of nohz.idle_cpus_mask so the clear
< 	 * of nohz.has_blocked can only happen after checking the new load
---
> 	 * If we're a completely isolated CPU, we don't play.
10534,10538c9084
< 	if (rq->nohz_tick_stopped)
< 		goto out;
< 
< 	/* If we're a completely isolated CPU, we don't play: */
< 	if (on_null_domain(rq))
---
> 	if (on_null_domain(cpu_rq(cpu)))
10541,10542d9086
< 	rq->nohz_tick_stopped = 1;
< 
10544a9089,9091
> 	set_bit(NOHZ_TICK_STOPPED, nohz_flags(cpu));
> }
> #endif
10546,10553c9093
< 	/*
< 	 * Ensures that if nohz_idle_balance() fails to observe our
< 	 * @idle_cpus_mask store, it must observe the @has_blocked
< 	 * store.
< 	 */
< 	smp_mb__after_atomic();
< 
< 	set_cpu_sd_state_idle(cpu);
---
> static DEFINE_SPINLOCK(balancing);
10555,10560c9095,9101
< out:
< 	/*
< 	 * Each time a cpu enter idle, we assume that it has blocked load and
< 	 * enable the periodic update of the load of idle cpus
< 	 */
< 	WRITE_ONCE(nohz.has_blocked, 1);
---
> /*
>  * Scale the max load_balance interval with the number of CPUs in the system.
>  * This trades load-balance latency on larger machines for less cross talk.
>  */
> void update_max_interval(void)
> {
> 	max_load_balance_interval = HZ*num_online_cpus()/10;
10563c9104,9110
< static bool update_nohz_stats(struct rq *rq)
---
> /*
>  * It checks each scheduling domain to see if it is due to be balanced,
>  * and initiates a balancing operation if so.
>  *
>  * Balancing parameters are set up in init_sched_domains.
>  */
> static void rebalance_domains(struct rq *rq, enum cpu_idle_type idle)
10565c9112,9120
< 	unsigned int cpu = rq->cpu;
---
> 	int continue_balancing = 1;
> 	int cpu = rq->cpu;
> 	unsigned long interval;
> 	struct sched_domain *sd;
> 	/* Earliest time when we have to do rebalance again */
> 	unsigned long next_balance = jiffies + 60*HZ;
> 	int update_next_balance = 0;
> 	int need_serialize, need_decay = 0;
> 	u64 max_cost = 0;
10567,10568c9122
< 	if (!rq->has_blocked_load)
< 		return false;
---
> 	update_blocked_averages(cpu);
10570,10571c9124,9136
< 	if (!cpumask_test_cpu(cpu, nohz.idle_cpus_mask))
< 		return false;
---
> 	rcu_read_lock();
> 	for_each_domain(cpu, sd) {
> 		/*
> 		 * Decay the newidle max times here because this is a regular
> 		 * visit to all the domains. Decay ~1% per second.
> 		 */
> 		if (time_after(jiffies, sd->next_decay_max_lb_cost)) {
> 			sd->max_newidle_lb_cost =
> 				(sd->max_newidle_lb_cost * 253) / 256;
> 			sd->next_decay_max_lb_cost = jiffies + HZ;
> 			need_decay = 1;
> 		}
> 		max_cost += sd->max_newidle_lb_cost;
10573,10574c9138,9139
< 	if (!time_after(jiffies, READ_ONCE(rq->last_blocked_load_update_tick)))
< 		return true;
---
> 		if (!(sd->flags & SD_LOAD_BALANCE))
> 			continue;
10576c9141,9152
< 	update_blocked_averages(cpu);
---
> 		/*
> 		 * Stop the load balance at this level. There is another
> 		 * CPU in our sched group which is doing load balancing more
> 		 * actively.
> 		 */
> 		if (!continue_balancing) {
> 			if (need_decay)
> 				continue;
> 			break;
> 		}
> 
> 		interval = get_sd_balance_interval(sd, idle != CPU_IDLE);
10578c9154,9210
< 	return rq->has_blocked_load;
---
> 		need_serialize = sd->flags & SD_SERIALIZE;
> 		if (need_serialize) {
> 			if (!spin_trylock(&balancing))
> 				goto out;
> 		}
> 
> 		if (time_after_eq(jiffies, sd->last_balance + interval)) {
> 			if (load_balance(cpu, rq, sd, idle, &continue_balancing)) {
> 				/*
> 				 * The LBF_DST_PINNED logic could have changed
> 				 * env->dst_cpu, so we can't know our idle
> 				 * state even if we migrated tasks. Update it.
> 				 */
> 				idle = idle_cpu(cpu) ? CPU_IDLE : CPU_NOT_IDLE;
> 			}
> 			sd->last_balance = jiffies;
> 			interval = get_sd_balance_interval(sd, idle != CPU_IDLE);
> 		}
> 		if (need_serialize)
> 			spin_unlock(&balancing);
> out:
> 		if (time_after(next_balance, sd->last_balance + interval)) {
> 			next_balance = sd->last_balance + interval;
> 			update_next_balance = 1;
> 		}
> 	}
> 	if (need_decay) {
> 		/*
> 		 * Ensure the rq-wide value also decays but keep it at a
> 		 * reasonable floor to avoid funnies with rq->avg_idle.
> 		 */
> 		rq->max_idle_balance_cost =
> 			max((u64)sysctl_sched_migration_cost, max_cost);
> 	}
> 	rcu_read_unlock();
> 
> 	/*
> 	 * next_balance will be updated only when there is a need.
> 	 * When the cpu is attached to null domain for ex, it will not be
> 	 * updated.
> 	 */
> 	if (likely(update_next_balance)) {
> 		rq->next_balance = next_balance;
> 
> #ifdef CONFIG_NO_HZ_COMMON
> 		/*
> 		 * If this CPU has been elected to perform the nohz idle
> 		 * balance. Other idle CPUs have already rebalanced with
> 		 * nohz_idle_balance() and nohz.next_balance has been
> 		 * updated accordingly. This CPU is now running the idle load
> 		 * balance for itself and we need to update the
> 		 * nohz.next_balance accordingly.
> 		 */
> 		if ((idle == CPU_IDLE) && time_after(nohz.next_balance, rq->next_balance))
> 			nohz.next_balance = rq->next_balance;
> #endif
> 	}
10580a9213
> #ifdef CONFIG_NO_HZ_COMMON
10582,10584c9215,9216
<  * Internal function that runs load balance for all idle cpus. The load balance
<  * can be a simple update of blocked load or a complete load balance with
<  * tasks movement depending of flags.
---
>  * In CONFIG_NO_HZ_COMMON case, the idle balance kickee will do the
>  * rebalancing for all the cpus for whom scheduler ticks are stopped.
10586,10587c9218
< static void _nohz_idle_balance(struct rq *this_rq, unsigned int flags,
< 			       enum cpu_idle_type idle)
---
> static void nohz_idle_balance(struct rq *this_rq, enum cpu_idle_type idle)
10589,10593d9219
< 	/* Earliest time when we have to do rebalance again */
< 	unsigned long now = jiffies;
< 	unsigned long next_balance = now + 60*HZ;
< 	bool has_blocked_load = false;
< 	int update_next_balance = 0;
10595d9220
< 	int balance_cpu;
10596a9222,9225
> 	int balance_cpu;
> 	/* Earliest time when we have to do rebalance again */
> 	unsigned long next_balance = jiffies + 60*HZ;
> 	int update_next_balance = 0;
10598,10614c9227,9229
< 	SCHED_WARN_ON((flags & NOHZ_KICK_MASK) == NOHZ_BALANCE_KICK);
< 
< 	/*
< 	 * We assume there will be no idle load after this update and clear
< 	 * the has_blocked flag. If a cpu enters idle in the mean time, it will
< 	 * set the has_blocked flag and trig another update of idle load.
< 	 * Because a cpu that becomes idle, is added to idle_cpus_mask before
< 	 * setting the flag, we are sure to not clear the state and not
< 	 * check the load of an idle cpu.
< 	 */
< 	WRITE_ONCE(nohz.has_blocked, 0);
< 
< 	/*
< 	 * Ensures that if we miss the CPU, we must see the has_blocked
< 	 * store from nohz_balance_enter_idle().
< 	 */
< 	smp_mb();
---
> 	if (idle != CPU_IDLE ||
> 	    !test_bit(NOHZ_BALANCE_KICK, nohz_flags(this_cpu)))
> 		goto end;
10616,10621c9231,9232
< 	/*
< 	 * Start with the next CPU after this_cpu so we will end with this_cpu and let a
< 	 * chance for other idle cpu to pull load.
< 	 */
< 	for_each_cpu_wrap(balance_cpu,  nohz.idle_cpus_mask, this_cpu+1) {
< 		if (!idle_cpu(balance_cpu))
---
> 	for_each_cpu(balance_cpu, nohz.idle_cpus_mask) {
> 		if (balance_cpu == this_cpu || !idle_cpu(balance_cpu))
10625,10626c9236,9237
< 		 * If this CPU gets work to do, stop the load balancing
< 		 * work being done for other CPUs. Next load
---
> 		 * If this cpu gets work to do, stop the load balancing
> 		 * work being done for other cpus. Next load
10629,10632c9240,9241
< 		if (need_resched()) {
< 			has_blocked_load = true;
< 			goto abort;
< 		}
---
> 		if (need_resched())
> 			break;
10636,10637d9244
< 		has_blocked_load |= update_nohz_stats(rq);
< 
10645c9252
< 			rq_lock_irqsave(rq, &rf);
---
> 			rq_lock_irq(rq, &rf);
10647c9254,9255
< 			rq_unlock_irqrestore(rq, &rf);
---
> 			cpu_load_update_idle(rq);
> 			rq_unlock_irq(rq, &rf);
10649,10650c9257
< 			if (flags & NOHZ_BALANCE_KICK)
< 				rebalance_domains(rq, CPU_IDLE);
---
> 			rebalance_domains(rq, CPU_IDLE);
10666,10673c9273,9274
< 
< 	WRITE_ONCE(nohz.next_blocked,
< 		now + msecs_to_jiffies(LOAD_AVG_PERIOD));
< 
< abort:
< 	/* There is still blocked load, enable periodic update */
< 	if (has_blocked_load)
< 		WRITE_ONCE(nohz.has_blocked, 1);
---
> end:
> 	clear_bit(NOHZ_BALANCE_KICK, nohz_flags(this_cpu));
10677,10678c9278,9286
<  * In CONFIG_NO_HZ_COMMON case, the idle balance kickee will do the
<  * rebalancing for all the cpus for whom scheduler ticks are stopped.
---
>  * Current heuristic for kicking the idle load balancer in the presence
>  * of an idle cpu in the system.
>  *   - This rq has more than one task.
>  *   - This rq has at least one CFS task and the capacity of the CPU is
>  *     significantly reduced because of RT tasks or IRQs.
>  *   - At parent of LLC scheduler domain level, this cpu's scheduler group has
>  *     multiple busy cpu.
>  *   - For SD_ASYM_PACKING, if the lower numbered cpu's in the scheduler
>  *     domain span are idle.
10680c9288
< static bool nohz_idle_balance(struct rq *this_rq, enum cpu_idle_type idle)
---
> static inline bool nohz_kick_needed(struct rq *rq)
10682,10765c9290,9291
< 	unsigned int flags = this_rq->nohz_idle_balance;
< 
< 	if (!flags)
< 		return false;
< 
< 	this_rq->nohz_idle_balance = 0;
< 
< 	if (idle != CPU_IDLE)
< 		return false;
< 
< 	_nohz_idle_balance(this_rq, flags, idle);
< 
< 	return true;
< }
< 
< /*
<  * Check if we need to run the ILB for updating blocked load before entering
<  * idle state.
<  */
< void nohz_run_idle_balance(int cpu)
< {
< 	unsigned int flags;
< 
< 	flags = atomic_fetch_andnot(NOHZ_NEWILB_KICK, nohz_flags(cpu));
< 
< 	/*
< 	 * Update the blocked load only if no SCHED_SOFTIRQ is about to happen
< 	 * (ie NOHZ_STATS_KICK set) and will do the same.
< 	 */
< 	if ((flags == NOHZ_NEWILB_KICK) && !need_resched())
< 		_nohz_idle_balance(cpu_rq(cpu), NOHZ_STATS_KICK, CPU_IDLE);
< }
< 
< static void nohz_newidle_balance(struct rq *this_rq)
< {
< 	int this_cpu = this_rq->cpu;
< 
< 	/*
< 	 * This CPU doesn't want to be disturbed by scheduler
< 	 * housekeeping
< 	 */
< 	if (!housekeeping_cpu(this_cpu, HK_FLAG_SCHED))
< 		return;
< 
< 	/* Will wake up very soon. No time for doing anything else*/
< 	if (this_rq->avg_idle < sysctl_sched_migration_cost)
< 		return;
< 
< 	/* Don't need to update blocked load of idle CPUs*/
< 	if (!READ_ONCE(nohz.has_blocked) ||
< 	    time_before(jiffies, READ_ONCE(nohz.next_blocked)))
< 		return;
< 
< 	/*
< 	 * Set the need to trigger ILB in order to update blocked load
< 	 * before entering idle state.
< 	 */
< 	atomic_or(NOHZ_NEWILB_KICK, nohz_flags(this_cpu));
< }
< 
< #else /* !CONFIG_NO_HZ_COMMON */
< static inline void nohz_balancer_kick(struct rq *rq) { }
< 
< static inline bool nohz_idle_balance(struct rq *this_rq, enum cpu_idle_type idle)
< {
< 	return false;
< }
< 
< static inline void nohz_newidle_balance(struct rq *this_rq) { }
< #endif /* CONFIG_NO_HZ_COMMON */
< 
< /*
<  * newidle_balance is called by schedule() if this_cpu is about to become
<  * idle. Attempts to pull tasks from other CPUs.
<  *
<  * Returns:
<  *   < 0 - we released the lock and there are !fair tasks present
<  *     0 - failed, no new tasks
<  *   > 0 - success, new (fair) tasks present
<  */
< static int newidle_balance(struct rq *this_rq, struct rq_flags *rf)
< {
< 	unsigned long next_balance = jiffies + HZ;
< 	int this_cpu = this_rq->cpu;
---
> 	unsigned long now = jiffies;
> 	struct sched_domain_shared *sds;
10767,10777c9293,9294
< 	int pulled_task = 0;
< 	u64 curr_cost = 0;
< 
< 	update_misfit_status(NULL, this_rq);
< 
< 	/*
< 	 * There is a task waiting to run. No need to search for one.
< 	 * Return 0; the task will be enqueued when switching to idle.
< 	 */
< 	if (this_rq->ttwu_pending)
< 		return 0;
---
> 	int nr_busy, i, cpu = rq->cpu;
> 	bool kick = false;
10779,10783c9296,9297
< 	/*
< 	 * We must set idle_stamp _before_ calling idle_balance(), such that we
< 	 * measure the duration of idle_balance() as idle time.
< 	 */
< 	this_rq->idle_stamp = rq_clock(this_rq);
---
> 	if (unlikely(rq->idle_balance))
> 		return false;
10785,10789c9299,9304
< 	/*
< 	 * Do not pull tasks towards !active CPUs...
< 	 */
< 	if (!cpu_active(this_cpu))
< 		return 0;
---
>        /*
> 	* We may be recently in ticked or tickless idle mode. At the first
> 	* busy tick after returning from idle, we will update the busy stats.
> 	*/
> 	set_cpu_sd_state_busy();
> 	nohz_balance_exit_idle(cpu);
10792,10795c9307,9308
< 	 * This is OK, because current is on_cpu, which avoids it being picked
< 	 * for load-balance and preemption/IRQs are still disabled avoiding
< 	 * further scheduler activity on it and we're being very careful to
< 	 * re-start the picking loop.
---
> 	 * None are in tickless mode and hence no need for NOHZ idle load
> 	 * balancing.
10797,10806c9310,9311
< 	rq_unpin_lock(this_rq, rf);
< 
< 	if (this_rq->avg_idle < sysctl_sched_migration_cost ||
< 	    !READ_ONCE(this_rq->rd->overload)) {
< 
< 		rcu_read_lock();
< 		sd = rcu_dereference_check_sched_domain(this_rq->sd);
< 		if (sd)
< 			update_next_balance(sd, &next_balance);
< 		rcu_read_unlock();
---
> 	if (likely(!atomic_read(&nohz.nr_cpus)))
> 		return false;
10808,10809c9313,9314
< 		goto out;
< 	}
---
> 	if (time_before(now, nohz.next_balance))
> 		return false;
10811c9316,9317
< 	raw_spin_rq_unlock(this_rq);
---
> 	if (rq->nr_running >= 2)
> 		return true;
10813d9318
< 	update_blocked_averages(this_cpu);
10815,10821c9320,9329
< 	for_each_domain(this_cpu, sd) {
< 		int continue_balancing = 1;
< 		u64 t0, domain_cost;
< 
< 		if (this_rq->avg_idle < curr_cost + sd->max_newidle_lb_cost) {
< 			update_next_balance(sd, &next_balance);
< 			break;
---
> 	sds = rcu_dereference(per_cpu(sd_llc_shared, cpu));
> 	if (sds) {
> 		/*
> 		 * XXX: write a coherent comment on why we do this.
> 		 * See also: http://lkml.kernel.org/r/20111202010832.602203411@sbsiddha-desk.sc.intel.com
> 		 */
> 		nr_busy = atomic_read(&sds->nr_busy_cpus);
> 		if (nr_busy > 1) {
> 			kick = true;
> 			goto unlock;
10824,10833c9332
< 		if (sd->flags & SD_BALANCE_NEWIDLE) {
< 			t0 = sched_clock_cpu(this_cpu);
< 
< 			pulled_task = load_balance(this_cpu, this_rq,
< 						   sd, CPU_NEWLY_IDLE,
< 						   &continue_balancing);
< 
< 			domain_cost = sched_clock_cpu(this_cpu) - t0;
< 			if (domain_cost > sd->max_newidle_lb_cost)
< 				sd->max_newidle_lb_cost = domain_cost;
---
> 	}
10835c9334,9339
< 			curr_cost += domain_cost;
---
> 	sd = rcu_dereference(rq->sd);
> 	if (sd) {
> 		if ((rq->cfs.h_nr_running >= 1) &&
> 				check_cpu_capacity(rq, sd)) {
> 			kick = true;
> 			goto unlock;
10836a9341
> 	}
10838c9343,9348
< 		update_next_balance(sd, &next_balance);
---
> 	sd = rcu_dereference(per_cpu(sd_asym, cpu));
> 	if (sd) {
> 		for_each_cpu(i, sched_domain_span(sd)) {
> 			if (i == cpu ||
> 			    !cpumask_test_cpu(i, nohz.idle_cpus_mask))
> 				continue;
10840,10846c9350,9354
< 		/*
< 		 * Stop searching for tasks to pull if there are
< 		 * now runnable tasks on this rq.
< 		 */
< 		if (pulled_task || this_rq->nr_running > 0 ||
< 		    this_rq->ttwu_pending)
< 			break;
---
> 			if (sched_asym_prefer(i, cpu)) {
> 				kick = true;
> 				goto unlock;
> 			}
> 		}
10847a9356
> unlock:
10849,10879c9358
< 
< 	raw_spin_rq_lock(this_rq);
< 
< 	if (curr_cost > this_rq->max_idle_balance_cost)
< 		this_rq->max_idle_balance_cost = curr_cost;
< 
< 	/*
< 	 * While browsing the domains, we released the rq lock, a task could
< 	 * have been enqueued in the meantime. Since we're not going idle,
< 	 * pretend we pulled a task.
< 	 */
< 	if (this_rq->cfs.h_nr_running && !pulled_task)
< 		pulled_task = 1;
< 
< 	/* Is there a task of a high priority class? */
< 	if (this_rq->nr_running != this_rq->cfs.h_nr_running)
< 		pulled_task = -1;
< 
< out:
< 	/* Move the next balance forward */
< 	if (time_after(this_rq->next_balance, next_balance))
< 		this_rq->next_balance = next_balance;
< 
< 	if (pulled_task)
< 		this_rq->idle_stamp = 0;
< 	else
< 		nohz_newidle_balance(this_rq);
< 
< 	rq_repin_lock(this_rq, rf);
< 
< 	return pulled_task;
---
> 	return kick;
10880a9360,9362
> #else
> static void nohz_idle_balance(struct rq *this_rq, enum cpu_idle_type idle) { }
> #endif
10893,10894c9375,9376
< 	 * If this CPU has a pending nohz_balance_kick, then do the
< 	 * balancing on behalf of the other idle CPUs whose ticks are
---
> 	 * If this cpu has a pending nohz_balance_kick, then do the
> 	 * balancing on behalf of the other idle cpus whose ticks are
10896c9378
< 	 * give the idle CPUs a chance to load balance. Else we may
---
> 	 * give the idle cpus a chance to load balance. Else we may
10900,10904c9382
< 	if (nohz_idle_balance(this_rq, idle))
< 		return;
< 
< 	/* normal load balance */
< 	update_blocked_averages(this_rq->cpu);
---
> 	nohz_idle_balance(this_rq, idle);
10913,10917c9391,9392
< 	/*
< 	 * Don't need to rebalance while attached to NULL domain or
< 	 * runqueue CPU is not active
< 	 */
< 	if (unlikely(on_null_domain(rq) || !cpu_active(cpu_of(rq))))
---
> 	/* Don't need to rebalance while attached to NULL domain */
> 	if (unlikely(on_null_domain(rq)))
10922,10923c9397,9400
< 
< 	nohz_balancer_kick(rq);
---
> #ifdef CONFIG_NO_HZ_COMMON
> 	if (nohz_kick_needed(rq))
> 		nohz_balancer_kick();
> #endif
10943,11055d9419
< #ifdef CONFIG_SCHED_CORE
< static inline bool
< __entity_slice_used(struct sched_entity *se, int min_nr_tasks)
< {
< 	u64 slice = sched_slice(cfs_rq_of(se), se);
< 	u64 rtime = se->sum_exec_runtime - se->prev_sum_exec_runtime;
< 
< 	return (rtime * min_nr_tasks > slice);
< }
< 
< #define MIN_NR_TASKS_DURING_FORCEIDLE	2
< static inline void task_tick_core(struct rq *rq, struct task_struct *curr)
< {
< 	if (!sched_core_enabled(rq))
< 		return;
< 
< 	/*
< 	 * If runqueue has only one task which used up its slice and
< 	 * if the sibling is forced idle, then trigger schedule to
< 	 * give forced idle task a chance.
< 	 *
< 	 * sched_slice() considers only this active rq and it gets the
< 	 * whole slice. But during force idle, we have siblings acting
< 	 * like a single runqueue and hence we need to consider runnable
< 	 * tasks on this CPU and the forced idle CPU. Ideally, we should
< 	 * go through the forced idle rq, but that would be a perf hit.
< 	 * We can assume that the forced idle CPU has at least
< 	 * MIN_NR_TASKS_DURING_FORCEIDLE - 1 tasks and use that to check
< 	 * if we need to give up the CPU.
< 	 */
< 	if (rq->core->core_forceidle && rq->cfs.nr_running == 1 &&
< 	    __entity_slice_used(&curr->se, MIN_NR_TASKS_DURING_FORCEIDLE))
< 		resched_curr(rq);
< }
< 
< /*
<  * se_fi_update - Update the cfs_rq->min_vruntime_fi in a CFS hierarchy if needed.
<  */
< static void se_fi_update(struct sched_entity *se, unsigned int fi_seq, bool forceidle)
< {
< 	for_each_sched_entity(se) {
< 		struct cfs_rq *cfs_rq = cfs_rq_of(se);
< 
< 		if (forceidle) {
< 			if (cfs_rq->forceidle_seq == fi_seq)
< 				break;
< 			cfs_rq->forceidle_seq = fi_seq;
< 		}
< 
< 		cfs_rq->min_vruntime_fi = cfs_rq->min_vruntime;
< 	}
< }
< 
< void task_vruntime_update(struct rq *rq, struct task_struct *p, bool in_fi)
< {
< 	struct sched_entity *se = &p->se;
< 
< 	if (p->sched_class != &fair_sched_class)
< 		return;
< 
< 	se_fi_update(se, rq->core->core_forceidle_seq, in_fi);
< }
< 
< bool cfs_prio_less(struct task_struct *a, struct task_struct *b, bool in_fi)
< {
< 	struct rq *rq = task_rq(a);
< 	struct sched_entity *sea = &a->se;
< 	struct sched_entity *seb = &b->se;
< 	struct cfs_rq *cfs_rqa;
< 	struct cfs_rq *cfs_rqb;
< 	s64 delta;
< 
< 	SCHED_WARN_ON(task_rq(b)->core != rq->core);
< 
< #ifdef CONFIG_FAIR_GROUP_SCHED
< 	/*
< 	 * Find an se in the hierarchy for tasks a and b, such that the se's
< 	 * are immediate siblings.
< 	 */
< 	while (sea->cfs_rq->tg != seb->cfs_rq->tg) {
< 		int sea_depth = sea->depth;
< 		int seb_depth = seb->depth;
< 
< 		if (sea_depth >= seb_depth)
< 			sea = parent_entity(sea);
< 		if (sea_depth <= seb_depth)
< 			seb = parent_entity(seb);
< 	}
< 
< 	se_fi_update(sea, rq->core->core_forceidle_seq, in_fi);
< 	se_fi_update(seb, rq->core->core_forceidle_seq, in_fi);
< 
< 	cfs_rqa = sea->cfs_rq;
< 	cfs_rqb = seb->cfs_rq;
< #else
< 	cfs_rqa = &task_rq(a)->cfs;
< 	cfs_rqb = &task_rq(b)->cfs;
< #endif
< 
< 	/*
< 	 * Find delta after normalizing se's vruntime with its cfs_rq's
< 	 * min_vruntime_fi, which would have been updated in prior calls
< 	 * to se_fi_update().
< 	 */
< 	delta = (s64)(sea->vruntime - seb->vruntime) +
< 		(s64)(cfs_rqb->min_vruntime_fi - cfs_rqa->min_vruntime_fi);
< 
< 	return delta > 0;
< }
< #else
< static inline void task_tick_core(struct rq *rq, struct task_struct *curr) {}
< #endif
< 
11057,11062c9421
<  * scheduler tick hitting a task of our scheduling class.
<  *
<  * NOTE: This function can be called remotely by the tick offload that
<  * goes along full dynticks. Therefore no local assumption can be made
<  * and everything must be accessed through the @rq and @curr passed in
<  * parameters.
---
>  * scheduler tick hitting a task of our scheduling class:
11076,11080d9434
< 
< 	update_misfit_status(curr, rq);
< 	update_overutilized_status(task_rq(curr));
< 
< 	task_tick_core(rq, curr);
11129,11131d9482
< 	if (rq->cfs.nr_running == 1)
< 		return;
< 
11137c9488
< 	if (task_current(rq, p)) {
---
> 	if (rq->curr == p) {
11165,11166c9516
< 	if (!se->sum_exec_runtime ||
< 	    (READ_ONCE(p->__state) == TASK_WAKING && p->sched_remote_wakeup))
---
> 	if (!se->sum_exec_runtime || p->state == TASK_WAKING)
11181,11182d9530
< 	list_add_leaf_cfs_rq(cfs_rq_of(se));
< 
11189,11195c9537
< 		if (!cfs_rq_throttled(cfs_rq)){
< 			update_load_avg(cfs_rq, se, UPDATE_TG);
< 			list_add_leaf_cfs_rq(cfs_rq);
< 			continue;
< 		}
< 
< 		if (list_add_leaf_cfs_rq(cfs_rq))
---
> 		if (cfs_rq_throttled(cfs_rq))
11196a9539,9540
> 
> 		update_load_avg(cfs_rq, se, UPDATE_TG);
11210c9554
< 	update_tg_load_avg(cfs_rq);
---
> 	update_tg_load_avg(cfs_rq, false);
11229c9573
< 	update_tg_load_avg(cfs_rq);
---
> 	update_tg_load_avg(cfs_rq, false);
11276c9620
< 		if (task_current(rq, p))
---
> 		if (rq->curr == p)
11288c9632
< static void set_next_task_fair(struct rq *rq, struct task_struct *p, bool first)
---
> static void set_curr_task_fair(struct rq *rq)
11290,11300c9634
< 	struct sched_entity *se = &p->se;
< 
< #ifdef CONFIG_SMP
< 	if (task_on_rq_queued(p)) {
< 		/*
< 		 * Move the next running task to the front of the list, so our
< 		 * cfs_tasks list becomes MRU one.
< 		 */
< 		list_move(&se->group_node, &rq->cfs_tasks);
< 	}
< #endif
---
> 	struct sched_entity *se = &rq->curr->se;
11380c9714
< 	tg->cfs_rq = kcalloc(nr_cpu_ids, sizeof(cfs_rq), GFP_KERNEL);
---
> 	tg->cfs_rq = kzalloc(sizeof(cfs_rq) * nr_cpu_ids, GFP_KERNEL);
11383c9717
< 	tg->se = kcalloc(nr_cpu_ids, sizeof(se), GFP_KERNEL);
---
> 	tg->se = kzalloc(sizeof(se) * nr_cpu_ids, GFP_KERNEL);
11418d9751
< 	struct rq_flags rf;
11425c9758,9759
< 		rq_lock_irq(rq, &rf);
---
> 
> 		raw_spin_lock_irq(&rq->lock);
11429c9763
< 		rq_unlock_irq(rq, &rf);
---
> 		raw_spin_unlock_irq(&rq->lock);
11452c9786
< 		raw_spin_rq_lock_irqsave(rq, flags);
---
> 		raw_spin_lock_irqsave(&rq->lock, flags);
11454c9788
< 		raw_spin_rq_unlock_irqrestore(rq, flags);
---
> 		raw_spin_unlock_irqrestore(&rq->lock, flags);
11491c9825
< static int __sched_group_set_shares(struct task_group *tg, unsigned long shares)
---
> int sched_group_set_shares(struct task_group *tg, unsigned long shares)
11495,11496d9828
< 	lockdep_assert_held(&shares_mutex);
< 
11504a9837
> 	mutex_lock(&shares_mutex);
11506c9839
< 		return 0;
---
> 		goto done;
11524,11601c9857
< 	return 0;
< }
< 
< int sched_group_set_shares(struct task_group *tg, unsigned long shares)
< {
< 	int ret;
< 
< 	mutex_lock(&shares_mutex);
< 	if (tg_is_idle(tg))
< 		ret = -EINVAL;
< 	else
< 		ret = __sched_group_set_shares(tg, shares);
< 	mutex_unlock(&shares_mutex);
< 
< 	return ret;
< }
< 
< int sched_group_set_idle(struct task_group *tg, long idle)
< {
< 	int i;
< 
< 	if (tg == &root_task_group)
< 		return -EINVAL;
< 
< 	if (idle < 0 || idle > 1)
< 		return -EINVAL;
< 
< 	mutex_lock(&shares_mutex);
< 
< 	if (tg->idle == idle) {
< 		mutex_unlock(&shares_mutex);
< 		return 0;
< 	}
< 
< 	tg->idle = idle;
< 
< 	for_each_possible_cpu(i) {
< 		struct rq *rq = cpu_rq(i);
< 		struct sched_entity *se = tg->se[i];
< 		struct cfs_rq *grp_cfs_rq = tg->cfs_rq[i];
< 		bool was_idle = cfs_rq_is_idle(grp_cfs_rq);
< 		long idle_task_delta;
< 		struct rq_flags rf;
< 
< 		rq_lock_irqsave(rq, &rf);
< 
< 		grp_cfs_rq->idle = idle;
< 		if (WARN_ON_ONCE(was_idle == cfs_rq_is_idle(grp_cfs_rq)))
< 			goto next_cpu;
< 
< 		idle_task_delta = grp_cfs_rq->h_nr_running -
< 				  grp_cfs_rq->idle_h_nr_running;
< 		if (!cfs_rq_is_idle(grp_cfs_rq))
< 			idle_task_delta *= -1;
< 
< 		for_each_sched_entity(se) {
< 			struct cfs_rq *cfs_rq = cfs_rq_of(se);
< 
< 			if (!se->on_rq)
< 				break;
< 
< 			cfs_rq->idle_h_nr_running += idle_task_delta;
< 
< 			/* Already accounted at parent level and above. */
< 			if (cfs_rq_is_idle(cfs_rq))
< 				break;
< 		}
< 
< next_cpu:
< 		rq_unlock_irqrestore(rq, &rf);
< 	}
< 
< 	/* Idle groups have minimum weight. */
< 	if (tg_is_idle(tg))
< 		__sched_group_set_shares(tg, scale_load(WEIGHT_IDLEPRIO));
< 	else
< 		__sched_group_set_shares(tg, NICE_0_LOAD);
< 
---
> done:
11605d9860
< 
11640,11641c9895,9896
< DEFINE_SCHED_CLASS(fair) = {
< 
---
> const struct sched_class fair_sched_class = {
> 	.next			= &idle_sched_class,
11649c9904
< 	.pick_next_task		= __pick_next_task_fair,
---
> 	.pick_next_task		= pick_next_task_fair,
11651d9905
< 	.set_next_task          = set_next_task_fair,
11654,11655d9907
< 	.balance		= balance_fair,
< 	.pick_task		= pick_task_fair,
11665a9918
> 	.set_curr_task          = set_curr_task_fair,
11680,11683d9932
< 
< #ifdef CONFIG_UCLAMP_TASK
< 	.uclamp_enabled		= 1,
< #endif
11702d9950
< 	struct numa_group *ng;
11704,11705d9951
< 	rcu_read_lock();
< 	ng = rcu_dereference(p->numa_group);
11711,11713c9957,9959
< 		if (ng) {
< 			gsf = ng->faults[task_faults_idx(NUMA_MEM, node, 0)],
< 			gpf = ng->faults[task_faults_idx(NUMA_MEM, node, 1)];
---
> 		if (p->numa_group) {
> 			gsf = p->numa_group->faults[task_faults_idx(NUMA_MEM, node, 0)],
> 			gpf = p->numa_group->faults[task_faults_idx(NUMA_MEM, node, 1)];
11717d9962
< 	rcu_read_unlock();
11729d9973
< 	nohz.next_blocked = jiffies;
11735,11832d9978
< 
< /*
<  * Helper functions to facilitate extracting info from tracepoints.
<  */
< 
< const struct sched_avg *sched_trace_cfs_rq_avg(struct cfs_rq *cfs_rq)
< {
< #ifdef CONFIG_SMP
< 	return cfs_rq ? &cfs_rq->avg : NULL;
< #else
< 	return NULL;
< #endif
< }
< EXPORT_SYMBOL_GPL(sched_trace_cfs_rq_avg);
< 
< char *sched_trace_cfs_rq_path(struct cfs_rq *cfs_rq, char *str, int len)
< {
< 	if (!cfs_rq) {
< 		if (str)
< 			strlcpy(str, "(null)", len);
< 		else
< 			return NULL;
< 	}
< 
< 	cfs_rq_tg_path(cfs_rq, str, len);
< 	return str;
< }
< EXPORT_SYMBOL_GPL(sched_trace_cfs_rq_path);
< 
< int sched_trace_cfs_rq_cpu(struct cfs_rq *cfs_rq)
< {
< 	return cfs_rq ? cpu_of(rq_of(cfs_rq)) : -1;
< }
< EXPORT_SYMBOL_GPL(sched_trace_cfs_rq_cpu);
< 
< const struct sched_avg *sched_trace_rq_avg_rt(struct rq *rq)
< {
< #ifdef CONFIG_SMP
< 	return rq ? &rq->avg_rt : NULL;
< #else
< 	return NULL;
< #endif
< }
< EXPORT_SYMBOL_GPL(sched_trace_rq_avg_rt);
< 
< const struct sched_avg *sched_trace_rq_avg_dl(struct rq *rq)
< {
< #ifdef CONFIG_SMP
< 	return rq ? &rq->avg_dl : NULL;
< #else
< 	return NULL;
< #endif
< }
< EXPORT_SYMBOL_GPL(sched_trace_rq_avg_dl);
< 
< const struct sched_avg *sched_trace_rq_avg_irq(struct rq *rq)
< {
< #if defined(CONFIG_SMP) && defined(CONFIG_HAVE_SCHED_AVG_IRQ)
< 	return rq ? &rq->avg_irq : NULL;
< #else
< 	return NULL;
< #endif
< }
< EXPORT_SYMBOL_GPL(sched_trace_rq_avg_irq);
< 
< int sched_trace_rq_cpu(struct rq *rq)
< {
< 	return rq ? cpu_of(rq) : -1;
< }
< EXPORT_SYMBOL_GPL(sched_trace_rq_cpu);
< 
< int sched_trace_rq_cpu_capacity(struct rq *rq)
< {
< 	return rq ?
< #ifdef CONFIG_SMP
< 		rq->cpu_capacity
< #else
< 		SCHED_CAPACITY_SCALE
< #endif
< 		: -1;
< }
< EXPORT_SYMBOL_GPL(sched_trace_rq_cpu_capacity);
< 
< const struct cpumask *sched_trace_rd_span(struct root_domain *rd)
< {
< #ifdef CONFIG_SMP
< 	return rd ? rd->span : NULL;
< #else
< 	return NULL;
< #endif
< }
< EXPORT_SYMBOL_GPL(sched_trace_rd_span);
< 
< int sched_trace_rq_nr_running(struct rq *rq)
< {
<         return rq ? rq->nr_running : -1;
< }
< EXPORT_SYMBOL_GPL(sched_trace_rq_nr_running);
